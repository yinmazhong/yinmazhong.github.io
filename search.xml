<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[javaIO]]></title>
    <url>%2F2019%2F06%2F04%2FjavaIO%2F</url>
    <content type="text"><![CDATA[JAVA IO整体架构图]]></content>
  </entry>
  <entry>
    <title><![CDATA[aTemp]]></title>
    <url>%2F2019%2F06%2F03%2FaTemp%2F</url>
    <content type="text"><![CDATA[我认为：到目前为止，作为居间方，乐有家的服务是不合格的 1.定金收据开错，这个问题还是我们业主检查出来的，这个错误能从诞生到呈现在我们面前，难道没有任何一个人把关负责？2.在对卖方业主没有任何知会的情况下擅自变更贷款银行，地点，事后才通知我们，可以看出根本不会尊重我们意见与感受3.做资金监管过程中，乐有家的经纪人、按揭员没有一个在场，重要事情靠电话沟通，您各位的时间是时间，我们的时间也是时间呀，况且我们也支付了服务费，不是吗？4.定金十五万，却要求开二十万的收据（收据错误暂且不表），签房产交易合同时，已经向买家开出五万定金收据，做资金监管当天，竟要求再开十五万收据给银行拍照用，加起来就是二十万，您各位可能觉得没什么，可五万块的风险却结结实实砸在我们脑袋上。5.依照《中华人民共和国合同法》《中华人民共和国城市房地产管理法》《城市房地产中介服务管理规定》，房地产中介机构是指依法设立、从事房地产经纪活动的中介服务机构，其主要目的为了促成房地产交易，向委托人提供房地产居间、代理等服务并收取佣金的行为。且不说付没付佣金，向各位咨询几个赎楼相关问题不过分吧？您要是知道呢，就确认好以后，给出【专业的，确定的答复】，您要是不知道，就直接回复【不知道】，我们也好自行花费时间精力，甚至金钱去寻找答案。6.在充分信任您各位的专业与诚意的情况下，6/3起了个大早，跑了趟罗湖，浪费了时间精力，什么都没干成；难道建行公积金中心是今天早上才改的这条规定？还是那句话，大家都很忙，谁的时间都是时间。 客观现实就是，这单交易到目前为止，已经漏洞百出，让人不得不质疑您各位的诚意，专业。 相关操作是否违反了相关法律法规请自省]]></content>
      <categories>
        <category>temp</category>
      </categories>
      <tags>
        <tag>temp file</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[o21bigdata.md]]></title>
    <url>%2F2019%2F05%2F28%2Fo21bigdata-md%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[skills]]></title>
    <url>%2F2019%2F05%2F27%2Fskills%2F</url>
    <content type="text"><![CDATA[通过实践来复习，学习 分布式文件系统理论基础： 共享数据是分布式系统的基础，分布式文件系统是分布式系统，和分布式应用程序的基础 体系结构客户-服务器体系结构（Network File System）NFS每个文件服务器都提供其本地文件系统的一个标准化视图 基于集群的分布式文件系统服务与并行应用程序，一种著名的技术是部署文件带区划分技术，通过该技术可以将一个文件分布在多个服务器上另外一种是把文件系统作为一个整体进行分区并且简单的把不同的文件存储在不同的服务器上，而不是把单个单个文件划分到多个服务器上 对称式体系结构进程文件系统进程是否应该是无状态的？ 通信RPC 命名同步一致性和复制容错性安全性]]></content>
      <tags>
        <tag>skills</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[imagetest]]></title>
    <url>%2F2019%2F05%2F27%2Fimagetest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[操作系统]]></title>
    <url>%2F2019%2F05%2F08%2Fos%2F</url>
    <content type="text"><![CDATA[2019年 05月 08日 星期三 09:53:05 CST计算机是怎么启动的]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 十月]]></title>
    <url>%2F2019%2F05%2F07%2F2018%E5%8D%81%E6%9C%88%2F</url>
    <content type="text"><![CDATA[ToDo 日志对象单例对象 脚数据和楦数据都是通过json加载的，初步看，9成都是无用的key数据，剂需优化 这些单独的服务做成微服务 setting up JVM 工厂方法 2018年 10月 08日 星期一 传统来说，主板上两个主要芯片，靠上方的叫北桥，靠下方的叫南桥。 大体上说：北桥负责与CPU通信，并且连接高速设备（内存/显卡），并且与南桥通信；南桥负责与低速设备（硬盘/USB）通信，时钟/BIOS/系统管理/旧式设备控制，并且与北桥通信。 2018年 10月 09日 星期二 09:40:08 CST 计算机硬件架构的发展 计算机科学领域的任何问题，都可以通过增加一个间接的中间层来解决 操作系统在任务调度上，经历了多道程序、分时系统与多任务系统等阶段 2018年 10月 10日 星期三 14:01:51 CST 2018年 10月 16日 星期二 14:54:35 CSTgradle 2018年 10月 17日 星期三 14:54:35 CST socketsocket的英文原义是“孔”或“插座”。作为4BDS UNIX的进程通信机制，取后一种意思。通常也称作”套接字”，用于描述IP地址和端口，是一个通信链的句柄。应用程序通常通过”套接字”向网络发出请求或者应答网络请求。 在Internet上的主机一般运行了多个服务软件，同时提供几种服务。每种服务都打开一个Socket，并绑定到一个端口上，不同的端口对应于不同的服务。Socket正如其英文原意那样，象一个多孔插座。一台主机犹如布满各种插座的房间，每个插座有一个编号，有的插座提供220伏交流电， 有的提供110伏交流电，有的则提供有线电视节目。 客户软件将插头插到不同编号的插座，就可以得到不同的服务。 2018年 10月 18日 星期四 10:27:01 CST 2018年 10月 19日 星期五 10:45:09 CST kafka-stream 只能用在同一个集群中？ 初步查了下：是 11/21更新，官网文档上写，可以将数据写回kafka，或发送到外部系统，研究下再补充 数据库设计三大范式 相比于 Spark-streaming storm 这种框架级别的流处理工具，kafka stream 提供了库级别的流处理工具 还有flink，但是对这个比了解 IDEA 中的Tranlation 插件很好使在没有配置plugin repo的情况下搜不到，直接到intellgin 官网搜就可以了，然后下载插件，安装，具体使用方案文档上有 搜插件 幂等 幂等是一个数学与计算机学概念，常见于抽象代数中 在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。 幂等函数与幂等方法是指可以使用相同参数重复执行，并能获得相同结果的函数 Java Map Java 反射 学习java应该如何理解反射？ 反射在很多框架的根据配置加载类部分用到 最常见的就是连接数据库时，加载相应数据库的Driver 2018年 10月 22日 星期一 15:23:46 CST java 注解 java 注解 bean hadoop 2.7.7 yarn伪分布式安装ok(spark 使用了hadoop-client lib(hdfs、yarn)) spark 2.3.2(hadoop 2.7) 安装ok hbase 安装ok kafka正在拉数据 hadoop 3 改动颇大，先用hadoop 2 spark 2改动也挺大，但是已经上了hadoop2.7.7.硬着头皮上车吧，毕竟已经一年多了，应该比较稳定，出了问题搜起来也比较方便 spark on yarn 没有装好 用的是stand alone 模式 2018年 10月 23日 星期二 11:31:33 CST sbt有些依赖拉不到 在build.sbt 中添加相应的源 1resolvers += &quot;cdh&quot; at &quot;https://repository.cloudera.com/artifactory/libs-release-local/&quot; Resolvers 貌似 cdh 只能用cdh 提供的依赖，不能用原生的依赖 昨天hexo gitpage 一直不能更新，检查过，本地静态网页生成正常，上传值github正常，但是博客就是不更新，今天发现已经恢复正常了，简直了 也没见发个邮件说明一下，过分了，对洋大人的期待有点高了 port in user / address in use solve: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//占用与否确认netstat -ln |grep ****(4000)// 找到process IDlsof -i:****(4000)kill -9 ID``` cdh 获取配置文件xml首页 -&gt; 某种服务 -&gt; 页面左上角action -&gt; Download Client configuration是个zip包 解压就行了------hadoop api 需要的依赖在官网一直没找到，查了下教程类的文章上也少有提到难道这是“显然”的，不用介绍？实战类的文章上提到应是这三个hadoop-commonhadoop-hdfshadoop-client总是觉得没见到官宣，会惴惴不安，以后会不会改变---2018年 10月 24日 星期三 09:08:33 CSTjps 展示的进程SparkSubmit 杀不掉ps -ef |grep spark 出来的进程杀死后，SparkSubmit 进程消失，why?[这个洋人说可以直接将jps出来的进程杀死](https://stackoverflow.com/questions/29990153/submit-kill-spark-application-program-programmatically-from-another-applicatio)下次试下二手 think pad e540 cpu 信息[cpu信息解读](https://blog.csdn.net/sycflash/article/details/6643492) cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 8 Intel(R) Core(TM) i7-4702MQ CPU @ 2.20GHz12 memory: sudo dmidecode -t memory dmidecode 3.1Getting SMBIOS data from sysfs.SMBIOS 2.7 present. Handle 0x0005, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 16 GB Error Information Handle: Not Provided Number Of Devices: 2 Handle 0x0006, DMI type 17, 34 bytesMemory Device Array Handle: 0x0005 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 8192 MB Form Factor: SODIMM Set: None Locator: ChannelA-DIMM0 Bank Locator: BANK 0 Type: DDR3 Type Detail: Synchronous Speed: 1600 MT/s Manufacturer: Hynix/Hyundai Serial Number: 08C66216 Asset Tag: None Part Number: HMT41GS6AFR8A-PB Rank: Unknown Configured Clock Speed: 1600 MT/s Handle 0x0007, DMI type 17, 34 bytesMemory Device Array Handle: 0x0005 Error Information Handle: Not Provided Total Width: Unknown Data Width: Unknown Size: No Module Installed Form Factor: DIMM Set: None Locator: ChannelB-DIMM0 Bank Locator: BANK 2 Type: Unknown Type Detail: None Speed: Unknown Manufacturer: Not Specified Serial Number: Not Specified Asset Tag: None Part Number: Not Specified Rank: Unknown Configured Clock Speed: Unknown 1234567891011121314151617181920212223242526272829303132333435363738淘宝入手8g 1.35v 内存，因为cpu是低电压的---hive1.2.2 安装按照官网文档安装，下载解压，配置HIVE_HOME、PATH在设置好了HADOOP_HOME的情况下可以运行了运行hiveserver2的时候很卡，很慢，kill 之---好用的top:htop------2018年 10月 25日 星期四 09:46:38 CSTsqoop 本地环境 hive 环境 内存条来后8g变成4g，醉了，又买了一根，这根先用着，用完退了---重启电脑后发现之前设置的ulimit 没用了找到终极解决办法： vim /etc/security/limits.conf //加入以下配置，重启即可生效 soft nofile 65535 hard nofile 65535 12345678[ulimit 设置](http://coolnull.com/2796.html)---hadoop 伪分布式 中 hdfs 最好配置好 dfs.namenode.name.dir 123456789该属性默认位置在/tmp目录下，重启后目录消失，需要重新 hdfs namenode -format ,造成namenode 数据全部丢失，并且启动namenode 失败--- ---mysql 密码重置 sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf 在[mysqld]下添加 skip-grant-tables service mysql restart mysql 免密免用户登录 use mysqlupdate mysql.user set authentication_string=password(‘admin123’) where user=’root’ and Host = ‘localhost’;update user set plugin=”mysql_native_password” where User=’root’; # THIS LINEflush privileges;exit; 最后删掉添加的免密登录配置，重启mysql,重新用刚设置的密码登录 12345[重置密码](https://blog.csdn.net/yelllowcong/article/details/79641313)[ERROR 1524 (HY000): Plugin &apos;auth_socket&apos; is not loaded](https://stackoverflow.com/questions/37879448/mysql-fails-on-mysql-error-1524-hy000-plugin-auth-socket-is-not-loaded)allow extenal in GRANT ALL PRIVILEGES ON . TO ‘root’@’%’ IDENTIFIED BY ‘Admin@321’ WITH GRANT OPTION; 12345678910111213141516授权完毕后还是不能访问检查防火墙检查 bind_address[Grand 后操作](https://blog.csdn.net/Plus_RE/article/details/79649427)------sqoop install &amp;&amp; test sqoop list-databases –connect jdbc:mysql://localhost:3306 –username root –password admin123 $ sqoop import –connect jdbc:mysql://localhost:3306/mysql –username root –password admin123 –table user –target-dir /sqoop/data –delete-target-dir –num-mappers 1 –fields-terminated-by “\t” 12345---github 上找到了能用的 sublime text 3 liscens —– BEGIN LICENSE —–sgbteamSingle User LicenseEA7E-11532598891CBB9 F1513E4F 1A3405C1 A865D53F115F202E 7B91AB2D 0D2A40ED 352B269B76E84F0B CD69BFC7 59F2DFEF E267328F215652A3 E88F9D8F 4C38E3BA 5B2DAAE4969624E7 DC9CD4D5 717FB40C 1B9738CF20B3C4F1 E917B5B3 87C38D9C ACCE7DD85F7EF854 86B9743C FADC04AA FB0DA5C0F913BE58 42FEA319 F954EFDD AE881E0B—— END LICENSE —— 1234567891011讨厌的提示应该不会有了吧，之前每次ctrl+c都会弹两个框------2018年 10月 29日 星期一 15:23:31 CSTshell nohup 重定向 还有 /dev/null 这个无底洞 ，一直在用，但是不熟悉什么意思 --- --- 2018年 10月 30日 星期二 16:31:13 CST 找最权威的依赖信息，去官网，比如github主页 --- 2018年 10月 31日 星期三 13:51:25 CST ? vim 插入时间 ? samza ---]]></content>
      <categories>
        <category>札记</category>
      </categories>
      <tags>
        <tag>札记</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F02%2Fscheduler%2F</url>
    <content type="text"><![CDATA[kafka权威指南第一遍 2019年 03月 20日 星期三 15:17:45 CST大数据日知录第三章ing数据结构java第一章ingscala并发编程第二章ing同步应用同步jvm深入拆解jvm垃圾回收深入理解jvmtoday’s todo list priority item starttime endtime ok remarks 1 上一天总结 08:30 09:00 ok 今天来的比较早，粗略的看了一下，玩了两把游戏]]></content>
  </entry>
  <entry>
    <title><![CDATA[工具使用]]></title>
    <url>%2F2019%2F03%2F26%2F%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[idea : psvm: public static void main sout: System.out.println fori: for循环]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流]]></title>
    <url>%2F2019%2F03%2F25%2F%E6%B5%81%2F</url>
    <content type="text"><![CDATA[流：何为流如何理解编程语言中「流」（stream）的概念如何形象的理解计算机中“流”的概念？ 流是由生产者生产并由一个或多个消费者消费的元素的序列 这种生产者——消费者模型也被称为source/sink模型或发布者——订阅者（publisher-subscriber ）模型。 在本章中，将其称为发布者订阅者模型。 有几种流处理机制，其中pull模型和push模型是最常见的。在push模型中，发布者将元素推送给订阅者。在pull模式中，订阅者将元素推送给发布者。发布者和订阅者都以同样的速率工作，这是一个理想的情况，这些模式非常有效。 我们会考虑一些情况，如果他们不按同样的速率工作，这种情况下涉及的问题以及对应的解决办法。 当发布者比订阅者快的时候，后者必须有一个无边界缓冲区来保存快速传入的元素，或者它必须丢弃它无法处理的元素。 另一个解决方案是使用一种称为背压（backpressure ）的策略，其中订阅者告诉发布者减慢速率并保持元素，直到订阅者准备好处理更多的元素。 使用背压可确保更快的发布者不会压制较慢的订阅者。 使用背压可能要求发布者拥有无限制的缓冲区，如果它要一直生成和保存元素。 发布者可以实现有界缓冲区来保存有限数量的元素，如果缓冲区已满，可以选择放弃它们。 可以使用另一策略，其中发布者将发布元素重新发送到订阅者，这些元素发布时订阅者不能接受。 订阅者在请求发布者的元素并且元素不可用时，该做什么？ 在同步请求中订阅者户必须等待，无限期地，直到有元素可用。 如果发布者同步地向订阅者发送元素，并且订阅者同步处理它们，则发布者必须阻塞直到数据处理完成。 解决方案是在两端进行异步处理，订阅者可以在从发布者请求元素之后继续处理其他任务。 当更多的元素准备就绪时，发布者将它们异步发送给订阅者。 reactive streams(响应式流)提供非阻塞背压的异步流处理，旨在解决处理元素流的问题－－ 如何讲元素流从发布者传递到订阅者，而不需要分布者阻塞，或订阅者有无无限制的缓冲区或丢弃 响应式流在pull模型和push模型流处理机制之间动态切换。 当订阅者较慢时，它使用pull模型，当订阅者更快时使用push模型。响应式流模型非常简单——订阅者向发布者发送多个元素的异步请求。 发布者向订阅者异步发送多个或稍少的元素。 http://www.reactive-streams.org/ reactive stream的各种实现 Java APIJava API的响应式流只包含四个接口： PublisherSubscriberSubscriptionProcessor&lt;T,R&gt; 发布者（publisher）是潜在无限数量的有序元素的生产者。 它根据收到的要求向当前订阅者发布（或发送）元素。 订阅者（subscriber）从发布者那里订阅并接收元素。 发布者向订阅者发送订阅令牌（subscription token）。 使用订阅令牌，订阅者从发布者哪里请求多个元素。 当元素准备就绪时，发布者向订阅者发送多个或更少的元素。 订阅者可以请求更多的元素。 发布者可能有多个来自订阅者的元素待处理请求。 订阅（subscription）表示订阅者订阅的一个发布者的令牌。 当订阅请求成功时，发布者将其传递给订阅者。 订阅者使用订阅令牌与发布者进行交互，例如请求更多的元素或取消订阅。 下图显示了发布者和订阅者之间的典型交互顺序。 订阅令牌未显示在图表中。 该图没有显示错误和取消事件。 处理者（processor）充当订阅者和发布者的处理阶段。 Processor接口继承了Publisher和Subscriber接口。 它用于转换发布者——订阅者管道中的元素。 Processor&lt;T,R&gt;订阅类型T的数据元素，接收并转换为类型R的数据，并发布变换后的数据。 下图显示了处理者在发布者——订阅和管道中作为转换器的作用。 可以拥有多个处理者。 JDK 9 中响应式流的APIJDK 9提供了Publisher接口的简单实现，可以将其用于简单的用例或扩展以满足自己的需求。 RxJava是响应式流的Java实现之一。 123456789101112131415161718192021222324252627import java.util.concurrent.CompletableFuture;import java.util.concurrent.ExecutionException;import java.util.concurrent.SubmissionPublisher;import java.util.stream.LongStream;public class ReactiveStreamsTest &#123; public static void main(String args[]) &#123; CompletableFuture&lt;Void&gt; subTask = null; try (SubmissionPublisher&lt;Long&gt; pub = new SubmissionPublisher&lt;&gt;()) &#123; System.out.println(&quot;Subscriber Buffer SizeL&quot; + pub.getMaxBufferCapacity()); subTask = pub.consume(System.out::println); LongStream.range(1L, 2566L).forEach( l -&gt; pub.submit(l)); &#125; if (subTask != null)&#123; try&#123; subTask.get(); &#125;catch (InterruptedException | ExecutionException e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; akka-streamskafka消息中间件 流处理引擎kafka streamsspark Streamingflink]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 三月]]></title>
    <url>%2F2019%2F03%2F11%2F2019May%2F</url>
    <content type="text"><![CDATA[2019年 03月 08日 星期五 15:37:20 CST 取模函数？ 总结记好 一致性哈希算法]]></content>
      <categories>
        <category>札记</category>
      </categories>
      <tags>
        <tag>札记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西门子品宜系列]]></title>
    <url>%2F2019%2F02%2F25%2Fpinyi%2F</url>
    <content type="text"><![CDATA[型号：5TA06133NC01 单价：￥25.7 型号：5TA06213NC01 单价：￥28.3 型号：5TA06233NC01 单价：￥34.0 型号：5TA06318NC01 单价：￥38.6 型号：5TA06338NC01 单价：￥45.1 型号：5TA06414NC01 单价：￥55.6 型号：5TA06434NC01 单价：￥64.1 型号：5TA86153NC01 单价：￥70.4 型号：5UB06183NC01 单价：￥29.5 型号：5UB06442NC01 单价：￥34.2 型号：5UB06483NC01 单价：￥41.7 型号：5UB06153NC01 单价：￥25.6 型号：5UB06193NC01 单价：￥34.7 型号：5UB86163NC01 单价：￥111.0 型号：5UB06312NC01 单价：￥24.2 型号：5TG06321NC01 单价：￥75.5 型号：5TG06322NC01 单价：￥113.4 型号：5TG06311NC01 单价：￥45.7 型号：5TG06312NC01 单价：￥82.0 型号：5TG06331NC01 单价：￥32.5 型号：5TG06342NC01 单价：￥58.0 型号：5TG06362NC01 单价：￥87.7 型号：5TG06372NC01 单价：￥113.4]]></content>
      <categories>
        <category>商品</category>
      </categories>
      <tags>
        <tag>商品</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala并发编程]]></title>
    <url>%2F2019%2F02%2F25%2Fscala%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[scala并发编程1.并发程序设计2.JVM和Java内存模型中的并发处理方式 从某种程度上讲，所有发挥作用的抽象都是有漏洞的from 月儿。思博斯基 3.构建并发程序的传统材料4.使用Future和Promise对象编写异步程序 java编程思想-并发仅仅是自己没有启动线程并不代表你就可以回避编写使用线程的代码 下面是并发编程的基础 并发编程的多面性更快的执行并发通常的提高运行在单处理器上的程序的性能,应对阻塞；区别事件驱动编程；最简单的并发是操作系统级别的进程并发；进程是运行在独自地址空间内的自包容程序；但是进程有数量和开销方面的限制函数式语言，每个函数不会产生任何副作用；java采用更加传统的方式，在顺序型语言的基础上提供对线程的支持；这样的好处是操作系统的透明性； 改进代码设计仿真现实业务更方便 基本的线程机制定义任务当从Runnable导出一个类时，他必须具有run方法，但是这个类不会产生内在的线程能力，要实现线程行为，必须显式的将一个任务附着在线程上 Thread类使用Executorjava.utils.concurrent包中的执行器(Executor),将为你管理Thread对象，从而简化了并发编程 从任务中产生返回值Callable接口 sleep优先级让步Thread.yield() 后台线程当所有非后台线程退出后，后台线程全部退出 加入一个线程jion() 捕获异常]]></content>
      <categories>
        <category>服务器端 - 并发编程</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>并发编程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Akka]]></title>
    <url>%2F2019%2F02%2F12%2FLearningakka%2F</url>
    <content type="text"><![CDATA[程序是怎样运行的？进程和线程在当前抢占式多任务操作系统中，程序员几乎或者彻底失去了命令处理器调整程序执行次序的权利将程序中的各个可执行部分分配给指定的处理器，通常是操作系统OS的工作，这种机制称作多任务处理，而且对计算机用户透明进程是指被执行的计算机程序的一个实例在同一个进程中出现的独立控制流实体称为线程，在执行程序的过程中，每个线程都会描述程序栈和程序计数器的当前状态可以将进程视为一组OS线程以及这些线程共享的内存和资源 启动一个新的JVM实例永远只会创建一个新的进程，在JVM进程中，多个线程可以同时运行，JVM使用java.lang.Thread 类代表其线程 每当一个新的jvm进程开始运行时，默认情况下他都会创建多个线程。而这些线程中最重要的线程是主线程，该线程会执行scala 程序中的main方法 线程池：基本思想还是一种对象池的思想，开辟一块内存空间，里面存放了众多的线程（都还生龙活虎的），池中的线程执行调度由池管理器来处理，当有线程任务时，从池中取出一个，执行完成后将线程对象归还给线程池，这样可以避免反复创建线程对象所带来的性能开销，节省了系统的资源。 akka中文文档 Actor 系统Actor 是封装状态和行为的对象，他们唯一的通讯方式是交换消息－－把消息存放在接收方的邮箱里 akka是瑞士的一座山 actor 模型是为了减少共享状态 object jingtaitiaojian { def main(args: Array[String]): Unit = { var i, j = 0 (1 to 10000).foreach(_ =&gt; i = i + 1) (1 to 10000).foreach(_ =&gt; Future(j = j + 1)) Thread.sleep(1000) println(s&quot;i $i and j $j&quot;) } } AKKAakka docsecurity announcementsakka remoteremote actor 需要先部署 然后通过配置文件获得该actor的ref，从而可以正常传递消息 官方github例子讲了写狗屁，浪费了老子大量时间 akka cluster横向扩展－集群化akka doc在介绍 akka-remote和akka-cluster的时候，不止一次的强调过，akka-remote这种点对点的模块一般不会单独使用，而是使用基于akka-remote的akka-cluster构建多机分布式集群 分布式系统基本概念集群：遵循某种协议的多台机器构成的一个整体 巨型单体应用 VS 微服务Martin Fowler 应用程序较小、团队也小，使用单体应用效率更高一些 当需要切分单体为多个微服务时，单体会帮助我们理解应该将那些部分分隔出来 负载较高的微服务扩展 akka 更多的时候使用配置而不是代码来做这些部署决定，使用多个代码库 -.-? 总结：从代码开发的一开始，就应该让不同的团队使用不同的代码库，不管是单体应用还是多个独立的微服务应用 集群定义 遵循某种协议的多台机器构成的一个整体 集群需要具备的两个功能：失败发现以及使得集群中的所有成员最终能够提供一致的视图 失败检测 问题：加入集群的每台机器都用跟其他节点通信以实现失败检测的话，集群的性能不会随着集群节点数的增加而线性提高，因为每增加一个节点，需要的通信开销成本会指数增加akka解决：为了降低监控其他节点健康程度的复杂度，akka中的失败检测（节点间发送心跳信息并接受回应）只会监控某个节点附近特定数据的节点 通过gossip 协议达到最终一致性 akka负责确定集群中是否有变化，并负责将发生的任何变化传递给集群中的所有节点 试了下官网的例子，但是文档还没有看完，慢慢看吧，现在有更有意思的事情来了。 Akka-Http最先接触的是spray,是typesafe推的一个scala servlet lib,后来akka开发了akka-http,是spray的升级版，spray也不再维护，转而推荐使用akka-http 自带服务器，写接口，方便， AKKA-HTTP性能测试 刚开启,初始化阶段 刚开启,初始化后期 1024每s的访问 连续多次,1024每s的访问 百万请求报错 报错,但是控制台却没有日志提示 发现了这个错误,我却不知道怎么解决,那就是知识已经不足以支持我的野心了akka-in-action 第三章:传递消息 Akka-http 加载使用静态网页(static html)，样式(css)，js万能的stackoverflow 世界上的一切问题都可以在stackoverflow上找到答案，如果没找打，那就是姿势不对 12345678910import akka.http.scaladsl.model.StatusCodesval staticResources = (get &amp; pathPrefix(&quot;admin&quot;))&#123; (pathEndOrSingleSlash &amp; redirectToTrailingSlashIfMissing(StatusCodes.TemporaryRedirect)) &#123; getFromResource(&quot;admin/index.html&quot;) &#125; ~ &#123; getFromResourceDirectory(&quot;admin&quot;) &#125; &#125; 就上面的代码而言，相对陌生的是getFromResourceDirectory 官方文档： Completes GET requests with the content of the given classpath resource directory. For details refer to getFromDirectory which works the same way but obtaining the file from the filesystem instead of the applications classpath. Note that it’s not required to wrap this directive with get as this directive will only respond to GET requests. 使用给定类路径资源目录的内容完成GET请求。 有关详细信息，请参阅getFromDirectory，它的工作方式相同，但是从文件系统而不是应用程序类路径获取文件。 请注意，不需要使用get包装此指令，因为此指令仅响应GET请求。 打包？依然使用打包工具sbt-native-packager plugins.sbt 1addSbtPlugin(&quot;com.typesafe.sbt&quot; % &quot;sbt-native-packager&quot; % &quot;1.2.0&quot;) build.sbt 1234567891011121314151617181920import com.typesafe.sbt.packager.MappingsHelper.&#123;contentOf, directory&#125;enablePlugins(JavaServerAppPackaging)mainClass in Compile := Some(&quot;com.qinglongmu.dataplatform.WebServer&quot;)mappings in Universal ++= &#123; // optional example illustrating how to copy additional directory directory(&quot;scripts&quot;) ++ // copy configuration files to config directory contentOf(&quot;src/main/resources&quot;).toMap.mapValues(&quot;config/&quot; + _)&#125;// add &apos;config&apos; directory first in the classpath of the start script,// an alternative is to set the config file locations via CLI parameters// when starting the applicationscriptClasspath := Seq(&quot;../config/&quot;) ++ scriptClasspath.valuelicenses := Seq((&quot;CC0&quot;, url(&quot;http://creativecommons.org/publicdomain/zero/1.0&quot;))) 这样可以直接修改config目录中的静态文件]]></content>
      <categories>
        <category>服务器端 - 并发编程</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>Akka</tag>
        <tag>actor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 二月]]></title>
    <url>%2F2019%2F02%2F11%2F2019February%2F</url>
    <content type="text"><![CDATA[2019年 02月 11日 星期一 15:59:38 CST win10激活—kms服务器自建 自建方法 1wget --no-check-certificate https://github.com/teddysun/across/raw/master/kms.sh &amp;&amp; chmod +x kms.sh &amp;&amp; ./kms.sh 再也不用满世界找激活码了，也不用担心kms工具自带病毒了]]></content>
      <categories>
        <category>札记</category>
      </categories>
      <tags>
        <tag>札记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 一月]]></title>
    <url>%2F2019%2F01%2F29%2F2019January%2F</url>
    <content type="text"><![CDATA[2019年 01月 03日 星期四 15:42:15 CST producer consumer 生产消费数据都有点慢,如何实现速度 &gt;1000条/s 每次new 一个 KafkaProducer,并在发送完毕后关闭都及其浪费资源的.每次只封装ProducerRecord就可以了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 def main(args: Array[String]): Unit = &#123; val startTime = LocalDateTime.now() logger.info(s&quot;start time:$&#123;startTime&#125;&quot;) val props = new util.HashMap[String, Object]() props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokerList) //leader 副本ok即recall back props.put(ProducerConfig.ACKS_CONFIG, &quot;1&quot;) props.put(ProducerConfig.RETRIES_CONFIG, &quot;2&quot;) props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, &quot;1000&quot;) props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;testradio&quot;) props.put(ProducerConfig.BATCH_SIZE_CONFIG, &quot;163480&quot;) props.put(ProducerConfig.LINGER_MS_CONFIG, &quot;100&quot;) props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, &quot;500000000&quot;) props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;) val producer = new KafkaProducer[Int, Int](props) val randomer = new Random(100) try &#123; (0 to 1000000).foreach(_ =&gt; productData2Kafka(producer, &quot;ini-t2&quot;, randomer.nextInt, randomer.nextInt)) &#125; catch &#123; case e: Exception =&gt; logger.error(e.getMessage) false &#125; finally &#123; if (producer != null) producer.close() &#125; val endTime = LocalDateTime.now() logger.info(s&quot;end time:$&#123;endTime&#125;&quot;) logger.info(s&quot;spend time:$&#123;startTime&#125; to $endTime&quot;) &#125;////////////////////////////////////////////////////////////////////////////////// def productData2Kafka(producer: KafkaProducer[Int, Int], topicName: String, key: Int, value: Int) = &#123; // Zookeeper connection properties //logger.info(s&quot;producer: $&#123;producer.toString&#125;&quot;) //val producer = new KafkaProducer[String, MeasurementItems](props) // Send messages val topic = topicName //val message = new ProducerRecord[String, MeasurementItems](topic, null, measurementItems) val message = new ProducerRecord[Int, Int](topic, key, value) //val message = new ProducerRecord[String, String](topic, null, measurementItems.UUID) //val message = new ProducerRecord[String, MeasurementItems](topic, null, measurementItems) //val future = producer.send(message) val callback = new KafkaProducerCallback producer.send(message, callback) //val res = future.get(5,SECONDS) //logger.info(&quot;res:&quot;+res.toString) //true &#125;/////////////////////////////////////////////////////////////////////////////////////////////class KafkaProducerCallback extends Callback &#123; override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = &#123; if (exception==null)&#123; logger.debug(s&quot;send message $&#123;metadata.offset()&#125; to $&#123;metadata.topic()&#125; success&quot;) &#125;else&#123; logger.error(&quot;121&quot;+exception.getMessage) throw exception &#125; &#125;&#125; 2019年 01月 04日 星期五 10:33:48 CST2019年 01月 09日 星期三 17:35:15 CST多线程的错误与恢复 多线程应用的健壮性就在于多线程的异常处理 2019年 01月 13日 星期日 00:05:22 CST线程池使用 2019年 01月 29日 星期二 10:32:59 CST pyCharm 激活码： 1SSUJFAQGMI-eyJsaWNlbnNlSWQiOiJTU1VKRkFRR01JIiwibGljZW5zZWVOYW1lIjoiWmhpd2VpIEhvbmciLCJhc3NpZ25lZU5hbWUiOiIiLCJhc3NpZ25lZUVtYWlsIjoiIiwibGljZW5zZVJlc3RyaWN0aW9uIjoiRm9yIGVkdWNhdGlvbmFsIHVzZSBvbmx5IiwiY2hlY2tDb25jdXJyZW50VXNlIjpmYWxzZSwicHJvZHVjdHMiOlt7ImNvZGUiOiJJSSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IkFDIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiRFBOIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiUFMiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJHTyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IkRNIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiQ0wiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSUzAiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSQyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IlJEIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiUEMiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSTSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IldTIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiREIiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJEQyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IlJTVSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9XSwiaGFzaCI6IjEwNjQ1NTE3LzAiLCJncmFjZVBlcmlvZERheXMiOjAsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-eNTyizE3kmBWEVd8daP6msWpn1/6mapFOi/fYBbc8LokedHKs0W1P+RNBR7eWPuD8efGE0EI00CydiPSOz+7qFHMaW69aW/2x5JTH3Nb6qIH9qVWCZDi1Sb5BDQxpen5OUVGks6rOtaNkOIAhQMbZyKTEQDd9rg0hUEY0BxhwDdR1zWlCWFL9h0smFWqncVvvt5wX09W4WnepJ+wYvUOgW0gPJTwV1NsCoa5hfgh5tVOKqfiuT3uD1QYYKh1Q6DYAKDMpkkObEt6BAwg7Gdg4MV7/f4R01RSRaZm7JJuoECeRSswzMLipDLMeAXTEAeHOumgZVsofvkhYAGQUuvNXA==-MIIEPjCCAiagAwIBAgIBBTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE1MTEwMjA4MjE0OFoXDTE4MTEwMTA4MjE0OFowETEPMA0GA1UEAwwGcHJvZDN5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQC9WZuYgQedSuOc5TOUSrRigMw4/+wuC5EtZBfvdl4HT/8vzMW/oUlIP4YCvA0XKyBaCJ2iX+ZCDKoPfiYXiaSiH+HxAPV6J79vvouxKrWg2XV6ShFtPLP+0gPdGq3x9R3+kJbmAm8w+FOdlWqAfJrLvpzMGNeDU14YGXiZ9bVzmIQbwrBA+c/F4tlK/DV07dsNExihqFoibnqDiVNTGombaU2dDup2gwKdL81ua8EIcGNExHe82kjF4zwfadHk3bQVvbfdAwxcDy4xBjs3L4raPLU3yenSzr/OEur1+jfOxnQSmEcMXKXgrAQ9U55gwjcOFKrgOxEdek/Sk1VfOjvS+nuM4eyEruFMfaZHzoQiuw4IqgGc45ohFH0UUyjYcuFxxDSU9lMCv8qdHKm+wnPRb0l9l5vXsCBDuhAGYD6ss+Ga+aDY6f/qXZuUCEUOH3QUNbbCUlviSz6+GiRnt1kA9N2Qachl+2yBfaqUqr8h7Z2gsx5LcIf5kYNsqJ0GavXTVyWh7PYiKX4bs354ZQLUwwa/cG++2+wNWP+HtBhVxMRNTdVhSm38AknZlD+PTAsWGu9GyLmhti2EnVwGybSD2Dxmhxk3IPCkhKAK+pl0eWYGZWG3tJ9mZ7SowcXLWDFAk0lRJnKGFMTggrWjV8GYpw5bq23VmIqqDLgkNzuoog==]]></content>
      <categories>
        <category>札记</category>
      </categories>
      <tags>
        <tag>札记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 总结]]></title>
    <url>%2F2019%2F01%2F14%2F2018%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总结成果: 1.承担现数据仓库的部分数据清洗 2.开发数据仓库实时流处理分支(脚数据,用户数据实时接收处理,并通过接口输出) 3.脚数据实时上传接口的维护(小改动,日常跟踪,机房迁移) 4.tensorflow模型(python)的java部署 5.前数据仓库的数据输出接口 6.基于aws-ec2,aws-emr的分布式计算平台(数据分析分布式,模型训练分布式)的开发 7.基于Django的数据输出接口界面 8.楦数据,商品数据按门店分组,并生产到kafka集群中,供商品推荐项目使用 9.临时性的数据分析处理工作,比如楦级放分析等不足: 1.对考勤制度(上班时间)不够严肃,早上迟到次数较多 计划配合业务需要,将数据仓库实时流处理部分做完善,做完美.]]></content>
      <categories>
        <category>札记</category>
      </categories>
      <tags>
        <tag>札记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[affinity readme]]></title>
    <url>%2F2019%2F01%2F11%2Faffinity%2F</url>
    <content type="text"><![CDATA[翻译: 原文 ###构建状态！CircleCI 设计目标 架构 基本原则 Http Layer 序列化 分布式协调 故障转移 状态管理 一致性注释 配置 日志 HOCON配置文件结构 Avro 管理员员 全球国家 Keyspaces 节点上下文 重要的Akka配置选项 指标 开发 交叉构建 当前主版本 Scala 2.11 JavaScript(affinity.js) 设计目标 用于在流媒体基础之上构建有状态的，可扩展数据的API库 可以附加到基于kafka的流处理管道并参与作为生产者/源或者作为消费者/materializer of state 容错(在分布式log之上构建) 水平可扩展和完全异步 零停机时间 架构 ！Cluster Architecture Akka作为主要的api和内部通信协议 Zookeeper用于分布式协调和弹性 Kafka作为容错存储和集成主干 Akka Http作为websocket图层的主界面 RocksDb，MemDb，用于状态存储实现的SimpleMap Avro具有用于所有序列化的中央模式注册表 基本原则Affinity 是一种流处理器，可以被视为一个“微观”世界其中卡夫卡生态系统是“宏观”。它采用线性设计可扩展性，容错性，零停机时间，一致性和弹性。 发布/订阅模型扩展到记录级粒度avro univerese从底层的kafka存储和spark shell(可以将kafka主题作为case classes的RDD)再到javascript实现,都能够从中央schema注册表中获取schema。内置Web服务:基于Akka Http提供大规模，高效且容错的API服务可以为数十亿台设备服务。这样，例如web-socket作为一等公民的客户端可以发布和订阅来自/来自任意数量的键空间内的单个键域的事件使用二进制avro编码器序列化的超高效delta消息。 在内部，以及外部Affinity是一个完全事件驱动的系统。在外面它提供了两个接口：Stream和Http是系统的出入口： Stream接口可用于简单地摄取/处理外部流 Http接口可用于定义访问内部数据的HTTP方法 两个接口都只与内部系统通信akka消息可以很简单 ! Tell flows，? Ask flows 还是 ?? Ask flows (这些是核心模块提供的强类型的Ask with retries)还有其他复杂的内部事件链，which(可能导致零或更多事件被反射回外部系统。) 在Http和Stream Gateway层下面有一个共同的Akka网关,所有的Keyspace都是简单的actor，每个actor管理一套分区 - 任意数量的节点都可以reference相同的Keyspace,这些节点保证完全一致。 应用程序通常扩展通用网关以创建功能特征在键空间周围，然后将它们混合成一个更高阶的特征，在那里进行编排行为可以实施。最终的复合逻辑附加到任一个一个StreamGateway，HttpGateway或两者兼而有之。 代表Keyspace的演员可以在任何网关特征中被引用并参与其中关注用于模仿默认分区的消息路由apache kafka的方案 - 这对于系统的正确运行不是必需的分区方案完全嵌入在Affinity Universe中，但它有所帮助要知道外部kafka生成器可以与其默认分区程序一起使用来创建一个与亲和力的数据视图完全兼容的主题 - 这会有所帮助迁移和其他杂项操作。 Keyspace Actor将所有请求路由到实现逻辑的Partition Actors数据分区 - 每个Keyspace因此是一个动态的Akka路由器使用Coordinator的内部实例维护活动分区Actors的副本(见下面的分布式协调部分)。 该系统只能通过R =复制因子和N =数来驱动简单的分区分配节点，但将来很高兴与以前的状态增加一种共振这将在关闭/启动时保留，直到创建了一定的不平衡状态通过添加或删除节点，增加复制因子等 - 基础工作为此已在MemStore级别完成，请参阅下面的“状态管理”部分。 当Keyspaces和他们的分区时，网关往往是编排层 通常只限于它们的范围，但系统中的任何Actor都可以容纳 对任何已定义的Keyspace的引用，例如，一个Keyspace的单独分区可以 将另一个Keyspace作为一个整体 - 这允许完整的事件链，而不是 有一些可以包装的轻量级事务的实用代码围绕协调逻辑，使用可逆指令来补偿失败的操作但这被放弃了，因为它必须使用一致的分布式锁。 该系统只能通过R =复制因子和N =数来驱动简单的分区分配节点，但将来很高兴与以前的状态增加一种共振这将在关闭/启动时保留，直到创建了一定的不平衡状态通过添加或删除节点，增加复制因子等 - 基础工作为此已在MemStore级别完成，请参阅下面的“状态管理”部分。 当Keyspaces和他们的分区时，网关往往是编排层 通常只限于它们的范围，但系统中的任何Actor都可以容纳 对任何已定义的Keyspace的引用，例如，一个Keyspace的单独分区可以 将另一个Keyspace作为一个整体 - 这允许完整的事件链，而不是 有一些可以包装的轻量级事务的实用代码围绕协调逻辑，使用可逆指令来补偿失败的操作但这被放弃了，因为它必须使用一致的分布式锁。 Akka Patterns标准Akka模式 ! Tell 是一个基本的Akka即发消息模式： ? Ask 是一个带有超时的标准Akka模式，可以从akka.patterns.ask导入 导入io.amient.affinity.core.ack后，还有几个额外的模式： ?? Typed Ask与Ask类似，但只能发送类型为’Reply [T]的消息，响应类型为T` 如果消息的类型为“Scatter [T]”且目标actor是一个组，则该组中的所有actor都将接收并回复T，它将由组合函数组合 ？！键入问题与重试是一样的??确认但是，使用超时隐式作为重试之间的延迟，重试最多3次错误 消息（发件人）！ T如果消息的类型为“Reply [T]”，即键入ask，则此模式可用于响应 消息(发件人) ! Future[T] 与上面相同，但这里它与pipeTo模式结合使用，以便将来的结果在完成后到达发送方 Http Layer开发代码库分为几个模块，其中一些模块可以并且应该独立使用akka核心： api是用于编写memstore和存储插件（Java）的内部编程api和utiliities avro scala case class &lt; - &gt;使用schema注册表包装器进行泛型avro转换（Scala） core是主要的scala库，带有js-avro扩展（Scala） examples / ..包含示例应用程序（Scala） 用于avro模块（Scala）的控制台消费者的kafka / avro-formatter-kafka kafka格式化程序 用于avro模块（Scala）的kafka / avro-serde-kafka kafka produer序列化器和消费者反序列化器 带有kafka存储和二进制流实现的kafka / storage-kafka模块 kafka / test-util-kafka提供EmbeddedZooKeeper，EmbeddedKafka和EmbeddedCfRegistry进行测试 使用RocksStb实现MemStore（Java）的rocksdb模块 spark使用底层流存储作为具有所有serde魔法的CompactRDD 支持avro的ws-client自定义Web套接字（Java）]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>affinity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 十一月]]></title>
    <url>%2F2019%2F01%2F03%2F2018november%2F</url>
    <content type="text"><![CDATA[2018年 11月 01日 星期四 10:07:33 CST昨天试了文档上的两个个kafka stream 的例子 2018年 11月 02日 星期五 09:52:44 CST昨天病了，没做事 ? 数据结构 2018年 11月 05日 星期一 10:51:19 CST 个人认为专业书籍在你能看出来“这翻译的是啥破玩意”之前都可以看中文版。当然某些压根就有翻译错误的除外。 vim 中查找 用 / 回车会定位到第一个匹配到的字符 n:下一个 N:上一个 vim 查找 2018年 11月 06日 星期二 10:42:27 CSThive 解析json 还真是够恶心，语法太死板 hive 卡住一个小时了，不动，吃完饭再说 hive –service hwi 想要正常的启动访问，要经历 下载源码 -&gt; 编译war包 -&gt; 下载各种jar 最后也就那么个玩意，可以看看表结构之类的 查看hadoop等的配置可以用 hive cli 中的set -v 2018年 11月 07日 星期三 10:37:09 CST 拆解后的表的正确性验证json 解析验证：sql 没有报错的情况下，首先是没有明显的错误，但是有两种可能，错误自动处理，真正没有问题， 最好还是自己代码实现，这样可以尽可能多的控制解析过程 history 表合并 一种是通过sql做distinct另外一种是mr进行hash 验证 scala 设计模式 貌似是个大神写的，好好研究下 //TODO //java.lang.RuntimeException: Column type is neither timestamp nor date! 自增使用时间戳会报的错误–incremental append –check-column id –merge-key id –last-value 0 –split-by id -m 8 hive query stuck at 99% If your query is getting stuck at 99% check out following options - Data skewness, if you have skewed data it might possible 1 reducer is doing all the work Duplicates keys on both side - If you have many duplicate join keys on both side your output might explode and query might get stuck One of your table is small try to use map join or if possible SMB join which is a huge performance gain over reduce side join Go to resource manager log and see amount of data job is accessing and writing. 2018年 11月 09日 星期五 15:14:38 CST昨晚喝酒喝到太晚，醉了 2018年 11月 14日 星期三 14:06:34 CST昨天没上班，前天像是睡着了，反正这两天都不在状态 想通过计算每条数据的hash值，来判断任意两条数据是否一致 hash函数/散列函数 wiki:所有散列函数都有如下一个基本特性：如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果，具有这种性质的散列函数称为单向散列函数。但另一方面，散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同，这种情况称为“散列碰撞（collision）”，这通常是两个不同长度的输入值，刻意计算出相同的输出值。输入一些数据计算出散列值，然后部分改变输入值，一个具有强混淆特性的散列函数会产生一个完全不同的散列值。 hash vs md5 MD5消息摘要算法（英语：MD5 Message-Digest Algorithm），一种被广泛使用的密码散列函数，可以产生出一个128位（16字节）的散列值（hash value），用于确保信息传输完整一致。 murmurhash3scala 中 LinearSeqLike.scala 默认的hashCode() 使用的是murmurhash3,是一种查找hash算法： murmurhash是 Austin Appleby于2008年创立的一种非加密哈希算法，适用于基于哈希进行查找的场景。murmurhash最新版本是MurMurHash3，支持32位、64位及128位值的产生。 MurMur经常用在分布式环境中，比如Hadoop，其特点是高效快速，但是缺点是分布不是很均匀。 所以我们想要选出重复数据，还是用加密hash算法比较好 SHA-224、SHA-256、SHA-384，和SHA-512并称为SHA-2。 新的哈希函数并没有接受像SHA-1一样的公众密码社区做详细的检验，所以它们的密码安全性还不被大家广泛的信任。 虽然至今尚未出现对SHA-2有效的攻击，它的算法跟SHA-1基本上仍然相似；因此有些人开始发展其他替代的哈希算法。 哈希函数介绍 2018年 11月 16日 星期五 11:52:40 CST? azkban 新建job let’s go &gt;&gt;&gt; 2018年 11月 17日 星期六 17:43:59 CST总结： todo hive json 解析 hash 使用 2018年 11月 21日 星期三 10:29:31 CST 生成随机字符串：org.apache.commons.lang包下有一个RandomStringUtils类，其中有一个randomAlphanumeric(int length)函数，可以随机生成一个长度为length的字符串。 12345String randomString=RandomStringUtils.randomAlphanumeric(10);println(RandomStringUtils.randomAlphabetic(10))println(RandomStringUtils.randomAlphanumeric(10))println(RandomStringUtils.randomAscii(10)) 生成随机字符串 在一定范围内随机数值（范围内随机经纬度） 123456789def randomLonLat(): List[Double] =&#123; val lonRange = List(22.5114637164,22.5219235145) val latRange = List(113.9546695974,113.9576442599) val randomLon = Math.random() * (lonRange.last - lonRange.head)+ lonRange.head val randomLat = Math.random() * (latRange.last - latRange.head)+ latRange.head randomLon :: randomLat :: Nil &#125; 格式化日期，java8提供了更多支持 1LocalDateTime.now().toString date format 2018年 11月 22日 星期四 10:17:23 CST好老师IBM: Java 8 中的 Streams API 详解 java8 stream 做为高级版本的Iterator,像scala的集合看齐 ** 抽时间好好弄一下apt-get\ 安装什么玩意都装不上 做个客户端，让大家自己动手比较好 2018年 11月 27日 星期二 10:36:52 CSTget akka-http static html\css\js,详见akka-http.md 12345678val staticResources = (get &amp; pathPrefix(&quot;elm&quot;))&#123; (pathEndOrSingleSlash &amp; redirectToTrailingSlashIfMissing(StatusCodes.TemporaryRedirect)) &#123; getFromResource(&quot;elm/index.html&quot;) &#125; ~ &#123; getFromResourceDirectory(&quot;elm&quot;) &#125; &#125; gitlab 迁移同步代码 2018年 11月 30日 星期五 10:40:08 CST倒排索引?Lineage ? 线性 分布式系统原理与实战 1 周 读完 mvn install java8 真是够难用，但是没办法，还是要用，艹 scala 123456val (a,b) = (1,2) # can&apos;t be compiledval List(a,b) = List(1,2) # correctval (a,b) = Tuple2(1,2) # correct]]></content>
      <categories>
        <category>札记</category>
      </categories>
      <tags>
        <tag>札记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 十二月]]></title>
    <url>%2F2019%2F01%2F03%2F2018December%2F</url>
    <content type="text"><![CDATA[2018年 12月 04日 星期二 16:18:05 CST办公室台式机装成了ubuntu18 2018年 12月 05日 星期三 10:47:36 CST上午整理了一下系统，下午与登记结婚了 2018年 12月 06日 星期四 10:47:36 CST用apt-get 安装的scala问题：repl console 输入不会显示 解决办法: 卸载apt-get 安装的版本，重新安装官方版本 To fix the problem completely removed scala and install it with dpkg (not with apt): sudo apt-get remove scala-library scala sudo wget www.scala-lang.org/files/archive/scala-2.11.12.deb sudo dpkg -i scala-2.11.12.deb 完后又有另外一个问题： 打开 repl. console就会报错误：java.lang.NumberFormatException: For input string: “0x100” 解决： export TERM=xterm-color 追加评论 at 2019年 01月 03日 星期四 09:56:26 CST: 输入又不会显示了,不清楚原因,现在干脆在idea中写一个Debugger Object 调试 md,重新安装一遍又解决,暂时不纠结这个问题了 2018年 12月 10日 星期一 15:33:16 CST正则表达式是个好东西，在idea中使用,替换字符很方便 在生产服务器上导出text格式的文件也失败了，错误是一样的， 123456789101112131415161718192021222324252627282918/12/10 16:12:41 ERROR tool.ExportTool: Encountered IOException running export job:ENOENT: No such file or directory at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmodImpl(Native Method) at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmod(NativeIO.java:230) at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:652) at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:490) at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:599) at org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:94) at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:98) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:191) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1297) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1294) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1294) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1315) at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324) at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301) at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442) at org.apache.sqoop.manager.SqlManager.exportTable(SqlManager.java:931) at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:80) at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252) 草，自己实现个mapreduce同步数据 2018年 12月 11日 星期二 10:11:03 CSTjava8 666 12345678910111213141516171819202122232425Legacy:for (int i = 0; i &lt; 10; i++) &#123; System.out.println(i);&#125;Fancy:IntStream.range(0, 10).forEach( nbr -&gt; System.out.println(nbr));Why? ...because the execution of the following snippet takes 1 second and not 10 seconds:IntStream.range(0, 10).parallel().forEach( nbr -&gt; &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException ex) &#123;&#125; System.out.println(nbr); &#125;); 从数据库中导出的文件默认是用’tab键’分割各字段的，将tab键转换从成’,’sed ‘s/\t/,/g’ bdp_order_filterbznstype_12month.csv &gt; bdp_order_filterbznstype_comma_12months.csv 统计文件的行数：wc -l bdp_order_comma_12months.csv linux统计行数方法及效率测试 2018年 12月 12日 星期三 14:02:12 CSTscala 冒泡排序: 1234567891011def insertSort(xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List() case x :: xs1 =&gt; insert(x, insertSort(xs1)) &#125; def insert(x: Int, xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List(x) case y :: ys =&gt; if (x &lt;= y) x :: xs else y :: insert(x, ys) &#125; 1234567891011121314def bubblesort[A &lt;% Ordered[A]](list: List[A]): List[A] = &#123; def sort(as: List[A], bs: List[A]): List[A] = if (as.isEmpty) bs else bubble(as, Nil, bs) def bubble(as: List[A], zs: List[A], bs: List[A]): List[A] = as match &#123; case h1 :: h2 :: t =&gt; if (h1 &gt; h2) bubble(h1 :: t, h2 :: zs, bs) else bubble(h2 :: t, h1 :: zs, bs) case h1 :: Nil =&gt; sort(zs, h1 :: bs) &#125; sort(list, Nil)&#125; 2018年 12月 13日 星期四 16:00:39 CST将表从hive export -&gt; mysql这种扫操作 不玩了 ubuntu上也有7z666sudo apt-get install p7zip-full 解压：7z x ****.7z 数据库连接池传统的有c3p0\dbcp ali 开源的druid 到现在没有用着太顺手的数据库连接器 //TODO sql 注入 volatile: 在程序设计中，尤其是在C语言、C++、C#和Java语言中，使用volatile关键字声明的变量或对象通常具有与优化、多线程相关的特殊属性。通常，volatile关键字用来阻止（伪）编译器认为的无法“被代码本身”改变的代码（变量/对象）进行优化。如在C语言中，volatile关键字可以用来提醒编译器它后面所定义的变量随时有可能改变，因此编译后的程序每次需要存储或读取这个变量的时候，都会直接从变量地址中读取数据。如果没有volatile关键字，则编译器可能优化读取和存储，可能暂时使用寄存器中的值，如果这个变量由别的程序更新了的话，将出现不一致的现象。 2018年 12月 17日 星期一 10:23:19 CST使用dbcp+dbutils连接hive 不成功 kafka-connect -&gt; hdfs 单向路并且一般场景中也难有hive -&gt; kafka的情况 回去老老实实作jdbc原始人,有时间再搞这些 scala中的implicit也真是够折磨人的 scala 中spray-json使用中除了引入自己定义的case class 的 implict json 以外,还要导出 1import spray.json._ 2018年 12月 18日 星期二 09:26:13 CSTmaven: mvn cleanmvn install mvn clean install -e -U-e 详细异常 -U强制更新 Try:函数式错误处理 Try 有两个子类型：Success[A]：代表成功的计算。封装了 Throwable 的 Failure[A]：代表出了错的计算。 对比Option[A] Option[A] 是一个可能有值也可能没值的容器， Try[A] 则表示一种计算： 这种计算在成功的情况下，返回类型为 A 的值，在出错的情况下，返回 Throwable 。 这种可以容纳错误的容器可以很轻易的在并发执行的程序之间传递。 hive 通过ThriftServer,jdbc连接失败,常见错误是hiveserver2进程没有启动 1234567891011121314解决方法：1. 检查hive server2是否启动： netstat -anp |grep 100002. 检查conf/hive-site.xml 下的配置是否正确： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.206.128&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hive export orc Can Sqoop export ORC? HOW TO SQOOP EXPORT A HIVE ORC TABLE TO A ORACLE DATABASE? Hope you must have some idea about Hive ORC tables. Let’s create a ORC table here and feed some sample data in Hive. Similarly let’s create a table structure in ORACLE in the meantime. We are going to use Sqoop-HCatalog Integration here. Just type “sqoop export help” in Bash and see what are all the sqoop parameter commands there for the Sqoop Export related to HCatalog. I got the details as below. HCatalog arguments: –hcatalog-database HCatalog database name –hcatalog-home Override $HCAT_HOME –hcatalog-table HCatalog table name –hive-home Override $HIVE_HOME –hive-partition-key Sets the partition key to use when importing to hive –hive-partition-value Sets the partition value to use when importing to hive –map-column-hive Override mapping for specific column to hive types. 2018年 12月 19日 星期三 14:34:26 CST 数据导入开发环境,mysql时报错,full```1234567891011121314151617181920212223242526272829303132333435363738394041一查,还真是磁盘满了,因为mysql的默认data 存放目录是在 /var/lib/mysql下系统装在了小小的ssd上,那么问题来了[How to change MySQL data directory?](https://stackoverflow.com/questions/1795176/how-to-change-mysql-data-directory)需要注意的地方是,mysqld 的配置文件在/etc/mysql/mysql.conf.d/mysqld.cnf中---kafka connectfile -&gt; kafka topicmysql -&gt; kafka topic----## 2018年 12月 20日 星期四 15:35:29 CST[java &lt;-&gt; scala ](https://stackoverflow.com/questions/16583265/how-can-i-convert-scala-map-to-java-map-with-scala-float-to-java-float-k-v-conve)今天碰上一个空指针异常,妈呀,竟然不知道怎么弄,null呀,呀呀---[Join on foreign key in Kafka stream](https://stackoverflow.com/questions/53260817/join-on-foreign-key-in-kafka-stream)//TODO: kafka ProducerRecord usage: public class ProducerRecord&lt;K, V&gt; { private final String topic; private final Integer partition; private final Headers headers; private final K key; private final V value; private final Long timestamp;123456timestamp headers partitons---------------------------偏函数(PartialFuction[A,B]) --并非定义所有输入的函数 {case 1 =&gt; 1case 2 =&gt; 2} 12scala () exp:(1,2) class:Tuple2 scala&gt; a: (Int, Int) = (0,10) scala&gt; res0: Class[_ &lt;: (Int, Int)] = class scala.Tuple2$mcII$sp 1234567## 2018年 12月 21日 星期五 10:14:25 CST[Counting Number of messages stored in a kafka topic](https://stackoverflow.com/questions/41792703/counting-number-of-messages-stored-in-a-kafka-topic/41799011) bin/kafka-run-class.sh kafka.tools.GetOffsetShell –broker-list localhost:9092,localhost:9093,localhost:9094 –topic test-topic –time -1 –offsets 1 | awk -F “:” ‘{sum += $3} END {print sum}’ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162---alt &lt;- :chrome 页面后退-------------------[pdf 合并\分割](https://www.ilovepdf.com/)---jmap(JVM Memory Map)命令用于生成heap dump文件，如果不使用这个命令，还阔以使用-XX:+HeapDumpOnOutOfMemoryError参数来让虚拟机出现OOM的时候·自动生成dump文件---开机启动还是没有搞明白ubuntu 运行级别查看runlevel---## 2018年 12月 24日 星期一 11:21:08 CSTsbt run 执行时,默认就会跟sbt一个jvm特别容易OOM(GC error)在build.sbt 中加配置 :fork:=true,单独开jvm运行独立进程在build.sbt中配置的javaoptions不一定能用,有时间再研究下sbt 0.13被官方抛弃了,不再维护了,桑心,该学习sbt 1.x了---//Todo:sbt dist javaoptions(jvm)kafka producer metadata---## 2018年 12月 26日 星期三 17:30:22 CSTmain app 在idea中build.run都没有问题,但是在在使用本地安装的sbt dist打包时报错:java.lang.StackOverflowErroridea 中run main 时,idea 另外开了一个jvm run main 线程,使用的也是配置的jdk,参数也是jdk默认的jvm参数,这个参数较大,默认max heap :4G,每个线程默认的stack也应该比较大而.sbt 默认的jvm参数都比较小,所以才有了这一幕,但是我到底写了什么东西,让默认的stack 满了.好好查下解决方法: xxh@xxh-YangTianM4900c-00:/codeold/footinfoloaderbj$ whereis sbtsbt: /usr/local/sbt /home/xxh/bin/sbt /usr/local/sbt/bin/sbt /usr/local/sbt/bin/sbt.batxxh@xxh-YangTianM4900c-00:/codeold/footinfoloaderbj$ cd /usr/local/sbt/xxh@xxh-YangTianM4900c-00:/usr/local/sbt$ kkkk：未找到命令xxh@xxh-YangTianM4900c-00:/usr/local/sbt$ ;;bash: 未预期的符号 `;;’ 附近有语法错误xxh@xxh-YangTianM4900c-00:/usr/local/sbt$ lbin/ conf/ lib/ sbt* xxh@xxh-YangTianM4900c-00:/usr/local/sbt$ cd conf/xxh@xxh-YangTianM4900c-00:/usr/local/sbt/conf$ ll总用量 16drwxrwxr-x 2 xxh xxh 4096 12月 4 17:40 ./drwxrwxr-x 5 xxh xxh 4096 12月 4 17:37 ../-rw-rw-r– 1 xxh xxh 255 12月 4 17:40 sbtconfig.txt-rw-rw-r– 1 xxh xxh 939 7月 27 2017 sbtoptsxxh@xxh-YangTianM4900c-00:/usr/local/sbt/conf$ cat sbtopts ————————————————The SBT Configuration file.————————————————Disable ANSI color codes# #-no-colors Starts sbt even if the current directory contains no sbt project.# -sbt-create Path to global settings/plugins directory (default: ~/.sbt)# #-sbt-dir /etc/sbt Path to shared boot directory (default: ~/.sbt/boot in 0.11 series)# #-sbt-boot ~/.sbt/boot Path to local Ivy repository (default: ~/.ivy2)# #-ivy ~/.ivy2 set memory options# #-mem Use local caches for projects, no sharing.# #-no-share Put SBT in offline mode.# #-offline Sets the SBT version to use.#-sbt-version 0.11.3 Scala version (default: latest release)# #-scala-home #-scala-version java version (default: java from PATH, currently $(java -version |&amp; grep version))# #-java-home xxh@xxh-YangTianM4900c-00:/usr/local/sbt/conf$ vim sbtopts =&gt; ————————————————The SBT Configuration file.————————————————Disable ANSI color codes# #-no-colors Starts sbt even if the current directory contains no sbt project.# -sbt-create Path to global settings/plugins directory (default: ~/.sbt)# #-sbt-dir /etc/sbt Path to shared boot directory (default: ~/.sbt/boot in 0.11 series)# #-sbt-boot ~/.sbt/boot Path to local Ivy repository (default: ~/.ivy2)# #-ivy ~/.ivy2 set memory options# #-mem Use local caches for projects, no sharing.# #-no-share Put SBT in offline mode.# #-offline Sets the SBT version to use.#-sbt-version 0.11.3 Scala version (default: latest release)# #-scala-home #-scala-version java version (default: java from PATH, currently $(java -version |&amp; grep version))# #-java-home #重点 ###################-J-Xmx6G-J-Xss2M ################### 重新sbt dist ,ok --- 本来打算用&quot;|&quot;做两个字符串的连接符,然后再分割,没想到用String.split分割不到 原来是因为: &gt; 1）split表达式，其实就是一个正则表达式。* ^ | 等符号在正则表达式中属于一种有特殊含义的字符，如果使用此种字符作为分隔符，必须使用转义符即\加以转义。 2）如果使用多个分隔符则需要借助 | 符号，如【2】所示，但需要转义符的仍然要加上分隔符进行处理。 如果要用&quot;|&quot;就得转义 --- # 总结: - 十二月下旬非常充实,临危受命,接下实时流处理的任务,这也是我一直以来很感兴趣的 - 用了一下akka,效率的确会提升,但是很容易oom,还是没掌握好, 并且我的这个场景并不适合使用akka,(akka的应用范围可真是窄呀),还是好好掌握好Future,Promise,java的多线程 - java8新特性没有看完,但是lambda会用了,虽然相较于scala的map略微有点生硬,但是还是生硬 - sed 处理文本文件效率还是很高的 - jvm相关 - sbt0.13被抛弃了,改换sbt1.0 # TODO Jvm kafka 深入]]></content>
      <categories>
        <category>札记</category>
      </categories>
      <tags>
        <tag>札记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式理论基础]]></title>
    <url>%2F2018%2F12%2F13%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[CAP分布式系统理论基础 - CAP 1 概述1.1 分布式系统的定义分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像是单个相关系统 1.2 目标1.2.1 使资源可访问1.2.2 透明性1.2.3 开放性1.2.4 可扩展性1.3 分布式系统的类型1.3.1 分布式计算系统1.3.1.1 集群计算系统1.3.1.2 网格计算系统1.3.2 分布式信息系统1.3.2.1 事务处理系统1.3.2.2 企业应用集成1.3.3 分布式普适系统2 体系结构2.1 体系结构的样式 分层 基于对象 以数据为中心 基于事件 2.1 系统体系结构3 进程3.1 进程3.2 虚拟化3.3 客户3.4 服务器3.4.2 服务器集群4 通信4个广泛使用的通信模型 远程过程调用(remote procedure call RPC) 远程方法调用(remote method invocation RMI) 面向消息的中间件(message-oriented middleware MOM) 流(stream) 4.1 基础知识]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 笔记]]></title>
    <url>%2F2018%2F12%2F07%2Fhive%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[好久没用hive了 三百多万数据，入库kafka 存储格式hive 从两个维度对表进行管理，分别是行格式（row format）和文件格式(file format) 行格式是指一行中的字段如何储存按照hive的术语，行格式有serde(serializer-deserializer) hive 实践 hive conf 目录下自带的tmp配置文件,hive-default.xml -&gt; hive-site.xml 12345678910hive-site.xml and hive-default.xml.templatehive-default.xml.template contains the default values for various configuration variables that come prepackaged in a Hive distribution. In order to override any of the values, create hive-site.xml instead and set the value in that file as shown above.hive-default.xml.template is located in the conf directory in your installation root, and hive-site.xml should also be created in the same directory.Please note that the template file hive-default.xml.template is not used by Hive at all (as of Hive 0.9.0) – the canonical list of configuration options is only managed in the HiveConf java class. The template file has the formatting needed for hive-site.xml, so you can paste configuration variables from the template file into hive-site.xml and then change their values to the desired configuration.In Hive releases 0.9.0 through 0.13.1, the template file does not necessarily contain all configuration options found in HiveConf.java and some of its values and descriptions might be out of date or out of sync with the actual values and descriptions. However, as of Hive 0.14.0 the template file is generated directly from HiveConf.java and therefore it is a reliable source for configuration variables and their defaults.The administrative configuration variables are listed below. User variables are listed in Hive Configuration Properties. As of Hive 0.14.0 you can display information about a configuration variable with the SHOW CONF command. hive jdbc 连接特普通 sbt mvn下依赖(jdbc驱动) Class.forName… DriverManager…. mysql 创建新用户设定密码报错，检查密码的安全度，想到我这操蛋的记忆力还是把这个检查关了吧 our password does not satisfy the current policy requirements 1234Here is what I do to remove the validate password plugin:1. Login to the mysql server as root mysql -h localhost -u root -p2. Run the following sql command: uninstall plugin validate_password; hive 安装推荐首席hive 安装参考推荐次之hive 安装参考2 错误： beeline&gt; !connect jdbc:hive2//localhost:10000scan complete in 0ms18/11/05 15:06:11 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: minlog-1.2.jar (No such file or directory)18/11/05 15:06:11 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: objenesis-1.2.jar (No such file or directory)18/11/05 15:06:11 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: reflectasm-1.07-shaded.jar (No such file or directory)scan complete in 670msNo known driver to handle “jdbc:hive2//localhost:10000” 解决： 其实这个问题是由于jdbc协议地址写错造成的，在hive2之后少了个“：”改成以下这个形式即可：beeline&gt; !connect jdbc:hive2://h2slave1:10000 jdbc 连接 报错： 1Required field &apos;client_protocol&apos; is unset 解决： 123456789//使用的jdbc lib 版本比 server端高，一般来讲 client端要比server端版本低才可以由于server端版本是1.2.2于是将libraryDependencies += &quot;org.apache.hive&quot; % &quot;hive-jdbc&quot; % &quot;2.1.1&quot;-&gt;libraryDependencies += &quot;org.apache.hive&quot; % &quot;hive-jdbc&quot; % &quot;1.2.2&quot;问题解决 hive 执行sql 会碰到hdfs权限问题，赋予其权限即可 hive –service hwi 想要正常的启动访问，要经历 下载源码 -&gt; 编译war包 -&gt; 下载各种jar 最后也就那么个玩意，可以看看表结构之类的 解析json 自带函数 开源json 序列化 hive 建表语句，没有指定默认值的关键字Hive table creation with a default value hive 查看表信息：内部表还是外部表 12345describe extended tablename；-- 主要是表字段信息desc formatted tablename；-- 主要是表字段信息show create table tablename; --查看table的存储路径等信息 ? alter table *** location ‘’ 执行不成功，问题没解决，直接重新建表了]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database,hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Amazon Redshift Learning]]></title>
    <url>%2F2018%2F12%2F04%2FAmazon%20Redshift%20Learning%2F</url>
    <content type="text"><![CDATA[overview 文档中有设计表、加载数据的最佳实践，从这里可以查看到redshift与其他数据库最主要的差别 它是一个企业级的关系数据库查询与管理系统 支持与多种应用程序客户端建立连接 表面上我们执行了一个简单ｑｕｅｒｙ,Redshift 在后台在多阶段中检索、比较、和计算大量的数据，最终产生结果，后台的操作对用户都是透明的 优点:高效存储和最优的查询性能 技术支撑：并行处理、列式数据存储和非常高效的具有针对性的数据压缩编码方案的组合 Amazon Redshift is based on PostgreSQL 8.0.2. postgresql vs mysql就基本的应用来讲，二者无差别，当然一定有sql方言方便的分别，记得注意]]></content>
      <categories>
        <category>数据存储处理 - database</category>
      </categories>
      <tags>
        <tag>Redshift</tag>
        <tag>Amazon</tag>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIY Cluster]]></title>
    <url>%2F2018%2F12%2F04%2FDIY-cluster%2F</url>
    <content type="text"><![CDATA[需求单机应用分布式执行 akka cluster单台机器也可用dispatcher提高机器利用率,这是后话，先按照要求做出一个集群来 desc不同于actor点对点的交互，而是有独特的 akka cluster机制 集群定义： 维基百科上的定义是 “计算机集群由一系列间接或直接连接的计算机组成,这些计算机共同完成工作,从很多角度上来说,可以将这些计算机看作是单个系统。具体到本章中对于集群的定义,我们可以认为一个集群就是一组 机器(也可能是虚拟机),我们将这些机器称作节点(node)或是成员(member),同一个组中的所有节点遵循某个协议。 巨型单体服务vs微服务Martin Fowler的观点： 1. 当团队较小，开发的应用程序也比较小的时候，建议使用单体应用，随着应用程序变大，团队内的开发人员变多时，可以试着查分单体为微服务。 2. 当查分单体应用时，如果没有在开发单体时的经验，很可能把错误的部分拆分出去 一旦有了些在生产环境中运行系统的经验和数据,要把哪个部分分离出来才能够在 新增功能时带来性能和团队开发速度的好处就会比较清楚了。开始部署微服务之后,我 们就可以用一种非平衡的方式对负载更高的服务进行扩展,给应用程序中使用更多的部 分分配更大的服务器集群。 集群定义集群就是一组可以相互通信的服务器必须的两个条件： 失败发现 以及使得集群中的所有成员最终能够提供一致的视图(什么意思) 失败检测节点基于能否获得其他节点的响应来确定节点是否可用 akka中，每个节点仅仅检测附近的节点，以节省网络开销，提高效率 通过gossip协议达到最终一致思考一下失败检测的处理,集群中的每个节点和它的邻居节点互相交换已知的状态。接着,这些邻居节点再将已知的状态传递给它们的“邻居”,以此往复,直到某个节点的已知状态传递给了集群中的所有成员。节点会仔细计算并得到集群中其他成员的状态 CAPC一致性 A可用性 p分区容错性 CAP理论的妥协分布式系统只能选择满足其中两个 使用Akka Cluster 构建系统Akka Cluster 基于 Akka Remoting,解决了实际应用中的高可用性等问题 场景比较简单，可以直接使用Akka Remoting 来构建系统的alpha版本官网上提供了akka-samples-remote-scala的例子，先学习一下 通读第一遍 看了下look up 的例子，真是不错，看来以后研究开源框架，还是要直接上官网看文档 原始人：ssh太原始了，慎重 研究了devops工具，听人劝，使用ansible更可贵的是ansible包含了对aws的支持]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bandwagon shadowsocks 配置使用]]></title>
    <url>%2F2018%2F12%2F04%2FBandwagon-shadowsocks-%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[好几个同事都来问我socks5怎么搭建的，现写下来 bandwagon vps 搬瓦工VPS优惠码(不知道有没有邀请码，让我也薅一根羊毛): BWH1ZBPVK（6.00%） bandwagon官网购买vps,支持支付宝(alipay),有人买的便宜的那一款网络状况堪忧，推荐购买’SPECIAL 10G KVM PROMO V3 - LOS ANGELES - CN2’代理访问网页足够了 shadowsocks server 通过网页bash或者本地终端远程连接vps,按照shadowsocks doc安装过程中可能碰到python、pip问题，不同的vps、不同的os遇见的问题可能会不一样，google解决方法(真实的的世界就是如此) 启动shadowsocks server推荐blog 配置文件1234567891011vim /etc/shadowsocks/config.json&#123; &quot;server&quot;:&quot;::&quot;, &quot;server_port&quot;:8388, //自定义端口 &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;mypassword&quot;,//设置shadowsocks密码，client配置该密码 &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, &quot;fast_open&quot;: false&#125; 启动服务 1ssserver -c /etc/shadowsocks/config.json 可以使用nohup &amp; 后台运行 shadowsocks clientshadowsocks 官网客户端下载配置，提供了各个平台的客户端 客户端启动：conf: 123456789&#123; &quot;server&quot;:&quot;112.11.11.11&quot;, //vps ip &quot;server_port&quot;:8032, // shadowsocks port &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:34196,// 设置本地端口 &quot;password&quot;:&quot;0juAtjMZVQ&quot;, //shadowsocks password &quot;timeout&quot;:600, &quot;method&quot;:&quot;aes-256-cfb&quot; //method 保持一致&#125; 1nohup /usr/bin/python /usr/bin/sslocal -c /etc/shadowsocks.json &amp; More info: Deployment]]></content>
      <categories>
        <category>工欲善其事，必先利其器</category>
      </categories>
      <tags>
        <tag>Bandwagon</tag>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Future&Promise]]></title>
    <url>%2F2018%2F12%2F04%2FFuture%26promise%2F</url>
    <content type="text"><![CDATA[Future和Promisescala 并发编程的两大利器：Future、Actor 顺序代码效率低这个例子很不错 What is Future scala.concurrent 包里的 Future[T] 是一个容器类型，代表一种返回值类型为 T 的计算。 计算可能会出错，也可能会超时； 从而，当一个 future 完成时，它可能会包含异常，而不是你期望的那个值。 Future 只能写一次： 当一个 future 完成后，它就不能再被改变了。 同时，Future 只提供了读取计算值的接口，写入计算值的任务交给了 Promise， 这样，API 层面上会有一个清晰的界限。 这篇文章里，我们主要关注前者，下一章会介绍 Promise 的使用。]]></content>
      <categories>
        <category>服务器端 - 并发编程</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>Future</tag>
        <tag>Promise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[H2数据库笔记]]></title>
    <url>%2F2018%2F12%2F04%2FH2%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[H2工具类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859object H2Database extends LogSupport &#123; private var server: Server = null; private var webServer: Server = null; def start(): Unit = &#123; try &#123; Server.shutdownTcpServer(s&quot;tcp://$&#123;ProConfig.h2DatabaseHost&#125;:$&#123;ProConfig.h2DatabaseTcpPort&#125;&quot;, &quot;&quot;, true, true) &#125; catch &#123; case e: Throwable =&gt; &#125; try &#123; server = Server.createTcpServer(&quot;-tcp&quot;, &quot;-tcpAllowOthers&quot;, &quot;-tcpPort&quot;, ProConfig.h2DatabaseTcpPort.toString).start() log.info(&quot;server status: &quot; + server.getStatus) &#125; catch &#123; case e: Throwable =&gt; log.error(e.getMessage, e) &#125; &#125; def startWebConsole(openBrowser: Boolean = false): Unit = &#123; try &#123; if (webServer == null) &#123; webServer = Server.createWebServer(&quot;-web&quot;, &quot;-webAllowOthers&quot;, &quot;-webPort&quot;, ProConfig.h2DatabaseWebPort.toString).start() &#125; log.info(&quot;webServerStatus&quot; + webServer.getStatus) if (openBrowser) Server.openBrowser(s&quot;http://$&#123;ProConfig.h2DatabaseHost&#125;:$&#123;ProConfig.h2DatabaseWebPort&#125;&quot;) &#125; catch &#123; case e: Throwable =&gt; log.error(e.getMessage, e) &#125; &#125; def stop(): Unit = &#123; def shutdownServer(server: Server): Unit = &#123; try &#123; server match &#123; case s if s != null =&gt; s.stop() case _ =&gt; log.info(&quot;H2 server did not start&quot;) &#125; &#125; catch &#123; case e: Throwable =&gt; log.error(e.getMessage, e) &#125; &#125; shutdownServer(server) shutdownServer(webServer) try&#123; Server.shutdownTcpServer(s&quot;tcp://$&#123;ProConfig.h2DatabaseHost&#125;:$&#123;ProConfig.h2DatabaseTcpPort&#125;&quot;,&quot;&quot;,true,true) log.debug(&quot;serverStatus: &quot;+server.getStatus) &#125;catch &#123; case e:Throwable =&gt; log.error(e.getMessage,e) &#125; &#125;&#125;object H2Test extends App &#123; H2Database.start() Thread.sleep(10000) H2Database.stop()&#125; JDBC开启H2数据库后可按照JDBC 正常连接 CSVWRITE: 导出表成为 CSV 文件1CALL CSVWRITE(&apos;/home/xxhbak/FEATURECONF.csv&apos;, &apos;SELECT * FROM FEATURECONF&apos;); CSVREAD:向表导入CSV文件使用数据库读取CSV文件 CSVREAD使用: 1SELECT * FROM CSVREAD(&apos;test.csv&apos;); Another option is to use INSERT INTO … SELECT. 123CREATE TABLE TEST AS SELECT * FROM CSVREAD(&apos;test.csv&apos;);CREATE TABLE TEST(ID INT PRIMARY KEY, NAME VARCHAR(255)) AS SELECT * FROM CSVREAD(&apos;test.csv&apos;); Please note for performance reason, CSVREAD should not be used inside a join. Instead, import the data first (possibly into a temporary table), create the required indexes if necessary, and then query this table. 参考 示例，亲测 CALL CSVREAD(‘/home/xd/us.csv’, ‘SELECT * FROM us ‘);insert into us (SELECT * FROM CSVREAD(‘/home/xd/USERCASERESULT.csv’,NULL,’UTF-8’,’|’));]]></content>
      <categories>
        <category>数据存储处理 - database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>H2</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 权威指南 3th Edition Part 1]]></title>
    <url>%2F2018%2F12%2F04%2FHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97V3-p1%2F</url>
    <content type="text"><![CDATA[第一章 数据！数据！ 这个世界数据存量很大，数据增长的速度很快，数据的价值很大 数据存储于分析 现有硬件存储容量上升，但是读写速率还是不够高，急需要并发从多个硬盘读取数据 硬盘故障解决：数据冗余备份(RAID,HDFS) 数据并发读取的正确性MapReduce编程模型将数据读写问题抽象转换成数据集的计算，并且是可靠的 相较于其他系统的优势 用蛮力处理整个庞大数据集或数据集的一部分 关系型数据库管理系统 现象：寻址速率的提升远远不及传输速率的提升 B树这有在小量数据时有优势，数据量大时，更新整个B树要花大量时间 RDBMS与hadoop相互补充，hadoop适合批处理，而rdbms适合点查询更新 RDBMS被索引后，能够提供低延迟的查询和少量的数据更新，mapreduce适合一次写入，多次读取做数据分析 RDBMS适合持续更新的数据集 Table 1-1. RDBMS compared to MapReduce item Traditional RDBMS MapReduce Data size Gigabytes Petabytes Access(数据存取) Interactive and batch Batch Updates Read and write many times Write once, read many times Structure Static schema Dynamic schema Integrity(完整性) High Low Scaling Nonlinear Linear 存储的数据不同：RDBMS:结构化数据，完整的且不含冗余hadoop:结构化、半结构化、非结构化数据 网格计算， mapreduce 尽量使得数据本地化 志愿计算： 志愿计算为计算所花的时间远远超过工作单元数据传输时间，也就是说志愿者贡献的是CPU周期，而不是网络带宽 Hadoop发展简史 o 好牛b哦 hadoop echosystem common 一系列组件和接口 avro 序列化系统 maoreduce 计算引擎 HDFS 分布式文件系统 hive 数据仓库，sql查询hdfs hbase 列存储数据仓库 zookeeper 分布式、可用性高的协调服务 sqoop:rdbms &lt;=&gt; hdfs oozie 调度作业 hadoop 发行版本 Table 1-2. Features supported by Hadoop release series Feature 1.x 0.22 2.x Secure authentication Yes No Yes Old configuration names Yes Deprecated Deprecated New configuration names No Yes Yes Old MapReduce API Yes Yes Yes New MapReduce API Yes (with somemissing libraries) Yes Yes MapReduce 1 runtime (Classic) Yes Yes No MapReduce 2 runtime (YARN) No No Yes HDFS federation No No Yes HDFS high-availability No No Yes 第二章气象数据处理 2.1 气象数据集首先要搞清楚数据格式 文本数据 每一行是一条记录 整个数据集有很多小文件组成 2.2 Unix 处理方式12345678910#!/usr/bin/env bashfor year in all/*doecho -ne `basename $year .gz`&quot;\t&quot;gunzip -c $year | \awk &apos;&#123; temp = substr($0, 88, 5) + 0;q = substr($0, 93, 1);if (temp !=9999 &amp;&amp; q ~ /[01459]/ &amp;&amp; temp &gt; max) max = temp &#125;END &#123; print max &#125;&apos;done 循环使用awk处理 单进程处理 2.3 hadoop处理方式map: 1234567891011121314151617181920212223242526272829import java.io.IOException;importimportimportimportorg.apache.hadoop.io.IntWritable;org.apache.hadoop.io.LongWritable;org.apache.hadoop.io.Text;org.apache.hadoop.mapreduce.Mapper;public class MaxTemperatureMapperextends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;private static final int MISSING = 9999;@Overridepublic void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException &#123;&#125;&#125;String line = value.toString();String year = line.substring(15, 19);int airTemperature;if (line.charAt(87) == &apos;+&apos;) &#123; // parseInt doesn&apos;t like leading plus signsairTemperature = Integer.parseInt(line.substring(88, 92));&#125; else &#123;airTemperature = Integer.parseInt(line.substring(87, 92));&#125;String quality = line.substring(92, 93);if (airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;)) &#123;context.write(new Text(year), new IntWritable(airTemperature));&#125; reduce: 1234567891011121314151617import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MaxTemperatureReducerextends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;@Overridepublic void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context)throws IOException, InterruptedException &#123;&#125;&#125;int maxValue = Integer.MIN_VALUE;for (IntWritable value : values) &#123;maxValue = Math.max(maxValue, value.get());&#125;context.write(key, new IntWritable(maxValue)); main: 123456789101112131415161718192021222324import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MaxTemperature &#123;public static void main(String[] args) throws Exception &#123;if (args.length != 2) &#123;System.err.println(&quot;Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;&quot;);System.exit(-1);&#125;Job job = new Job();job.setJarByClass(MaxTemperature.class);job.setJobName(&quot;Max temperature&quot;);FileInputFormat.addInputPath(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1]));job.setMapperClass(MaxTemperatureMapper.class);job.setReducerClass(MaxTemperatureReducer.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(IntWritable.class);System.exit(job.waitForCompletion(true) ? 0 : 1);&#125;&#125; 2.4 横向扩展2.5 Hadoop Streaming]]></content>
      <categories>
        <category>数据存储处理 - hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 权威指南 3th Edition Part 2]]></title>
    <url>%2F2018%2F12%2F04%2FHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97V3-p2%2F</url>
    <content type="text"><![CDATA[第三章 Hadoop 分布式文件系统数据多了单台服务器的文件系统存不下了，就要将数据分区存储在多台服务器上，就需要分布式文件系统 3.1 HDFS设计,the design of hdfs设计目标： HDFS 以流式数据访问模式来存储超大文件，运行于商用硬件集群上 超大文件 流式数据访问 商用硬件 低延迟 大量小文件 多用户写入，任意修改文件 3.2 HDFS Concepts 概念blocks类似于磁盘的块，是最小的读写单位 与单一磁盘上的文件系统类似，HDFS上的文件也被划分为多个块大小的多个分块 hdfs 中小于块大小的文件不会占用整个块空间 fsck指令可以显示块信息 1hadoop fsck / -files -blocks namenode &amp; datanode管理者－工作者模式 一个namenode,多个datanode namenode 管理文件系统的命名空间，他维护者文件系统树及整棵树内所有的文件和目录，这些信息以两个文件的方式永久保存在本地磁盘上：命名空间镜像文件和编辑日志文件 每个文件中各个块所在的数据节点的信息 client作为代理通过于namenode和datanode交互来访问整个文件系统 datanode 存储、检索文件的块，并定期向namenode发送它们所存储的块的列表 namenode的容错很重要， 备份文件 辅助namenode 联邦HDFSnamenode横向扩展 HDFS高可用性HA 命令行接口hadoop fs -help hadoop文件系统hadoop 有一个抽象的文件系统概念，hdfs只是其中的一个实现。java抽象类org.apache.hadoop.fs.FileSystem定义了一个文件系统接口， 基于该抽象类的几个具体实现： java接口FileSystem类]]></content>
      <categories>
        <category>数据存储处理 - hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase使用]]></title>
    <url>%2F2018%2F12%2F04%2FHbase%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[简介HBase 应用场景及特点特点： 海量数据存储 关系型数据库： 列数不超过30 单表行数据量一般不会超过500万vshbase:上百亿行*上百万列 准实时查询 百毫秒 面向列 面向列的存储和权限控制，列是可以动态增加的，并且列支持独立索引。这样在查询行中少数几个字段时，能大大减少读取的数据量 多版本 每一列的数据存储有多个Version 稀疏性：为空的列不占用存储空间，表可以设计的非常稀疏 扩展性： 底层依赖HDFS 高可靠性 高性能应用场景： 交通 金融 电商 电信 概念与定位数据模型Hbase表可以当成一个多维的Map Table一个表有多行 Row在HBase里，row是由row key和一个或多个列及其值组成。数据在被存储的时候，是按row key的字母排序存储的。因此，row key的设计是非常重要的。这样存储数据的目的就是为了让相关的行存储在相邻的位置。 Column（列）HBase的一列由列簇和列修饰符组成，他们通常由冒号（：）分隔。 Column Family(列簇)通常为了性能原因的考虑，列簇会把列和相应的值物理上联合在一起 每个列簇是一个存储属性的集合，例如它的值是否应该缓存在内存中，它的数据是如何压缩或它的row key是如何编码，以及其他。表中的每个行都有相同的列簇，但是一行也可能没有存储一个列簇里的任何数据。 Column Qualifier列修饰符被加到一个列簇，以提供对一个数据片段的索引。规定一个列簇为content，一个列修饰符可能是content:html，另外一个可能是content:pdf。尽管列簇是在创建表的时候固定了，但列修饰符是可变的，在行之间可能存在很大的不同。 Cell一个Cell是行，列簇和列修饰符的组合，并且包含一个值和时间戳，时间戳代表着值的版本。 Timestamp一个时间戳是連同值一起被写入的，是值版本的唯一标识，默认情况下，时间戳表示数据写入时RegionServer的时间，但是当你在写数据到Cell的时候，你可以指定一个不同的时间戳。 安装官网文档getstart写的通俗易懂 增加配置-&gt; conf/hbase-site.xml,以使用伪分布式 1234&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 增加配置 -&gt; conf/hbase-env.sh，以使用独立安装的zk 1export HBASE_MANAGES_ZK=false 过程算是比较顺畅的]]></content>
      <categories>
        <category>数据存储处理 - database</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[https Certificate配置]]></title>
    <url>%2F2018%2F12%2F04%2FHttps%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[用在aliyun上申请到的https Certificate配置到akka-http服务器上，并将应用部署到aws ec2上 certificate证书是同事发给我的，是在aliyun上申请的，绑定了公司的域名，只能运行在配置了公司域名（二级、三级、、、）的服务器上，也就是说不能本地调试，略麻烦 1-rw-rw-r-- 1 xxh xxh 4083 12月 6 17:22 214043592950638.zip unzip 后是两个文件，一个是.key文件即私钥文件，另一个是.pem文件即证书文件 12-rw-rw-r-- 1 xxh xxh 1675 6月 20 17:36 214043592950638.key-rw-rw-r-- 1 xxh xxh 3892 6月 20 17:36 214043592950638.pem 生成.pfx文件12openssl pkcs12 -export -out 214043592950638.pfx -inkey214043592950638.key -in 214043592950638.pem 这时会提示我们输入password输入密码并确认后便生成了下面的.pfx文件-rw-rw-r-- 1 xxh xxh 4437 12月 7 09:28 214043592950638.pfx pfx证书安装我用的是sbt构建app的，所以把pfx文件放入resources目录中 webserver编写： 12345678910111213141516171819202122232425262728293031323334353637383940object WebServer extends LogSupport&#123; implicit val system = ActorSystem() implicit val mat = ActorMaterializer() implicit val dispatcher = system.dispatcher def main(args: Array[String]): Unit = &#123; // Manual HTTPS configuration val password: Array[Char] = &quot;mima&quot;.toCharArray val ks: KeyStore = KeyStore.getInstance(&quot;PKCS12&quot;) val keystore: InputStream = getClass.getClassLoader.getResourceAsStream(&quot;214043592950638.pfx&quot;) require(keystore != null, &quot;Keystore required!&quot;) ks.load(keystore, password) val keyManagerFactory: KeyManagerFactory = KeyManagerFactory.getInstance(&quot;SunX509&quot;) keyManagerFactory.init(ks, password) val tmf: TrustManagerFactory = TrustManagerFactory.getInstance(&quot;SunX509&quot;) tmf.init(ks) val sslContext: SSLContext = SSLContext.getInstance(&quot;TLS&quot;) sslContext.init(keyManagerFactory.getKeyManagers, tmf.getTrustManagers, new SecureRandom) val https: HttpsConnectionContext = ConnectionContext.https(sslContext) val routes: Route = path(&quot;test&quot;)&#123; get &#123; logger.info(&quot;response hello&quot;) complete(&quot;Hello world!&quot;) &#125; &#125; // sets default context to HTTPS – all Http() bound servers for this ActorSystem will use HTTPS from now on Http().setDefaultServerHttpContext(https) Http().bindAndHandle(routes, &quot;0.0.0.0&quot;, 443, connectionContext = https) logger.info(&quot;WebServer start&quot;) &#125;&#125; test上文提到过，必须在绑定域名的服务器上才能使用该证书，所以上传到ec2服务器上测试，记得开启443的入站端口 ps:我不小心将routes 放到了Http()绑定之后，出现了空指针异常，调整routes位置后ok rel 知乎相关问题 阿里云服务器绑定https]]></content>
      <categories>
        <category>服务器端 - web开发</category>
      </categories>
      <tags>
        <tag>https</tag>
        <tag>Certificate配置</tag>
        <tag>webapp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Http小记]]></title>
    <url>%2F2018%2F12%2F04%2FHttp%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Http调试工具Postman使用 Postman Postman测试接口之JSON结构化数据提交 Postman使用详解 JSONJSON:JavaScript Object NotationJSON is a syntax for storing and exchanging dataJSON is text,written with JavaScript object notationJSONw3school Http learning这篇博客总结的不错Http协议，超文本传输协议（HyperText Transfer P rotocol）客户端请求消息格式： 服务器端响应消息格式： 平常主要用到的请求方法只有：GET和POSTGET 方法：请求指定的页面信息，并返回实体主体 GET 请求可被缓存 GET 请求保留在浏览器历史记录中 GET 请求可被收藏为书签 GET 请求不应在处理敏感数据时使用 GET 请求有长度限制 GET 请求只应当用于取回数据POST 方法：向指定的资源提交表单或者文件，数据包含在请求体中，POST请求会导致新资源的建立或者原有资源的更改。 POST 请求不会被缓存 POST 请求不会保留在浏览器历史记录中 POST 不能被收藏为书签 POST 请求对数据长度没有要求HTTP 方法：GET 对比 POST四种POST请求 RESTful架构理解RESTful架构 REST: resource representation state transfer]]></content>
      <categories>
        <category>服务器端 - web开发</category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA 快捷键整理]]></title>
    <url>%2F2018%2F12%2F04%2FIntellij%20IDEA%20%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[【常规】 Ctrl+Shift + Enter，语句完成 “！”，否定完成，输入表达式时按 “！”键 Ctrl+E，最近的文件 Ctrl+Shift+E，最近更改的文件 Shift+Click，可以关闭文件 Ctrl+[ OR ]，可以跑到大括号的开头与结尾 Ctrl+F12，可以显示当前文件的结构 Ctrl+F7，可以查询当前元素在当前文件中的引用，然后按 F3 可以选择 Ctrl+N，可以快速打开类 Ctrl+Shift+N，可以快速打开文件 Alt+Q，可以看到当前方法的声明 Ctrl+P，可以显示参数信息 Ctrl+Shift+Insert，可以选择剪贴板内容并插入 Alt+Insert，可以生成构造器/Getter/Setter等 Ctrl+Alt+V，可以引入变量。例如：new String(); 自动导入变量定义 Ctrl+Alt+T，可以把代码包在一个块内，例如：try/catch Ctrl+Enter，导入包，自动修正 Ctrl+Alt+L，格式化代码 Ctrl+Alt+I，将选中的代码进行自动缩进编排，这个功能在编辑 JSP 文件时也可以工作 Ctrl+Alt+O，优化导入的类和包 Ctrl+R，替换文本 Ctrl+F，查找文本 Ctrl+Shift+Space，自动补全代码 Ctrl+空格，代码提示（与系统输入法快捷键冲突） Ctrl+Shift+Alt+N，查找类中的方法或变量 Alt+Shift+C，最近的更改 Alt+Shift+Up/Down，上/下移一行 Shift+F6，重构 - 重命名 Ctrl+X，删除行 Ctrl+D，复制行 Ctrl+/或Ctrl+Shift+/，注释（//或者/**/） Ctrl+J，自动代码（例如：serr） Ctrl+Alt+J，用动态模板环绕 Ctrl+H，显示类结构图（类的继承层次） Ctrl+Q，显示注释文档 Alt+F1，查找代码所在位置 Alt+1，快速打开或隐藏工程面板 Ctrl+Alt+left/right，返回至上次浏览的位置 Alt+left/right，切换代码视图 Alt+Up/Down，在方法间快速移动定位 Ctrl+Shift+Up/Down，向上/下移动语句 F2 或 Shift+F2，高亮错误或警告快速定位 Tab，代码标签输入完成后，按 Tab，生成代码 Ctrl+Shift+F7，高亮显示所有该文本，按 Esc 高亮消失 Alt+F3，逐个往下查找相同文本，并高亮显示 Ctrl+Up/Down，光标中转到第一行或最后一行下 Ctrl+B/Ctrl+Click，快速打开光标处的类或方法（跳转到定义处） Ctrl+Alt+B，跳转到方法实现处 Ctrl+Shift+Backspace，跳转到上次编辑的地方 Ctrl+O，重写方法 Ctrl+Alt+Space，类名自动完成 Ctrl+Alt+Up/Down，快速跳转搜索结果 Ctrl+Shift+J，整合两行 Alt+F8，计算变量值 Ctrl+Shift+V，可以将最近使用的剪贴板内容选择插入到文本 Ctrl+Alt+Shift+V，简单粘贴 Shift+Esc，不仅可以把焦点移到编辑器上，而且还可以隐藏当前（或最后活动的）工具窗口 F12，把焦点从编辑器移到最近使用的工具窗口 Shift+F1，要打开编辑器光标字符处使用的类或者方法 Java 文档的浏览器 Ctrl+W，可以选择单词继而语句继而行继而函数 Ctrl+Shift+W，取消选择光标所在词 Alt+F7，查找整个工程中使用地某一个类、方法或者变量的位置 Ctrl+I，实现方法 Ctrl+Shift+U，大小写转化 Ctrl+Y，删除当前行 Shift+Enter，向下插入新行 psvm/sout，main/System.out.println(); Ctrl+J，查看更多 Ctrl+Shift+F，全局查找 Ctrl+F，查找/Shift+F3，向上查找/F3，向下查找 Ctrl+Shift+S，高级搜索 Ctrl+U，转到父类 Ctrl+Alt+S，打开设置对话框 Alt+Shift+Inert，开启/关闭列选择模式 Ctrl+Alt+Shift+S，打开当前项目/模块属性 Ctrl+G，定位行 Alt+Home，跳转到导航栏 Ctrl+Enter，上插一行 Ctrl+Backspace，按单词删除 Ctrl+&quot;+/-&quot;，当前方法展开、折叠 Ctrl+Shift+&quot;+/-&quot;，全部展开、折叠【调试部分、编译】 Ctrl+F2，停止 Alt+Shift+F9，选择 Debug Alt+Shift+F10，选择 Run Ctrl+Shift+F9，编译 Ctrl+Shift+F10，运行 Ctrl+Shift+F8，查看断点 F8，步过 F7，步入 Shift+F7，智能步入 Shift+F8，步出 Alt+Shift+F8，强制步过 Alt+Shift+F7，强制步入 Alt+F9，运行至光标处 Ctrl+Alt+F9，强制运行至光标处 F9，恢复程序 Alt+F10，定位到断点 Ctrl+F8，切换行断点 Ctrl+F9，生成项目 Alt+1，项目 Alt+2，收藏 Alt+6，TODO Alt+7，结构 Ctrl+Shift+C，复制路径 Ctrl+Alt+Shift+C，复制引用，必须选择类名 Ctrl+Alt+Y，同步 Ctrl+~，快速切换方案（界面外观、代码风格、快捷键映射等菜单） Shift+F12，还原默认布局 Ctrl+Shift+F12，隐藏/恢复所有窗口 Ctrl+F4，关闭 Ctrl+Shift+F4，关闭活动选项卡 Ctrl+Tab，转到下一个拆分器 Ctrl+Shift+Tab，转到上一个拆分器【重构】 Ctrl+Alt+Shift+T，弹出重构菜单 Shift+F6，重命名 F6，移动 F5，复制 Alt+Delete，安全删除 Ctrl+Alt+N，内联【查找】 Ctrl+F，查找 Ctrl+R，替换 F3，查找下一个 Shift+F3，查找上一个 Ctrl+Shift+F，在路径中查找 Ctrl+Shift+R，在路径中替换 Ctrl+Shift+S，搜索结构 Ctrl+Shift+M，替换结构 Alt+F7，查找用法 Ctrl+Alt+F7，显示用法 Ctrl+F7，在文件中查找用法 Ctrl+Shift+F7，在文件中高亮显示用法【VCS】 Alt+~，VCS 操作菜单 Ctrl+K，提交更改 Ctrl+T，更新项目 Ctrl+Alt+Shift+D，显示变化]]></content>
      <categories>
        <category>工欲善其事，必先利其器</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8笔记]]></title>
    <url>%2F2018%2F12%2F04%2FJava8%2F</url>
    <content type="text"></content>
      <categories>
        <category>编程语言 - java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java任务调度]]></title>
    <url>%2F2018%2F12%2F04%2FJava%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[Java.util.Timer 在Java中有一个任务处理类java.util.Timer，非常方便于处理由时间触发的事件任务，只需建立一个继承java.util.TimerTask的子类，重载父类的run()方法实现具体的任务，然后调用Timer的public void schedule(TimerTask task, long delay, long period)方法实现任务的调度。 但是这种方法只能实现简单的任务调度，不能满足任务调度时间比较复杂的需求。比如希望系统在每周的工作日的8：00时向系统用户给出一个提示，这种方法实现起来就困难了，还有更为复杂的任务调度时间要求。 12345678910111213141516171819202122232425262728293031package scheduler;import java.util.Timer;import java.util.TimerTask;/** * Created by xhh on 2017/2/4. */public class TimerTest extends TimerTask &#123; private String jobName = &quot;&quot;; public TimerTest(String jobName) &#123; super(); this.jobName = jobName; &#125; @Override public void run() &#123; System.out.println(&quot;executing: &quot; + jobName); &#125; public static void main(String[] args)&#123; Timer timer = new Timer(); long delay1 = 1 * 1000; long period1 = 1000; timer.schedule(new TimerTest(&quot;job1&quot;),delay1,period1); long delay2 = 2 * 1000; long period2 = 2000; timer.schedule(new TimerTest(&quot;job2&quot;),delay2,period2); &#125;&#125; Timer源码解析 Quartz OpenSymphony 的Quartz提供了一个比较完美的任务调度解决方案。 Quartz 是个开源的作业调度框架，为在 Java 应用程序中进行作业调度提供了简单却强大的机制。 Quartz有两个基本的概念：作业和调度器。作业是能够调度的可执行任务，触发器提供了对作业的调度。 1、作业 2、调度器]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>任务调度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript笔记]]></title>
    <url>%2F2018%2F12%2F04%2FJavaScript%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[cookie基础： W3C http://www.cnblogs.com/Darren_code/archive/2011/11/24/Cookie.html（写的比较罗嗦） 代码干货 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// delete cookie 通过设置有效期实现function delete_cookie( name, path, domain ) &#123; if( get_cookie( name ) ) &#123; document.cookie = name + &quot;=&quot; + ((path) ? &quot;;path=&quot;+path:&quot;&quot;)+ ((domain)?&quot;;domain=&quot;+domain:&quot;&quot;) + &quot;;expires=Thu, 01 Jan 1970 00:00:01 GMT&quot;; &#125;&#125;//ORfunction delete_cookie( name ) &#123; document.cookie = name + &apos;=; expires=Thu, 01 Jan 1970 00:00:01 GMT;&apos;;&#125;//Anotherfunction createCookie(name,value,days) &#123; if (days) &#123; var date = new Date(); date.setTime(date.getTime()+(days*24*60*60*1000)); var expires = &quot;; expires=&quot;+date.toGMTString(); &#125; else var expires = &quot;&quot;; document.cookie = name+&quot;=&quot;+value+expires+&quot;; path=/&quot;;&#125;function readCookie(name) &#123; var nameEQ = name + &quot;=&quot;; var ca = document.cookie.split(&apos;;&apos;); for(var i=0;i &lt; ca.length;i++) &#123; var c = ca[i]; while (c.charAt(0)==&apos; &apos;) c = c.substring(1,c.length); if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length,c.length); &#125; return null;&#125;function eraseCookie(name) &#123; createCookie(name,&quot;&quot;,-1);&#125;I use this// what I have usedfunction removeItem(sKey, sPath, sDomain) &#123; document.cookie = encodeURIComponent(sKey) + &quot;=; expires=Thu, 01 Jan 1970 00:00:00 GMT&quot; + (sDomain ? &quot;; domain=&quot; + sDomain : &quot;&quot;) + (sPath ? &quot;; path=&quot; + sPath : &quot;&quot;);&#125;removeItem(&quot;cookieName&quot;); HTMLDOM（w3c文档对象模型）:定义了访问和操作 HTML文档 的标准方法，DOM将 HTML文档 表达为树结构 HTML DOM Node Tree HTML DOM 定义了所有 HTML 元素的对象和属性，以及访问它们的方法。 JS Window浏览器对象模型 (BOM) 使 JavaScript 有能力与浏览器“对话”。浏览器对象模型 (BOM) 浏览器对象模型（Browser Object Model）尚无正式标准。 由于现代浏览器已经（几乎）实现了 JavaScript 交互性方面的相同方法和属性，因此常被认为是 BOM 的方法和属性。 所有浏览器都支持window对象。它表示浏览器窗口 JS函数参数设置默认值scala 在定义函数时可以直接给参数设置默认值，如：def afunc(a = 1,b=2){} 但是js却不能这么定义，如果 function simue(a=1,b=2){} 会提示缺少对象。 js函数中有个存储参数的数组 arguments,所有函数获得的参数都会被编译器挨个保存到这个数组中。 于是我们可以用另一种变通的方法实现， 1234function simue()&#123;var a = arguments[0] ? arguments[0]:1;var b = arguments[1] ? arguments[1] : 2;&#125; JS实现文件上传单文件上传 AjaxUpload.js123456789101112131415161718192021function uploadFile() &#123; new AjaxUpload($(&quot;#importFile&quot;), &#123; action: url, type: &quot;POST&quot;, data: &#123;&quot;userId&quot;: userId&#125;, autoSubmit: true, responseType: &quot;json&quot;, name: &apos;file&apos;, onSubmit: function (file, ext) &#123; if (!(ext &amp;&amp; /^(rar|zip|pdf|pdfx|txt|csv|xls|xlsx|doc|docx|RAR|ZIP|PDF|PDFX|TXT|CSV|XLS|XLSX|DOC|DOCX)$/.test(ext))) &#123; pNotifyAutoCloseCenter(&quot;fail&quot;, &quot;您上传的文档格式不对，请重新选择！&quot;, &quot;error&quot;); return false; &#125; console.log(&quot;onsubmit&quot;); &#125;, onComplete: function (file, response) &#123; pNotifyAutoCloseCenter(&quot;info&quot;, &quot;数据上传&quot; + response.status + &quot;！&quot;, &quot;info&quot;); console.log(&quot;upload file complete, savePath: &quot; + response.savePath); &#125; &#125;);&#125; 多文件上传 支持多文件上传 WebUploader (produced by baidu)官方文档 http://fex.baidu.com/webuploader/getting-started.html]]></content>
      <categories>
        <category>前端数据可视化</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>笔记</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java笔记]]></title>
    <url>%2F2018%2F12%2F04%2FJava%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[设计模式单例模式java中单例模式是一种常见的设计模式，单例模式的写法有好几种，这里主要介绍三种： 懒汉式单例、 饿汉式单例、 登记式单例。单例模式有以下特点： 单例类只能有一个实例。单例类必须自己创建自己的唯一实例。 单例类必须给所有其他对象提供这一实例。 单例模式确保某个类只有一个实例，而且自行实例化并向整个系统提供这个实例。 在计算机系统中，线程池、缓存、日志对象、对话框、打印机、显卡的驱动程序对象常被设计成单例。这些应用都或多或少具有资源管理器的功能。每台计算机可以有若干个打印机，但只能有一个Printer Spooler，以避免两个打印作业同时输出到打印机中。每台计算机可以有若干通信端口，系统应当集中管理这些通信端口，以避免一个通信端口同时被两个请求同时调用。总之，选择单例模式就是为了避免不一致状态，避免政出多头。 123456789101112131415161718192021222324252627282930懒汉模式：public class SingletonTest &#123; public static void main(String[] args)&#123; System.out.println(&quot;Singleton1.1:&quot;+Singleton1.getInstance()); System.out.println(&quot;Singleton1.2:&quot;+Singleton1.getInstance()); System.out.println(&quot;Singleton2.1:&quot;+Singleton2.getInstance()); System.out.println(&quot;Singleton2.2:&quot;+Singleton2.getInstance()); System.out.println(&quot;ton1:&quot;+new ton()); System.out.println(&quot;ton2:&quot;+new ton()); &#125;&#125;//懒汉模式：第一次是使用的时候实例化自己class Singleton1&#123; private static Singleton1 singleton1 = null; public static Singleton1 getInstance()&#123; if (singleton1 == null)&#123; singleton1 = new Singleton1(); &#125; return singleton1; &#125;&#125;//饱汉模式class Singleton2&#123; private static final Singleton2 singleton2 = new Singleton2(); public static Singleton2 getInstance()&#123; return singleton2; &#125;&#125; Java service domain dao 分层思路 鼓励使用service domain dao 分层设计理念； 第一：dao层操作表单，不涉及复杂逻辑，主要是表的增删改查操作，完全按照domain的要求来查询数据； 第二：domain层考虑业务逻辑，例如过滤条件，放行或者返回，以及数据的处理，为调用dao层做好准备，一个domain可以调用一个或者一组相关的dao层 第三：service 层调用一个或者一组domain层，主要是展现需要开放出去的接口，其中并不是所有的domain层都要在service层展现， 此外，主要接口需要对接受的参数要尽量的扩大化，也就是说可以容纳各种类型的参数的接入（object），然后再service 层做好转换，以备domain层使用。 正确的设计应该是，一个领域活动会聚合对应一个或一组DAO ，来完成一个领域活动。而一个服务可能包含两个领域活动， 比如一个转账的业务，对应两个领域活动。两个帐户的金额分别发生变化，需要操作一组领域活动，而每个活动需要操作很多表（调用多个DAO ）。 使用 接口方式可以实现系统之间的松耦合：一般定义： IUserService UserService IUserDomain UserDomain IUserDao UserSqlDao UserHbaseDao 使用接口可以有不同的实现类 ,不需要改代码，可以通过配置更改不同的实现类 Q:I need to learn the difference between the type of methods (in term of business logic) that should be inside the Domain, DAO and Service layers objects. For example, if I am building a small web application to create, edit and delete customers data, as far as I understand inside Domain layer object I should add methods that Get/Set Customers object properties, for example (getName, getDOB, setAddress, setPhone...etc). Now what I am trying to learn is what methods shall I put in DAO and Service layers objects. Thanks in advance for your time and efforts. A:Speaking generally (not Hibernate or Spring specific): The DAO layer contains queries and updates to save your domain layer into your datastore (usually a relational DB but doesn&apos;t have to be). Use interfaces to abstract your DAO away from the actual datastore. It doesn&apos;t happen often, but sometimes you want to change datastores (or use mocks to test your logic), and interfaces make that easier. This would have methods like &quot;save&quot;, &quot;getById&quot;, etc. The Service layer typically contains your business logic and orchestrates the interaction between the domain layer and the DAOs. It would have whatever methods make sense for your particular domain, like &quot;verifyBalance&quot;, or &quot;calculateTotalMileage&quot;.String使用String类中的compareTo（）方法比较。如：s1.compareTo(s2)但是这个方法有个缺点,就是要求两个参与比较的字符串的形式及长度必须相同才能够正确比较,否则,比较结果是错的 Java中设置classpath、path、JAVA_HOME的作用:Java中设置classpath、path、JAVA_HOME的作用]]></content>
      <categories>
        <category>编程语言 - java</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Master Json]]></title>
    <url>%2F2018%2F12%2F04%2FMasterJson%2F</url>
    <content type="text"><![CDATA[Json 是什么类似于XML的标记性语言，在对象序列化，交换信息领域应用广泛 JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。它使得人们很容易的进行阅读和编写。同时也方便了机器进行解析和生成 json简单说就是javascript中的对象和数组，所以这两种结构就是对象和数组两种结构，通过这两种结构可以表示各种复杂的结构 对象：对象在js中表示为“{}”括起来的内容，数据结构为 {key：value,key：value,…}的键值对的结构，在面向对象的语言中，key为对象的属性，value为对应的属性值，所以很容易理解，取值方法为 对象.key 获取属性值，这个属性值的类型可以是 数字、字符串、数组、对象几种。 数组：数组在js中是中括号“[]”括起来的内容，数据结构为 [“java”,”javascript”,”vb”,…]，取值方式和所有语言中一样，使用索引获取，字段值的类型可以是 数字、字符串、数组、对象几种。 Json 使用scala常用 spray Json 依赖：12&quot;com.typesafe.akka&quot; %% &quot;akka-http-spray-json&quot; % &quot;10.0.10&quot;, &quot;io.spray&quot; % &quot;spray-json_2.11&quot; % &quot;1.3.4&quot; Json Array 12345import spray.json._ //parseJsonval kkkk = &quot;&quot;&quot; |[&#123;&quot;a&quot;: &quot;1&quot;&#125;, &#123;&quot;b&quot;: &quot;2&quot;&#125;] &quot;&quot;&quot;.stripMargin.parseJson.asInstanceOf[JsArray] 22+play-json-extensions 22+ field case class formatter and more for play-json 大数据场景下case class 必备,是play-json 的升级版跟spray-json也很像 1234567891011121314151617181920package data.creationimport ai.x.play.json.Jsonximport common.HttpClientHelperimport data.beans.LastInfoimport play.api.libs.json.Json/** * Created by xxh on 18-7-30. */object LastInfoHelper &#123; implicit lazy val lastInfoFormat = Jsonx.formatCaseClass[LastInfo] def getLastInfo(url:String,email:String,shoeLastBaseNo:String,basicsize:Int): LastInfo =&#123; val lastArray = new HttpClientHelper().post(url = url, postJsonObj = s&quot;&quot;&quot;&#123;&quot;email&quot;:&quot;$email&quot;,&quot;shoeLastBaseNo&quot;:&quot;$shoeLastBaseNo&quot;,&quot;basicsize&quot;:$basicsize&#125;&quot;&quot;&quot;) val lastJsonArray = Json.parse(lastArray) val lastInfo = lastJsonArray(0).as[LastInfo] lastInfo &#125;&#125;]]></content>
      <categories>
        <category>服务器端 - Json</category>
      </categories>
      <tags>
        <tag>Json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Learning]]></title>
    <url>%2F2018%2F12%2F04%2FRedis%20Learning%2F</url>
    <content type="text"><![CDATA[应用场景：redis 适用于小而热的数据 Redis应用场景 Redis作者谈Redis应用场景我们在应用redis的场景是 缓存将收到的post报文缓存下，然后批量插入redshift github上的scala版redis client官方推荐了好几款开源的scala 版客户端，都不知道选哪个比较好了，应该迅速读一遍scala 并发编程 Redis is an open source(BSD Licensed),in-memory data structure store,used as a database,cache,and message broker.It supports data structures such as strings,hashes,lists,sets,sorted sets with range queries,bitmaps,hyperloglogs and geospatial indexes with radius queries.build-in: replication Lua scripting LRU eviction transaction different levels of on-disk persistence provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster. You can run atomic operations on these types, like appending to a string; incrementing the value in a hash; pushing an element to a list; computing set intersection, union and difference; or getting the member with highest ranking in a sorted set. in-memory dataset: In order to achieve its outstanding performance, Redis works with an in-memory dataset. Depending on your use case, you can persist it either by dumping the dataset to disk every once in a while, or by appending each command to a log. Persistence can be optionally disabled, if you just need a feature-rich, networked, in-memory cache. no-blocking replication: Redis also supports trivial-to-setup master-slave asynchronous replication, with very fast non-blocking first synchronization, auto-reconnection with partial resynchronization on net split. transcation数据库事务彻底搞懂事务 6379-为何Redis选择它作为默认端口号？redis 使用服务器端／客户端架构的数据库架构通过client去连接redis服务端 我们可以通过redis-cli去连接redis数据库，这是学习测试redis最好的方法，这时redis-cli就是一个client 之前连接关系型数据库一般都是通过JDBC的方式连接的，这个redis是怎么连接上去的我暂时还没有搞清楚，官方推荐了很多类似与sdk的工具库，使用这些库可以很方便的连接redis， redis 持久化有两种方式做持久化，其一是RDB，定期备份数据库的状态；其二是AOF，记录数据库的相关命令优先使用AOF方式 使用list来存储数据，LPUSH mylist “1”// BRPOP mylist 0 取出最先放入的元素，每分钟生成一个key,同步数据只处理之前的key里面的数据做一个事务，保证该元素处理的原子性因为BRPOP 只能取出最后一个元素，所以应该使用BPOPLPUSH 散列算法(Hash Algorithm),又称哈希算法，杂凑算法，是从任意文件中创造小的数字[指纹]的方法。散列算法是一种以较短的信息来保证文件唯一性标志，这种标志与文件的每一个字节都相关，而且难以找到逆向规律。因此，当原有文件发生改变时，其标志值也会发生改变，从而告诉文件使用者当前文件已经不是你所需求的文件。 先举个例子。我们每个活在世上的人，为了能够参与各种社会活动，都需要一个用于识别自己的标志。也许你觉得名字或是身份证就足以代表你这个人，但是这种代表性非常脆弱，因为重名的人很多，身份证也可以伪造。最可靠的办法是把一个人的所有基因序列记录下来用来代表这个人，但显然，这样做并不实际。而指纹看上去是一种不错的选择，虽然一些专业组织仍然可以模拟某个人的指纹，但这种代价实在太高了。 而对于在互联网世界里传送的文件来说，如何标志一个文件的身份同样重要。比如说我们下载一个文件，文件的下载过程中会经过很多网络服务器、路由器的中转，如何保证这个文件就是我们所需要的呢？我们不可能去一一检测这个文件的每个字节，也不能简单地利用文件名、文件大小这些极容易伪装的信息，这时候，我们就需要一种指纹一样的标志来检查文件的可靠性，这种指纹就是我们现在所用的Hash算法(也叫散列算法)。 参考：Hash算法总结]]></content>
      <categories>
        <category>数据存储处理 - database</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SBT增加国内镜像并生效]]></title>
    <url>%2F2018%2F12%2F04%2FSBT%E5%A2%9E%E5%8A%A0%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E5%B9%B6%E7%94%9F%E6%95%88%2F</url>
    <content type="text"><![CDATA[第五章 SBT国内源配置 前几天执行：sbt update 失败 主要是 project/plugins.sbt里的插件下载失败 resolved xx failed,说找不到该插件 试了n多方法未果，今天发现用idea中的sbt插件可以正常获取到plugins.sbt中插件，所以严重怀疑是本机安装SBT有问题， sudo apt-get remove sbt 然后,按照官网安装教程重新安装sbt, 1234`echo &quot;deb https://dl.bintray.com/sbt/debian /&quot; | sudo tee -a /etc/apt/sources.list.d/sbt.listsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823sudo apt-get updatesudo apt-get install sbt` 果然，之前的问题已解决了（以上基于ubuntu16操作环境，其他开发环境类似）]]></content>
      <categories>
        <category>项目管理 - sbt</category>
      </categories>
      <tags>
        <tag>SBT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell笔记]]></title>
    <url>%2F2018%2F12%2F04%2FShell%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[awk将数据put到hdfs,然后用hive组织的过程中，发现数据混乱，从某行开始全是NULL 将一行数据copy到idea中,按照逗号分割后，发现只有223(理论上是226)，用awk的NF处理文件，每行也是 226列 —&gt; 于是，使用awk 的 $17 打印开始乱的列，果然找到问题， 用navicat导出的数据，默认有一个‘N*’符号 ，这个符号可以用notepad++来识别并替换掉ps: 该工具导出的数据另一个坑是：不管什么类型的数据，导出来都是到这双引号，往hive中导入时，有数值型的就尿了 所有后来直接用了sqoop 这里用到了awk ,觉得这个工具还是很强大的 123awk -F &apos;,&apos; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0 &quot;,分隔符:&quot; OFS&#125;&apos; xxx.csvawk -F &apos;,&apos; &apos;&#123;if(NF&lt;226)print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0&#125;&apos; ./xxx.csv awk -F &apos;,&apos; -v OFS=&quot;+++&quot; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0&#125;&apos; ./xxx.csv awk 是一个强大的文本分析工具，相对与grep的查找，sed的编辑，awk在对其数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。linux awk命令详解awk分隔符awk ‘{pattern + action}’ {filenames} chkconfig 命令chkconfig命令主要用来更新（启动或停止）和查询系统服务的运行级信息。 谨记chkconfig不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接。12345[root@fa ~]# chkconfig --list NetworkManagerNetworkManager 0:off 1:off 2:on 3:on 4:on 5:on 6:off[root@fa ~]# chkconfig NetworkManager off[root@fa ~]# chkconfig --list NetworkManagerNetworkManager 0:off 1:off 2:off 3:off 4:off 5:off 6:off 参考 curl get问题例如 url 为 http://mywebsite.com/index.PHP?a=1&amp;b=2&amp;c=3 web形式下访问url地址，使用$_GET是可以获取到所有的参数 然而在Linux下 curl http://mywebsite.com/index.php?a=1&amp;b=2&amp;c=3 $_GET只能获取到参数a 由于url中有&amp;，其他参数获取不到，在linux系统中&amp; 会使进程系统后台运行 必须对&amp;进行下转义才能$_GET获取到所有参数 curl http://mywebsite.com/index.php?a=1\&amp;b=2\&amp;c=3shell 时间 date 获取12[root48@vmaxverserver Qimingxing4AutoTestCoverageRate]$ date +%F-%T2016-10-20-10:20:41 参考 cron 表达式参考 等于/不等于/大于/小于 -eq //等于 -ne //不等于 -gt //大于 -lt //小于 ge //大于等于 le //小于等于 修改hostname:/etc/hosts 和 /etc/sysconfig/network 两个文件 升级中脚本：12345678910111213#!/bin/shscriptPath=$1sh /home/upgrade/shutdown.shcd $scriptPathfor i in *updating.shdohostName=echo $i|cut -d _ -f 1runAt=echo $i|cut -d _ -f 2|cut -d . -f 1sh /home/upgrade/upgrade.sh $hostName $runAt $scriptPath/$idonesh /home/upgrade/startup.sh linux grep命令//todo shell编程 for in 循环&amp; 后台执行scp 回显写入log：1234echo &quot;script -a \$basedir/conf/scp_drs_ftpini.log -q -c \&quot;ssh \$scpuser@\$sybaseip mkdir -p \$scppath\&quot;&quot; &gt;&gt; $outputecho &quot;script -a \$basedir/conf/scp_drs_ftpini.log -q -c \&quot;scp \$configFile \$scpuser@\$sybaseip:\$scppath\&quot;&quot; &gt;&gt; $outputecho &quot;script -a \$basedir/conf/scp_drs_ftpini.log -q -c \&quot;ssh \$scpuser@\$sybaseip chmod 777 -R \$scppath\&quot;&quot; &gt;&gt; $outputecho &quot;exit&quot; &gt;&gt; $output 参考自 ————-_—————————————2015/8/31—————————_——————– 备份文件：123456789101112basedir=$(pwd) # 获取当前目录的绝对路径，并赋值给 basedirbakpath=/home/bak_netnumenif [ &quot;$bakpath&quot; ]; then #判断某个目录是否存在，（[] 内 左右空格不能少） rm -rf $bakpathfids=$(ls $basedir | grep -E &apos;vmax-app|vmax-data&apos;) #ls 某个目录，并用&quot;grep -E&quot; 查找 包含&quot;vamx-app&quot;或者&quot;vmax-data&quot;的项for s in $&#123;ds[@]&#125; #for 每一项，do ... do mkdir -p $bakpath/$s cp -r $s/database $bakpath/$s done shell dirname basename123456789101112131415161718[root@hadoopname ~]# clear[root@hadoopname ~]# lltotal 136-rw-------. 1 root root 2696 Jun 24 04:41 anaconda-ks.cfgdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Desktopdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Documentsdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Downloads-rwxr-xr-x. 1 root root 32 Dec 6 13:20 first-rw-r--r--. 1 root root 39935 Dec 5 17:38 i.log-rw-r--r--. 1 root root 39935 Jun 24 04:40 install.log-rw-r--r--. 1 root root 10175 Jun 24 04:38 install.log.syslogdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Musicdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Picturesdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Publicdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Templatesdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Videos[root@hadoopname ~]# pwd/root dirname: 取一个文件的存储路径，分两种情况 仅仅是个文件名的 12[root@hadoopname ~]# dirname first. 带绝对路径 123[root@hadoopname ~]# dirname /root/first/root[root@hadoopname ~]# basename:取一个文件的文件名想要去掉后缀名的话 要将 后缀名 做为参数加在后面 12345[root@hadoopname ~]# basename i.logi.log[root@hadoopname ~]# basename i.log logi.[root@hadoopname ~]#]]></content>
      <categories>
        <category>服务器端 - shell</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单线性回归]]></title>
    <url>%2F2018%2F12%2F04%2FSimpleLinearRegression%2F</url>
    <content type="text"><![CDATA[简单线性回归WHAT简单线性回归模型是用于估计一个连续预测变量和一个连续回应变量的线性关系。回归方程或者是估计回归方程 y¯=b0+b1x y¯是回应变量的估计值 b0是回归线在y轴上的截距 b1是回归线的斜率 b0和b1b1称为回归系数 Simple linear regression(wiki) HOW怎样才能得到一个简单线性回归模型，也就是求得一个方程式 既有的数值处理库python的sk-learn、scala的breeze等sk-learn 例子： 123456789101112131415161718192021222324252627282930313233343536import pandas as pdfrom sklearn import linear_modelimport matplotlib.pyplot as pltdf = pd.read_csv(&apos;C:/Users/lenovo/Desktop/3VD_1.csv&apos;)&quot;&quot;&quot;# 建立线性回归模型&quot;&quot;&quot;regr = linear_model.LinearRegression()&quot;&quot;&quot;# 拟合&quot;&quot;&quot;regr.fit(df[&apos;basicsize&apos;].values.reshape(-1,1), df[&apos;footaround&apos;])a, b = regr.coef_, regr.intercept_print(a[0])feats =[&apos;footaround&apos;, &apos;footbackarcdistance54mm&apos;, &apos;footbackarcdistancemouth&apos;, &apos;footbackbodylength&apos;, &apos;footbackfacedistance&apos;, &apos;footbackgirth&apos;, &apos;footbasicwidth&apos;, &apos;footbitfingeroutsidewidth&apos;, &apos;footboatcurveheight&apos;, &apos;footbottomheartconcavity&apos;, &apos;footdegreerejection&apos;, &apos;footfifthtoeoutsidewidth&apos;, &apos;footfirstplantarjointheight&apos;, &apos;footfirsttoeinsidewidth&apos;, &apos;footforepalmconvexity&apos;, &apos;footfrontcross&apos;, &apos;footheadthickness&apos;, &apos;footheelbulgeheight&apos;, &apos;footheelheartconvexity&apos;, &apos;footheelheartwidth&apos;, &apos;footheelheight&apos;, &apos;footlandspot&apos;, &apos;footmetatarsalgirth&apos;, &apos;footmouthbackheight&apos;, &apos;footmouthlength&apos;, &apos;footmouthwidth&apos;, &apos;footpalmwidth&apos;, &apos;footpocketheelgirth&apos;, &apos;footshoelastcalculatelength&apos;, &apos;foottarsalboneheight&apos;, &apos;foottarsalgirth&apos;, &apos;footthumbinsidewidth&apos;, &apos;footwaistgirth&apos;, &apos;footwaistwidth&apos;, &apos;footwidth&apos;]para_lin = []for feat in feats: regr.fit(df[&apos;basicsize&apos;].values.reshape(-1,1), df[feat]) a, b = regr.coef_, regr.intercept_ para_lin.append((feat,a[0],b)) 使用公式 在统计学中，线性回归是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。一个带有一个自变量的线性回归方程代表一条直线。我们需要对线性回归结果进行统计分析。 通过定义和最小化一个成本函数，简单线性回归可以拟合一条直线。一般用最小二乘法（OLS)来计算系数。 线性回归的公式可以表示为 y=w0+w1x 截距 w0 斜率 w1 推导过程可参看wikipedia scala实现： 12345678910111213141516def simple_linear_regression(x: Vector[Int], y: Vector[Double]) = &#123; //initial sums val n = x.length.toFloat val sum_x = x.sum val sum_y = y.sum val sum_xy = (x, y).zipped.map(_ * _).sum val sum_xx = x.map(n =&gt; n * n).sum //formula for w0 val slope = (sum_xy - (sum_x * sum_y) / n) / (sum_xx - (sum_x * sum_x) / n) //formula for w1 val intercept = sum_y / n - slope * (sum_x / n) (slope,intercept) &#125; PS:以往查询数据库都是按行查询的，但是在这里是去数据中的两个维度来分别作为自变量和因变量，以求得简单线性回归模型，所以查询数据库数据时还是按列查询比较好 不然的话就要做n个Map,像这样： 123456789101112131415val is = getClass.getClassLoader.getResourceAsStream(&quot;origin-fat-last-data&quot;) val source = scala.io.Source.fromInputStream(is).getLines().map(_.split(&quot;\t&quot;).toList).toList val title = source.head val data = source.tail.filter(_.last == &quot;normal&quot;).toVector val mapData = data.map(_.zip(title).map(field=&gt; Map(field._2 -&gt; field._1)).reduce(_ ++ _)) val basicSizeVector = mapData.map(_.get(&quot;basicsize&quot;).get.toInt) val yMapDatas = mapData.map(_.filterNot(m =&gt; (&quot;shoes_size_specification&quot;,&quot;brand&quot;,&quot;shoelastbaseno&quot;,&quot;gender&quot;,&quot;headform&quot;,&quot;shoepadthicknessforefoot&quot;,&quot;shoepadthicknessheel&quot;,&quot;shoetype&quot;,&quot;footmouthgirth&quot;,&quot;footstruct&quot;).productIterator.contains(m._1))) .map(_.map(m =&gt; Map(m._1 -&gt; m._2.toDouble)).reduce(_ ++ _)) val yMapDataKeys =yMapDatas.head.map(_._1) val keyValues = yMapDataKeys.map(key =&gt; Map(key -&gt; yMapDatas.map(d=&gt; d.getOrElse(key,0.00d)))) def res = keyValues.map(kv =&gt;&#123; Map(kv.head._1 -&gt; simple_linear_regression(basicSizeVector,kv.head._2)) &#125;).reduce(_ ++ _)]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-API-RDD]]></title>
    <url>%2F2018%2F12%2F04%2FSpark-RDD%2F</url>
    <content type="text"><![CDATA[spark version: Spark2.3.0 Spark Core首先看一张图: Combine SQL, streaming, and complex analytics. Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application. spark-core: RDDsprak-sql: DataFrame,DataSetspark-streaming:spark-MLib:spark-GraphX: SparkContext我们的Spark App 如何同spark cluster 建立连接，答案就是SparkContext? sparkSession VS sparkContext 每个JVM只能存在一个sparkcontext 1val sparkContext = new SparkContext(&quot;spark://localhost:7077&quot;,&quot;idea&quot;) 还可以先构建SparkConf对象，其中设置好要连接的集群等参数，然后作为构建SparkContext对象的参数之一 SparkConfRDD从数据层次上看：RDD是spark的灵魂 Spark在数据层面提供的主要抽象就是RDD(弹性分布式数据集)，分布在集群不同节点的不同分区中的元素的集合，可以在这个数据集上使用各种并行算法 RDD的生成方式有两种： hadoop file system,或者是hadoop-supported file system Driver 程序中的scala 集合，比如Array RDD除了是对分布式数据集的抽象外，还包含了该RDD的由来，即血缘关系图，当出现错误时，RDD可以血缘关系图恢复 两个共享变量： broadcast variables accumulators]]></content>
      <categories>
        <category>数据存储处理 - spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>RDD</tag>
        <tag>spark-API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow Model java 部署]]></title>
    <url>%2F2018%2F12%2F04%2FTensorflow4--javadeploy%2F</url>
    <content type="text"><![CDATA[Tensorflow Model java 部署tensorflow最好用的api非python莫属，也是分析师最喜欢使用的， 但是到了模型部署阶段，使用python部署上线的应用常常碰到性能瓶颈， 所以问题来了：用python训练好的模型，能不能用java重新加载并预测结果？我们使用mnist来试一下 Python 训练模型//todo: 注释 README.md Java 部署训练好的模型//todo: 注释 README.md]]></content>
      <categories>
        <category>机器学习、深度学习 - tensorflow</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow Model java 部署]]></title>
    <url>%2F2018%2F12%2F04%2FTensorflow5--seq2seq%2F</url>
    <content type="text"><![CDATA[IntroductionSeq2Seq模型在机器翻译、语音识别、文本摘要等领域取得了巨大的成功 以神经机器翻译(NMT)为例介绍 使用最新的 decoder / attention wrapper API，TensorFlow 1.2 data iterator 在构建模型时加入了我们最成功的经验 为构建最强大的 NMT 模型，我们提供了许多 tips 和 tricks，并复制了Google的 NMT系统 Basic神经机器翻译的相关背景 | Background on Neural Machine Translation 传统的 phrase-based 翻译系统的工作原理是把源句子分成几块，然后一个短语一个短语的翻译，但是翻译结果的流畅度较差，并且不符合人类的翻译方式， 我们人类会读取完整的句子，理解它的意思，然后进行翻译。NMT 模型正是在模仿这个机制！ NMT模型使用encoder读取源句子，然后编码得到一个“有意义”的 vector，即一串能代表源句子意思的数字。然后 decoder 将这个 vector 解码得到翻译，就想图1展示的那样。这就是所谓的 encoder-decoder 结构 本教程将以单向的多层（multi-layer） RNN 为例，其使用 LSTM 作为 RNN 单元 Traning - How to build our first NMT system实战 - 训练一个 NMT 模型 Hands-on - Let’s train an NMT model12345678910111213python -m nmt.nmt \ --src=vi --tgt=en \ --vocab_prefix=/home/xxh/data/nmt_data_2/vocab \ --train_prefix=/home/xxh/data/nmt_data_2/train \ --dev_prefix=/home/xxh/data/nmt_data_2/tst2012 \ --test_prefix=/home/xxh/data/nmt_data_2/tst2013 \ --out_dir=/home/xxh/data/nmt_model \ --num_train_steps=12000 \ --steps_per_stats=100 \ --num_layers=2 \ --num_units=128 \ --dropout=0.2 \ --metrics=bleu python -m nmt.nmt –src=vi –tgt=en –vocab_prefix=/home/xxh/data/my_nmt_data/train –train_prefix=/home/xxh/data/my_nmt_data/train –dev_prefix=/home/xxh/data/my_nmt_data/tst2012 –test_prefix=/home/xxh/data/my_nmt_data/tst2012 –out_dir=/home/xxh/data/my_nmt_model –num_train_steps=12000 –steps_per_stats=100 –num_layers=2 –num_units=128 –dropout=0.2 –metrics=bleu 12 python -m nmt.nmt –out_dir=/home/xxh/data/nmt_model –inference_input_file=/home/xxh/data/nmt_data_2/my.vi –inference_output_file=/home/xxh/data/nmt_attention_model/output_infer]]></content>
      <categories>
        <category>机器学习、深度学习 - tensorflow</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat笔记]]></title>
    <url>%2F2018%2F12%2F04%2FTomcat%2F</url>
    <content type="text"><![CDATA[TOMCAT目录结构Tomcat 作为一个Web服务器软件，监听指定的端口，处理相应的网络请求；支持Servlt; |—bin Tomcat：存放启动和关闭tomcat脚本； |—conf Tomcat：存放不同的配置文件（server.xml和web.xml）； |—doc：存放Tomcat文档； |—lib/japser/common：存放Tomcat运行需要的库文件（JARS）； |—logs：存放Tomcat执行时的LOG文件； |—src：存放Tomcat的源代码； |—webapps：Tomcat的主要Web发布目录（包括应用程序示例）； |—work：存放jsp编译后产生的class文件； web.xml12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_4.dtd&quot;&gt;&lt;web-app&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;listener&gt; &lt;listener-class&gt;spray.servlet.Initializer&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;SprayConnectorServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;spray.servlet.Servlet30ConnectorServlet&lt;/servlet-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;SprayConnectorServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/autotest/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; servlet所有 servlet 都会直接或间接通过实现 javax.servlet.Servlet 接口的辅助类来实现该接口。Servlet 还可以使用容器提供的 API 公开容器服务。例如，servlet 可以从容器获得数据库连接来访问关系数据库。Servlet 通常用于实现 Web 应用程序逻辑。Servlet 可以获取和处理数据，然后将数据传递给 JSP 进行显示（例如，动态生成用户界面）。Servlet 还可用于频繁地处理通过基于 Web 形式提交的数据。 webApp的组成一个Web应用程序基本上由以下项目组成： 静态资源（静态资源(HTML，CSS，图片，声音等)） servlet jsp 自定义类 工具类 部署描述文件（web.xml等) 、设置信息 Web应用程序目录结构必须符合规范。例如，如果一个应用程序的环境路径（Context path）是/HelloServlet，则所有的资源项目必须以/HelloServlet为根目录依规定结构摆放。基本上根目录中的资源可以直接下载，若index.html位于/HelloServlet下，则可以直接以/HelloServlet/index.html来取得。 Web应用程序存在一个特殊的/WEB-INF子目录，此目录中存在的资源不会被列入应用程序根目录中可直接访问的项。即，客户端（如Browser）不可以直接请求/WEB-INF中的资源(直接在网址上指明访问/WEB-INF)，否则就是404 Not Found的错误结果。/WEB-INF中的资源项目有着一定的名称与结构。 （1）/WEB-INF/web.xml 是部署描述文件 （2）/WEB-INF/classes 用来放置应用程序用到的自定义类(.class)，必须包括包(package)结构。 （3）/WEB-INF/lib 用来放置应用程序用到的JAR文件。 Web应用程序用到的JAR文件，其中可以放置Servlet、JSP、自定义类、工具类、部署描述文件等，应用程序的类载入器可以从JAR中载入对应的资源。 可以在JAR文件中的/META-INF/resources目录中放置静态资源或JSP等，例如若在/META-INF中放个index.html，若请求的URL中包括/HelloServlet/index.html,但实际上/HelloServlet根目录下不存在index.html，则会使用JAR中的/META-INF/resources/index.html。 如果要用到某个类，则Web应用程序会到/WEB-INF/classes中试着载入类，若无，再试着从/WEB-INF/lib的JAR文件中寻找类文件(若还没有找到，则会到容器实现本身存放类或JAR的目录中寻找，但位置视不同实现厂商而有所不同，以Tomcat为例，搜索的路径是Tomcat的安装目录下的lib目录)。 客户端不能直接请求/WEB-INF中的资源，但可以通过程序的控制，让程序来取得/WEB-INF中的资源，如使用ServletContext的getResource()与getResourceAsStream()，或是通过RequestDispatcher请求调派。 Web app 默认主页的设置：访问URL如：localhost:8080/FirstServlet/ &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; 如果找不到以上的文件，则会尝试至JAR的/META-INF/resources中寻找已放置的资源页面。 整个Web应用可以被封装为一个WAR文件，如FirstServlet.war，以便于部署至Web容器。]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04/18LS小记]]></title>
    <url>%2F2018%2F12%2F04%2FUbuntu16.04LS%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[使用ubuntu有段时间了，有些问题解决了没记录，有时间找回来 在这里将使用过程中遇到值得记录的Ｍａｒｋ下来 卸载软件由于software总是打不开，所以选择用apt-get卸载//列出所有已安装的软件包dpkg --list结果ii xserver-common 2:1.18.3-1ub all common files used by various X se ii xserver-xorg 1:7.7+13ubun amd64 X.Org X server . . . ii xz-utils 5.1.1alpha+2 amd64 XZ-format compression utilities ii yelp 3.18.1-1ubun amd64 Help browser for GNOME ii yelp-xsl 3.18.1-1 all XSL stylesheets for the yelp help ii youdao-dict 1.1.0-0~ubun amd64 Youdao Dict for Linux ii zenity 3.18.1.1-1ub amd64 Display graphical dialog boxes fr//按照软件包名字卸载sudo apt-get --purge remove youdao-dict其中–purge 是指连软件的配置文件一块删除参考 重新安装软件包：sudo dpkg -i xxxsudo dpkg -i youdao-dict.deb 双系统－ubuntu连不上网办公室电脑安装了双系统，自带win10,自行安装ubuntu,有次重启电脑后，应该是去登录了win10，重新登录ubuntu发现连接不上网络，windows 一切正常，试了N种方法均无效，最后找到问题是 网卡驱动问题应该是win 系统更新了网卡驱动，而ubuntu 系统无法识别，解决办法：卸载新版本的网卡驱动，安装旧版本的网卡驱动 /home/xxh/package/r8168-8.044.021sh autorun.sh 可悲的是，每次重启都要重重重重新执行该脚本，还好只是执行下一个脚本没时间，也没兴趣去查深层次的原因了，暂时先这样啊，主要是一般不会关机 因各种原因，出于提高开发效率的考虑，将开发环境迁移到Ubuntu操作系统上来，果然碰到了高睿之前遇到的Ubuntu系统与PC自带显卡驱动不兼容问题，睿总帮忙解决，感谢 显卡驱动问题 使用的联想扬天电脑有点奇葩，与安装的UBUNTU16兼容性不好，ray给出了经验,安装最新的英伟达显卡驱动 安装JAVA - 安装了官方版java. 连接EC2 - 使用terminal直接连接ec2(sudo ssh -i ~/aws/xuxianhong.pem ec2-user@ec2-52-197-178-250.ap-northeast-1.compute.amazonaws.com) 其中公钥是ec2 description写明了的，用户名是ec2-user. ubuntu18 - 笔记本电脑合盖不休眠拿张某的笔记本做测试服务器，但是这个本合盖即锁屏，试了很多方法（界面配置等等）没能解决，下面这个方法好使Ubuntu16.04 笔记本合上盖子时不进入休眠]]></content>
      <categories>
        <category>工欲善其事，必先利其器</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用ansible自动部署集群环境]]></title>
    <url>%2F2018%2F12%2F04%2Fansible%2F</url>
    <content type="text"><![CDATA[what Working in IT, you’re likely doing the same tasks over and over. What if you could solve problems once and then automate your solutions going forward? Ansible is here to help.类似的组件还有： puppetsaltstackchef install官方文档上有多种安装方法，包括apt-get、yum,我这里用的是github方式 1git clone git://github.com/ansible/ansible.git --recursive 官网上的描述特详细，也特别好使，安装时建议直接看最新的文档ansible freshman get startinventory服务器清单，配置文件 动态InventorypatternsAd-Hoc Commands即使执行的命令，跟playbooks一起，构成ansible的强大战力 playbooksplaybooks 介绍playbooks 是ansible的配置，部署，编排语言 他们可以被描述为一个需要希望远程主机执行命令的方案，或者一组程序运行的命令集合 ps:难道类似存储过程？ 简单来说,playbooks 是一种简单的配置管理系统与多机器部署系统的基础.与现有的其他系统有不同之处,且非常适合于复杂应用的部署. playbook 实例Playbooks的格式是YAML playbook 由一个或多个 ‘plays’ 组成.它的内容是一个以 ‘plays’ 为元素的列表. 在 play 之中,一组机器被映射为定义好的角色.在 ansible 中,play 的内容,被称为 tasks,即任务.在基本层次的应用中,一个任务是一个对 ansible 模块的调用,这在前面章节学习过. ‘plays’ 好似音符,playbook 好似由 ‘plays’ 构成的曲谱,通过 playbook,可以编排步骤进行多机器的部署,比如在 webservers 组的所有机器上运行一定的步骤, 然后在 database server 组运行一些步骤,最后回到 webservers 组,再运行一些步骤,诸如此类. 123456789101112131415161718---- hosts: webservers vars: http_port: 80 max_clients: 200 remote_user: root tasks: - name: ensure apache is at the latest version yum: pkg=httpd state=latest - name: write the apache config file template: src=/srv/httpd.j2 dest=/etc/httpd.conf notify: - restart apache - name: ensure apache is running service: name=httpd state=started handlers: - name: restart apache service: name=httpd state=restarted play 基础：… Playbook角色(Roles) 和Include语句variables配置用 条件选择循环最佳实践ansible 有提供aws相关的模块，可以直接使用ansible tower注册 -&gt; 发下载链接到邮箱 -&gt; 下载 -&gt; 安装 error 不支持ubuntu18.04error 266701ERROR! Unexpected Exception, this is probably a bug: &apos;module&apos; object has no attribute &apos;SSL_ST_INIT&apos; resolved: 1sudo pip2 install -U pyOpenSSL 安装了一个小时，在办公主机上，ec2上安装都不成功，各种b问题，同时发现其使用的组件过于繁杂还是自己写吧 ansible-galaxy 上面有很多好用的role sudo 不好使 -&gt; 修改配置文件中的sudo-user remote-user,然后执行的时候+ –become 已经先不在使用ansible,优先直接使用bootstrap.ssh 完成相关操作 后面该操作使用akka remote封装]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[breeze使用]]></title>
    <url>%2F2018%2F12%2F04%2Fbreeze%2F</url>
    <content type="text"><![CDATA[whatBreeze is a library for numerical processing, machine learning, and natural language processing. Its primary focus is on being generic, clean, and powerful without sacrificing (much) efficiency. Breeze is the merger of the ScalaNLP and Scalala projects, because one of the original maintainers is unable to continue development. howinstall快速安装用sbt在命令行中快速安装，体验一下 推荐的sbt依赖配置]]></content>
      <categories>
        <category>数据分析 - scala</category>
      </categories>
      <tags>
        <tag>breeze</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[build-tool]]></title>
    <url>%2F2018%2F12%2F04%2Fbuild-tool%2F</url>
    <content type="text"><![CDATA[构建工具总结对比： 旧时代的Java世界，构建工具等同于两个词Ant和Maven Ant源自Make，同JUnit一样，它也是一个航班产物（参见《Ant权威指南》序）。不过，Make的基础注定了它的起点不高，所有一切都要手工打造。我还依稀记得曾几何时，每每开始一个项目都要重新复制一份Ant脚本，修修改改。Maven给了人们新的希望，目录结构的约定、依赖管理、IDE集成，那时看来，几近完美。是的，几近，还差了那么一点点。就是那么一点点，却是致命伤。 只要一个项目进行一段时间，一个必然遇到的问题就是，编写新的自动化脚本。因为每个项目都有自己的特定需求，标准做法必然是无法满足的。扩展Maven对任何新手都是一件头疼的事，我们要学会编写插件，要搞清楚生命周期，这时，突然会唤起一丝丝对于ANT的怀念，虽然它做简单事不容易，但做复杂事却也没这么困难。 如果那些日子，我们不得不忍受Ant和Maven的不完美，那现在，是时候抛弃它们了。 新时代Java构建有两个很好的候选：Gradle和Buildr。 在我看来，它们真正比Maven更强大的地方，编写自己的任务更加容易。更值得兴奋的一点是，我们终于可以抛弃冗长的XML，选择一种更优雅的程序设计语言来写代码了，这几乎意味着你可以做到想做的一切。 Buildr是Apache出品的构建工具，它以Ruby作为构建脚本。《软件开发地基》讨论软件项目应该具备一些基础构建项，就是以Buildr为基础的。有兴趣的话，可以参考一下。这里就不再赘述。顺便说一下，那篇文章里的内容，除了某些写法现在需要做一些微调，大部分内容依然是适用于大多数Java项目。 Gradle现在是整个Java社区的构建新宠，它采用Groovy作为自己的构建语言。如果你知道，Groovy是一门诞生自JVM平台的语言，这就决定了它要比其它移植到JVM上的语言能更好的适应JVM平台，它可以采用更符合Java世界的方式无缝地整合Java既有的程序库，而不必像移植语言那样削足适履。 初涉Gradle，最让人吃惊的一点莫过于它详尽的文档，涵盖了Gradle使用的方方面面，这是许多开源软件项目无法媲美，即便早在它的1.0版本尚未发布之时。当然，能做到这一点是因为它背后有一个公司在支撑：GradleWare，这意味着如果你需要商业支持，也是可以的。 Gradle 1.0尚未发布之，它就捧回2010年的Spring大奖和入围了2011年的JAX大奖。如果你还需要更多的信心，作为Java开发人员，你不可能不知道Spring，那Spring转投Gradle怀抱，应该是对Gradle最有利的支持了。 说了这么多，程序员最喜欢看到的东西还是代码。 首先，请自行下载安装Gradle，然后，按照常见的Java项目布局把代码准备好（感谢Maven为我们给予我们约定），比如： src/main/java，源代码文件目录src/main/resources，资源文件目录src/test/java，测试代码目录下面是一个简单的Gradle构建脚本，将其存放于build.gradle apply plugin: ‘java’ repositories { mavenCentral()} dependencies { compile( ‘com.google.guava:guava:13.0.1’, ‘joda-time:joda-time:2.1’ ) testCompile( ‘junit:junit:4.10’, ‘org.mockito:mockito-all:1.9.0’ )} （build.gradle） 接下来的事情就很简单了，在命令行里键入 gradle build 如果一切正常，我们应该会看到构建成功的字样，然后，到build/lib，你就看到自己构建出来的JAR文件了。当然，这只是一个简单得不能再简单的例子，如果需要了解更多，Gradle那详尽的文档便是最好的去处。 著作权归作者所有，转载需联系作者以获得授权，引用需注明出处。作者ID:s8达克斯链接:https://www.applysquare.com/topic-cn/SlxE26k3q/ gradlegradle install开发主机使用ubuntu16,按照官网文档上的manually方法安装，测试可用 kafka源码编译的时候执行 1gradle build 超时报错 debug: 12gradle build --debug 打印出比较详细的信息，也清晰的显示出问题所在 问题：安装过程中，安装脚本自动将系统配置的代理信息写入gradle的配置文件~/.gradle/gradle.properties;而我的shadowsocks配置的额https代理不能用，所以访问不了外网解决方法：删除配置文件中的代理信息 sbtmaven]]></content>
      <categories>
        <category>build tool</category>
      </categories>
      <tags>
        <tag>build tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库理论基础与实践]]></title>
    <url>%2F2018%2F12%2F04%2Fdatawarehouse%2F</url>
    <content type="text"><![CDATA[理论数据仓库与联机分析处理技术 OLTP：联机事务处理 OLAP：联机分析处理 两者之间的差异使得传统的数据库技术不能同时满足两类数据的处理要求，数据仓库技术应运而生 数据仓库技术数据仓库是为了构建新的分析处理环境而出现的一种数据存储和组织技术 操作型数据 分析型数据 表示业务处理的动态情况 表示业务处理的静态情况 在存取的瞬间是正确的 代表过去的数据 可更新，由录入人员或经过专门培训的输入事务而更新 不可更新，终端用户的访问权限常常是只读的 处理细节问题 受到更多关注的是结论性的数据，是综合的，或是提炼的 操作需求事先可知，系统可按预计的工作量进行优化 操作需求事先不知道，永远不知道下一步用户要做什么 有许多事务，每个事务影响数据的一小部分 有数目不多的一些查询，每个查询可访问大量数据 面向应用，支持日常操作 面向分析，支持管理需求 用户不必理解数据库，只是输入数据 用户需要理解数据库，以从数据中得出有意义的结论 数据仓库的定义：数据仓库是一个用以更好地支持企业或组织决策分析处理的，面向主题的，集成的，不可更新的，随着时间不断变化的数据集合。数据仓库本质上和数据库一样，是长期存储在计算机内的，有组织，可共享的数据集合 数据仓库的基本特征 数据仓库的数据是面向主题的； 数据仓库的数据是集成的; 数据仓库的数据是不可更新的; 数据仓库的数据是随时间不断变化的. 主题与面向主题在逻辑意义上，它对应企业中某一宏观分析领域所涉及的分析对象 主题域应该具有以下两个特点：独立性和完备性 各个主题应用相对应的主题表 数据仓库中的数据组织数据仓库中的数据分为多个级别：早期细节级、当前细节级、轻度综合级、高度综合级 数据仓库系统的体系结构数据仓库系统架构由数据仓库的后台工具，数据仓库服务器、OLAP服务器和前台工具组成 OLAP多维数据模型多维分析操作数据仓库基础知识 关系数据库概念三大范式三大范式参考如何理解关系数据库的第一第二第三范式范式是关系数据库理论的基础，也是我们在设计数据库结构过程中索要遵循的规则和指导方法，目前有8中范式， 但是通用的是三大范式(1NF\2NF\3NF) 1NF强调列的原子性，即列不能够再分成其他几列 2NF 首先是1NF,另外表必须要有主键，而是没有包含在主键中的列必须完全依赖于主键，而不是主键的一部分 考虑一个订单明细表：【OrderDetail】（OrderID，ProductID，UnitPrice，Discount，Quantity，ProductName）。因为我们知道在一个订单中可以订购多种产品，所以单单一个 OrderID 是不足以成为主键的，主键应该是（OrderID，ProductID）。显而易见 Discount（折扣），Quantity（数量）完全依赖（取决）于主键（OderID，ProductID），而 UnitPrice，ProductName 只依赖于 ProductID。所以 OrderDetail 表不符合 2NF。不符合 2NF 的设计容易产生冗余数据。可以把【OrderDetail】表拆分为【OrderDetail】（OrderID，ProductID，Discount，Quantity）和【Product】（ProductID，UnitPrice，ProductName）来消除原订单表中UnitPrice，ProductName多次重复的情况。 第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的唯一标识。简而言之，第二范式就是在第一范式的基础上属性完全依赖于主键。 3NF 在1NF的基础上，任何非主属性不依赖于其他非主属性[在2NF基础上消除传递依赖 ] 第三范式（3NF）是第二范式（2NF）的一个子集，即满足第三范式（3NF）必须满足第二范式（2NF）。 首先是 2NF，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。 考虑一个订单表【Order】（OrderID，OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity）主键是（OrderID）。 其中 OrderDate，CustomerID，CustomerName，CustomerAddr，CustomerCity 等非主键列都完全依赖于主键（OrderID），所以符合 2NF。不过问题是 CustomerName，CustomerAddr，CustomerCity 直接依赖的是 CustomerID（非主键列），而不是直接依赖于主键，它是通过传递才依赖于主键，所以不符合 3NF。通过拆分【Order】为【Order】（OrderID，OrderDate，CustomerID）和【Customer】（CustomerID，CustomerName，CustomerAddr，CustomerCity）从而达到 3NF。 第二范式（2NF）和第三范式（3NF）的概念很容易混淆，区分它们的关键点在于，2NF：非主键列是否完全依赖于主键，还是依赖于主键的一部分；3NF：非主键列是直接依赖于主键，还是直接依赖于非主键列。 视图视图是从一个或几个基本表或视图中导出的表.他与基本表不同,是一个虚表,数据库只存放视图的定义,而不存放视图对应的数据,这些数据还存放在原来的基本表中,从这个角度讲,视图就像是一个窗口,透过视图就可以看到数据库中自己感兴趣的数据及其变化视图一经定义,他就可以和基本表一样被查询\被删除,等等,但是视图的更新有限制 定义视图建立视图123create view &lt;视图名&gt; [(&lt;列名&gt;[,&lt;列名&gt;]...)]as &lt;子查询&gt;[with check option] 实践]]></content>
      <categories>
        <category>datawarehouse</category>
      </categories>
      <tags>
        <tag>datawarehouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmpeg]]></title>
    <url>%2F2018%2F12%2F04%2Fffmpeg%2F</url>
    <content type="text"><![CDATA[安装 ubuntu16:Linux下安装ffmpeg ubuntu18:系统自带，免安装 使用1./ffmpeg -threads 2 -y -r 1 -framerate 30 -i ~/data/fmpgtest/p/i%d.png -vf scale=853:480 ~/data/fmpgtest/output1.mp4]]></content>
      <categories>
        <category>误入歧途 - ffmpeg</category>
      </categories>
      <tags>
        <tag>ffmpeg</tag>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github访问卡、慢、不稳定]]></title>
    <url>%2F2018%2F12%2F04%2Fgithub%E5%8D%A1%E6%85%A2%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[震惊！访问云服务器慢、连不上去、github不能访问、慢,震惊，原来是需要更新dns \2017/12/04参考 blog中说需要添加修改hosts 文件，我F12后发现现在的github的ip有变化，就没添加hosts 但是更新了dns，效果很明显，速度杠杠的 2017/12/05 迅速被打脸，一点都不好使 很奇怪，办公室机器google-chrome很卡，firefox好很多]]></content>
      <categories>
        <category>工欲善其事，必先利其器</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 笔记]]></title>
    <url>%2F2018%2F12%2F04%2Fhadoop%2F</url>
    <content type="text"><![CDATA[Map reduce job getting stuck at map 0% reduce 0%一般是没有资源导致卡住，比如查看yarn中active node是否为0 ，内存配置不足，cpu配置之类的 但也有可能是map加载输入的时间较长导致Map reduce job getting stuck at map 0% reduce 0% ? 比较详细的mapreduce job 日志 哪里找得到Hadoop 2.6 日志文件和MapReduce的log文件研究心得]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce,hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 编程指南 笔记]]></title>
    <url>%2F2018%2F12%2F04%2Fhive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[《hive 编程指南》 第一章与mr接口相比，sql简单易用 第二章hive 安装 hive笔记安装 hive内部是什么 lib 其中的每个jar都实现了hive的特定功能 bin 执行各种hive服务的可执行文件 conf 配置文件 hive 命令cli变量和属性hivevar/hiveconf/system/env .hivec文件第三章 数据类型和文件格式支持关系型数据库中绝大多数基本数据类型，还支持三种集合数据类型 要考虑这些数据类型是如何正在文本文件中表示的，还要考虑文本存储中为了解决各种性能问题以及其他问题有哪些替代方案 基本数据类型 string 字符串无长度限制 timestamp 可以使整数、浮点数、字符串 binary 字节数组 数值比较时，数据类型在 double float int tinyint smallint bigint 间自动向上转换 字符串转化： cast(s as int) 集合数据类型 struct map array 缺点是不符合三大范式，第一范式都不符合 数据冗余，在大数据情况下，按数据集封装的话可以通过减少寻址次数来提供查询的速度，而如果根据外键关系关联的话则需要磁盘间的寻址操作，这样会有非常高的性能消耗 文本文件数据编码文本文件在字段间、集合数据类型中的分隔符 读时模式传统数据库是写时模式(schema on write)hive是读时模式(schema on read) hive在读时会尽可能的规避问题，这样的功能很强大，也会导致问题被掩盖 第四章 hiveql 定义]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database,hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka笔记]]></title>
    <url>%2F2018%2F12%2F04%2Fkafka%2F</url>
    <content type="text"><![CDATA[introduction三个主要能力 publist and subscribe 类似于管道 容错的存储数据 即时处理数据处理 两大应用场景： 构建可靠的应用与系统间Streaming data 管道 构建Streaming 程序来传输或react数据流 Kafka is generally used for two broad classes of applications: Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data First a few concepts: Kafka is run as a cluster on one or more servers that can span multiple datacenters. The Kafka cluster stores streams of records in categories called topics. Each record consists of a key, a value, and a timestamp. 四个主要API producer api cunsumer api streams api connector api kafka源码 git clone gradle build 超时报错 debug: 12gradle build --debug 打印出比较详细的信息，也清晰的显示出问题所在 问题：安装过程中，安装脚本自动将系统配置的代理信息写入gradle的配置文件~/.gradle/gradle.properties;而我的shadowsocks配置的额https代理不能用，所以访问不了外网解决方法：删除配置文件中的代理信息 阶段性可执行后，结果为BUILG FAILD 搜索后有人表示：有多个scala 版本，导致这样的错误 尝试执行 1./gradlew -PscalaVersion=2.11.11 clean core:test 依然build failed 这次是 1Could not find org.jacoco.agent.jar 《Apache Kafka 源码剖析》上说的源码编译步骤不好使，转参考下面的方法Getting started with contributing to Apache Kafka (Part 1): Build and run Kafka from source code棒棒哒 123&gt; cd kafka&gt; gradle&gt; ./gradlew jar importa kafka project run zookeeper 修改server.properties中的zk配置 run kafka cp conf/log4j.properties 到 core/src/main/resources/log4j.properties Run -&gt; Kafka 报错 没有创建日志文件的权限 原来日志文件呢中默认的日志存放位置是/ 添加配置 kafka.logs.dir=~/data/kafka-source-logs 修复 修改log4j.rootLogger=TRACE, stdout 打印出更多信息 server.propertiessocket 缓冲区 socketsocket的英文原义是“孔”或“插座”。作为4BDS UNIX的进程通信机制，取后一种意思。通常也称作”套接字”，用于描述IP地址和端口，是一个通信链的句柄。应用程序通常通过”套接字”向网络发出请求或者应答网络请求。 在Internet上的主机一般运行了多个服务软件，同时提供几种服务。每种服务都打开一个Socket，并绑定到一个端口上，不同的端口对应于不同的服务。Socket正如其英文原意那样，象一个多孔插座。一台主机犹如布满各种插座的房间，每个插座有一个编号，有的插座提供220伏交流电， 有的提供110伏交流电，有的则提供有线电视节目。 客户软件将插头插到不同编号的插座，就可以得到不同的服务。 进程send数据时，实际上是先写入了socket缓冲区，接受类似，进程去read 接受缓冲区内的数据而不是直接拿到。 日志保留策略 １．一定时间之前的数据可删除２．日志存储文件所剩空间小于一定大小时可删除最早的数据 producer API 从某集群消费，然后生产到其他集群中，数据量是千万级别，但是速度很慢， 装个kafka monitor 看下 系统监控资料：《systems performance》 1234567891011121314151617181920212223# kafka start bash#!/bin/bashkafka_home=/home/xxh/wkspc/kafka/kafka_2.11-2.0.0kafka_start_sh=$kafka_home/bin/kafka-server-start.shkafkaconf0=$kafka_home/config/server.propertieskafkaconf1=$kafka_home/config/server-1.propertieskafkaconf2=$kafka_home/config/server-2.propertieskafkalog0=$kafka_home/logs/nohup.logkafkalog1=$kafka_home/logs/nohup_1.logkafkalog2=$kafka_home/logs/nohup_2.lognohup $kafka_start_sh $kafkaconf0 &gt; $kafkalog0 2&gt;&amp;1 &amp;nohup $kafka_start_sh $kafkaconf1 &gt; $kafkalog1 2&gt;&amp;1 &amp;nohup $kafka_start_sh $kafkaconf2 &gt; $kafkalog2 2&gt;&amp;1 &amp;jps# close use ./bin/kafka-server-stop.sh kafka 跑了几个例子，但是并没有讲的很清晰 还是要学一下，java8的函数式特性 不得不承认，越来越多的接口会重新用java写 挑战者： Pulsar kafka Stream]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux服务器相关配置]]></title>
    <url>%2F2018%2F12%2F04%2Flinux%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[本地ubuntu网络不稳定这几天 命令行执行操作各种不爽，间歇性精神病，git ssh 都会犯病没有直接搜索到什么原因导致的，安装wirshark抓包看看 123sudo add-apt-repository ppa:wireshark-dev/stablesudo apt-get updatesudo apt-get install wireshark wireshark-gnome 定位是shadowsocks代理的问题想改一下环境变量env中http_proxy 等环境变量，尝试删掉，但是有些环境变量是只读的，不能删除，wtfk回头一想，改掉shadowsocks的本地代理接口好了1080 -&gt; 34177 ok,问题暂时解决windows大法好 知乎问题 AWSEC2自动断开连接问题： ssh 连接aws ec2，若没有更改任何配置，ec2会在很短的时间内中断该ssh连接 解决 多方搜索，尝试（苦），最后更改以下配置解决问题更改ec2服务端sshd配置文件： 1/etc/ssh/sshd_config =&gt; 12ServerAliveInterval 60ServerAliveCountMax 10 参考 linux服务器重装java8问题：新开的云服务器默认安装的都是java7,我们的应用都是基于java8开发的 重装java8步骤： 找出现在装好的jdk1.7 rpm -qa | grep java 1234[ec2-user@ip-10-0-1-89 ~]$ rpm -qa |grep javajava-1.7.0-openjdk-1.7.0.151-2.6.11.0.74.amzn1.x86_64tzdata-java-2017b-1.69.amzn1.noarchjavapackages-tools-0.9.1-1.5.amzn1.noarch remove jdk1.7 yum -y remove java-1.7.0-openjdk-1.7.0.151-2.6.11.0.74.amzn1.x86_64 check 12345[ec2-user@ip-10-0-1-89 ~]$ java -version-bash: java: command not found[ec2-user@ip-10-0-1-89 ~]$ rpm -qa |grep javatzdata-java-2017b-1.69.amzn1.noarchjavapackages-tools-0.9.1-1.5.amzn1.noarch res:CentOS 卸载OpenJdk install jdk8 上传从甲骨文下载的java8 rpm安装包到服务器 安装 1.方法1 123456789101112131415161718192021222324 [ec2-user@ip-10-0-1-89 ~]$ java -version -bash: java: command not found [ec2-user@ip-10-0-1-89 ~]$ rpm -ivh .bash_history .bash_profile jdk-8u152-linux-x64.rpm .bash_logout .bashrc .ssh/ [ec2-user@ip-10-0-1-89 ~]$ rpm -ivh jdk-8u152-linux-x64.rpm error: can&apos;t create transaction lock on /var/lib/rpm/.rpm.lock (Permission denied) [ec2-user@ip-10-0-1-89 ~]$ sudo rpm -ivh jdk-8u152-linux-x64.rpm Preparing... ################################# [100%] Updating / installing... 1:jdk1.8-2000:1.8.0_152-fcs ################################# [100%] Unpacking JAR files...tools.jar...plugin.jar...javaws.jar...deploy.jar...rt.jar...jsse.jar...charsets.jar...localedata.jar... [ec2-user@ip-10-0-1-89 ~]$ java -version java version &quot;1.8.0_152&quot; Java(TM) SE Runtime Environment (build 1.8.0_152-b16) Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode) 2.方法2 12sudo apt-get install oracle-java8-installer[详细参见](https://www.cnblogs.com/a2211009/p/4265225.html)]]></content>
      <categories>
        <category>服务器端 - 环境</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log4j踢平记]]></title>
    <url>%2F2018%2F12%2F04%2Flog4j%E8%B8%A2%E5%B9%B3%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[错误：** log4j:WARN No appenders could be found for logger (scala.runtime.Nothing$). ** 错误原因：我猜是依赖自带的log4j之间冲突了 解决： stackoverflow Just to get you going you have two simple approaches you can take. First one is to just add this line to your main method:BasicConfigurator.configure();Second approach is to add this standard log4j.properties (taken from the above mentioned guide) file to your classpath: Set root logger level to DEBUG and its only appender to A1.log4j.rootLogger=DEBUG, A1 A1 is set to be a ConsoleAppender.log4j.appender.A1=org.apache.log4j.ConsoleAppender A1 uses PatternLayout.log4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n The first one 好使]]></content>
      <categories>
        <category>服务器端 - 日志</category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag>log4j</tag>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn]]></title>
    <url>%2F2018%2F12%2F04%2Fml-sklearn%2F</url>
    <content type="text"><![CDATA[Scikit-learn 安装之前安装了anaconda3，其中已经打包安装好了scikit-learn Scikit-learn 使用]]></content>
      <categories>
        <category>机器学习、深度学习 - scikit-learn</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>scikit-learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy]]></title>
    <url>%2F2018%2F12%2F04%2Fnumpy%2F</url>
    <content type="text"><![CDATA[科学运算当中最为重要的两个模块，一个是 numpy,一个是 pandas。任何关于数据分析的模块都少不了它们两个。 Why 运算速度快：numpy 和 pandas 都是采用 C 语言编写, pandas 又是基于 numpy, 是 numpy 的升级版本。 消耗资源少：采用的是矩阵运算，会比 python 自带的字典或者列表快好多 Numpy 安装Anaconda打包安装 Numpy]]></content>
      <categories>
        <category>数据分析 - Numpy</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[别人家的博客]]></title>
    <url>%2F2018%2F12%2F04%2Fothers-blog%2F</url>
    <content type="text"><![CDATA[[夏天的森林：https://home.cnblogs.com/u/sharpxiajun/feed/1.html [zhaorongcuncsdn:http://blog.csdn.net/zrc199021 [写点什么 关于scala 后端http://hongjiang.info/ [鸟窝 [stark_summer[树上月 [cwiki [http://blog.csdn.net/honglei915/article/details/37697655 [面向对象软件开发和过程(18摸) [美团技术团队 [陈先生 [哥不是小萝莉 [看懂UML [浅析hadoop 文件格式 [通过IDEA远程调试 [concurrnetHashMap 解析 [全科-不安分的码农 [linux shell [阿里JAVA [我的java问题排查工具单 [github王下月熊]]></content>
      <categories>
        <category>书签</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas]]></title>
    <url>%2F2018%2F12%2F04%2Fpandas%2F</url>
    <content type="text"><![CDATA[### 之前安装了anaconda3，其中已经打包安装好了scikit-learn Scikit-learn 使用]]></content>
      <categories>
        <category>数据分析 - pandas</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 笔记]]></title>
    <url>%2F2018%2F12%2F04%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[python 笔记日常python使用笔记 python List 2 String How to convert list to string By using ‘’.join 12list1 = [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;]str1 = &apos;&apos;.join(list1) Or if the list is of integers, convert the elements before joining them. 12list1 = [1, 2, 3]str1 = &apos;&apos;.join(str(e) for e in list1) python 写文件1234567file = &quot;/home/xxh/data/footlastlist&quot;+time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()) f= open(file,&quot;w+&quot;) try: f.write(&apos;&apos;.join(str(e) for e in footlastlist)) finally: f.close() file文件open后一定记得close(还有一种写法with) open的参数w+可以自动创建不存在的文件，前提是，设置的file路径为绝对路径]]></content>
      <categories>
        <category>编程语言 - python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sbt package VS sbt assembly VS sbt dist]]></title>
    <url>%2F2018%2F12%2F04%2FsbtpackageVSsbtassembly%2F</url>
    <content type="text"><![CDATA[sbt打包： sbt package sbt assembly sbt dist 1. sbt package在sbt Doc的[运行]章节有明确的介绍：Running package 将 src/main/resources 下的文件和 src/main/scala 以及 src/main/java 中编译出来的 class 文件打包成一个 jar 文件。package Creates a jar file containing the files in src/main/resources and the classes compiled from src/main/scala and src/main/java. 使用：在项目根目录下执行： 1sbt package 运行结果会展示已经在target//下，所打成功的包 2. sbt assemblyhttps://github.com/sbt/sbt-assembly Deploy fat JARs. Restart processes. sbt-assembly is a sbt plugin originally ported from codahale’s assembly-sbt, which I’m guessing was inspired by Maven’s assembly plugin. The goal is simple: Create a fat JAR of your project with all of its dependencies. 相比于 sbt package,sbt assembly会将project的依赖都打包进来 使用：github项目下的README.md中有详细的使用方法，这里简要摘抄如下： 在project/plugin.sbt中添加如下项： 1addSbtPlugin(&quot;com.eed3si9n&quot; % &quot;sbt-assembly&quot; % &quot;0.14.6&quot;) 在build.sbt文件中设置： 1234//使用java jar 执行时的main类，(入口类)mainClass in assembly := Some(&quot;AwsSupportS3&quot;)//设置目标jar包的名字assemblyJarName in assembly := s&quot;my-api-hdfs1.1.0607.jar 打包，在根目录下执行： 1sbt assembly 同样会有包在target/***/目录下 3. sbt distsbt 墙裂推荐 github地址：https://github.com/sbt/sbt-native-packager sbt官方支持：https://www.scala-sbt.org/sbt-native-packager/ 宣言： “Code once, deploy anywhere” 官方也做了相近的使用说明，文档算是很不错了 这里mark下我的使用： 在project/plugin.sbt中添加如下项： 1addSbtPlugin(&quot;com.typesafe.sbt&quot; % &quot;sbt-native-packager&quot; % &quot;1.1.5&quot;) 在build.sbt文件中设置： 1234567891011121314151617181920enablePlugins(JavaServerAppPackaging)//main class 配置，同上mainClass in Compile := Some(&quot;***.aip.ml.platform.rest.WebServer&quot;)//外部配置文件路径，project代码src/main/resources中的配置文件自动生成一份放在配置的路径下，如要更改配置文件，仅更改外部的即可mappings in Universal ++= &#123; // optional example illustrating how to copy additional directory directory(&quot;scripts&quot;) ++ // copy configuration files to config directory contentOf(&quot;src/main/resources&quot;).toMap.mapValues(&quot;config/&quot; + _)&#125;// add &apos;config&apos; directory first in the classpath of the start script,// an alternative is to set the config file locations via CLI parameters// when starting the applicationscriptClasspath := Seq(&quot;../config/&quot;) ++ scriptClasspath.valuelicenses := Seq((&quot;CC0&quot;, url(&quot;http://creativecommons.org/publicdomain/zero/1.0&quot;))) 打包，在根目录下执行： 1sbt dist 以上工具在打包前，最好先执行： 1sbt clean update 确保project没有问题]]></content>
      <categories>
        <category>项目管理 - sbt</category>
      </categories>
      <tags>
        <tag>sbt</tag>
        <tag>package</tag>
        <tag>assembly</tag>
        <tag>dist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala implict]]></title>
    <url>%2F2018%2F12%2F04%2Fscala%20implicit%2F</url>
    <content type="text"><![CDATA[why 应用中自己写的代码和调用的第三方函数库有着一个基本的区别：也就是你可以任意修改和扩展自己写的代码，而一般来说在没有源码的情况下很难扩展第三方函数库，只能利用函数库提供什么就是什么。 在Scala中解决这个问题是使用隐含类型变换和隐时参数。它们可以使调用函数库变得更加方便，并避免一些繁琐和显而易见的细节。 how在scala中 implicit 类隐式调用函数可以转换调用方法的对象，比如但编译器看到X .method，而类型 X 没有定义 method（包括基类)方法，那么编译器就查找作用域内定义的从 X 到其它对象的类型转换，比如 Y，而类型Y定义了 method 方法，编译器就首先使用隐含类型转换把 X 转换成 Y，然后调用 Y 的 method。 implicit 方法 一旦编译器看到了X,但是需要Y,就会检查从X到Y的隐式转换函数 12345678910scala&gt; implicit def double2Int(d:Double) = d.toIntwarning: there were 1 feature warning(s); re-run with -feature for detailsdouble2Int: (d: Double)Intscala&gt; implicit def double2Int(d:Double):Int = d.toIntwarning: there were 1 feature warning(s); re-run with -feature for detailsdouble2Int: (d: Double)Intscala&gt; val i:Int = 3.5i: Int = 3 implicit 参数当我们在定义方法时，可以把最后一个参数列表标记为implicit，表示该组参数是隐式参数。一个方法只会有一个隐式参数列表，置于方法的最后一个参数列表。如果方法有多个隐式参数，只需一个implicit修饰即可。 当调用包含隐式参数的方法是，如果当前上下文中有合适的隐式值，则编译器会自动为改组参数填充合适的值。如果没有编译器会抛出异常。当然，标记为隐式参数的我们也可以手动为该参数添加默认值 隐式操作规则 标记规则:标记为implict才可用 作用域：插入的隐式转换必须以单一标识符的形式处于操作域中 无歧义原则：隐式转换唯有不存在其他可插入转换的前提下才能插入 单一调用原则：只会尝试一个隐式操作 显示操作先行规则：若编写的代码类型检查无误，则不会尝试任何隐式操作，编译器不会改变已经可以正常工作的代码 命名隐式转换隐式转换可以任意命名,隐式转换 引用 与名字无关，编译器主要检察输入输出类型 开闭原则规定，软件中的对象对扩展应该是开放的，对于修改应该是关闭的]]></content>
      <categories>
        <category>编程语言 - scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala Try]]></title>
    <url>%2F2018%2F12%2F04%2FscalaTry%2F</url>
    <content type="text"><![CDATA[Scala Try优雅的错误处理方式:Try 异常的抛出和捕获123456789101112131415161718192021222324252627package tryAndException/** * Created by xxh on 18-7-17. */object MasterTry &#123; case class Customer(age:Int) class Cigarettes case class UnderAgeException(message:String) extends Exception(message) def buyCigarettes(customer: Customer):Cigarettes = &#123; if (customer.age &lt;16) throw UnderAgeException(s&quot;Customer must be older than 16 but was $&#123;customer.age&#125;&quot;) else new Cigarettes &#125; val youngCustomer = Customer(15) try &#123; buyCigarettes(youngCustomer) &quot;Yo, here are your cancer sticks! Happy smokin&apos;!&quot; &#125; catch &#123; case UnderAgeException(msg) =&gt; msg &#125;&#125; 函数式的错误处理问题：对于高并发应用来说，这也是一个很差劲的解决方式，比如， 假设需要处理在其他线程执行的 actor 所引发的异常，显然你不能用捕获异常这种处理方式， 你可能会想到其他解决方案，例如去接收一个表示错误情况的消息。 解决： 一般来说，在 Scala 中，好的做法是通过从函数里返回一个合适的值来通知人们程序出错了。scala 使用一个特定的类型来表示可能会导致异常的计算，这个类型就是 Try Try 的语义Option[A] 是一个可能有值也可能没值的容器， Try[A] 则表示一种计算： 这种计算在成功的情况下，返回类型为 A 的值，在出错的情况下，返回 Throwable 。 这种可以容纳错误的容器可以很轻易的在并发执行的程序之间传递。 Try 有两个子类型 Success[A]:代表成功的计算 封装了Throwable的Falure[A]:代表了出错了的计算 使用12345import scala.util.Tryimport java.net.URLdef parseURL(url: String): Try[URL] = Try(new URL(url))val url = parseURL(Console.readLine(&quot;URL: &quot;)) getOrElse new URL(&quot;http://duckduckgo.com&quot;) 你可以调用 isSuccess 方法来检查一个 Try 是否成功，然后通过 get 方法获取它的值， 但是，这种方式的使用并不多见，因为你可以用 getOrElse 方法给 Try 提供一个默认值： gitbook未完待续。。。]]></content>
      <categories>
        <category>编程语言 - s cala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala操作XML]]></title>
    <url>%2F2018%2F12%2F04%2Fscalaxml%2F</url>
    <content type="text"><![CDATA[scala 加载并解析xml12345678910111213141516171819202122232425262728293031323334353637val loadXML = xml.XML.loadFile(AutomatedTestingWebConfig.distributeInfoXML)//使用loadFile方法加载XML，得到的是一个一个scala.xml.Elem对象val version = (loadXML \\ &quot;autotest&quot; \ &quot;@version&quot;).text//属性值获取“@”符val hostSeqNode = (loadXML \ &quot;hosts&quot; \ &quot;host&quot;)//得到的是一个scala.xml.NodeSeq对象，可以在其上做一下转换操作def getAttrValue(node: Node)(attrName: String): String = &#123; node.attributes.get(attrName) match &#123; case Some(x) =&gt; x.text case _ =&gt; &quot;&quot; &#125; &#125; val hosts = &#123; (loadXML \ &quot;hosts&quot; \ &quot;host&quot;).map(hostNode =&gt; &#123; def getHostAttrValue(attrName: String) = getAttrValue(hostNode)(attrName) Host(getHostAttrValue(&quot;name&quot;), getHostAttrValue(&quot;desc&quot;), getHostAttrValue(&quot;config&quot;)) &#125;).toList &#125; val sqlScript = &#123; (loadXML \ &quot;script&quot; \ &quot;sql&quot;).map(sqlNode =&gt; &#123; def getSqlScriptAttrValue(attrName: String) = getAttrValue(sqlNode)(attrName) val lines = sqlNode.nonEmptyChildren.filterNot(_.text.trim == &quot;&quot;).map(_.text).mkString(&quot;;&quot;) val database = getSqlScriptAttrValue(&quot;database&quot;) SqlScript(database, lines) &#125;).toList &#125;case class AutoTest(version: String, hosts: List[Host], sqlScript: List[SqlScript], shellScript: List[ShellScript])case class Host(name: String, desc: String, config: String)case class SqlScript(database: String, line: String)case class ShellScript(host: String, line: List[String]) 参考 SCALA XML pattern attrbute参考 虽然这个guy炒鸡罗嗦，但是还是讲到我要的那句话： Because Scala doesn’t support XML patterns with attributes. scala的模式匹配模式根本就不支持 属性 还是老老实实用XPath吧]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>XML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala 笔记]]></title>
    <url>%2F2018%2F12%2F04%2Fscala%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[TomCat使用：Tomcat笔记 安装配置环境变量JAVA_HOME; 解压下载的tomcat压缩包； netstat -an 查看端口占用情况； 修改server.xml; 双击startup.bat 启动； scala List类的高阶方法 forall: xs forall p以列表xs和论断p为参数，如果列表所有元素满足p则返回true; 与之相对应的 exists: xs exists pxs 中只要有一个值满足p就返回true。 scala List类 TO String 方法 mkString(哈哈，当年这个都要记一下) 简单的单元测试 12345678910object TestGroupDao extends App&#123; new GroupDao().updateGroup(GroupInfo(3456,&quot;centrum&quot;))&#125;object MainTestGroupDao&#123; def main (args: Array[String]) &#123; val groupDao = new GroupDao println(&quot;groupDao: &quot; + groupDao) &#125;&#125; case class 实例 =&gt; List RE123456789101112131415161718192021productIterator.toListscala&gt; case class aaa(i:Int,j:Int)defined class aaascala&gt; val q = aaa(1,2)q: aaa = aaa(1,2)scala&gt; q.toList&lt;console&gt;:11: error: value toList is not a member of aaa q.toList ^scala&gt; q.toStringres1: String = aaa(1,2)scala&gt; q.productIterator.toListres2: List[Any] = List(1, 2)scala&gt; q.productIterator.toList.mkString(&quot;&apos;&quot;)res3: String = 1&apos;2 case class 实例 转 Map （使用反射）RE123456789101112131415def updateUser(userInfo: UserInfo): Boolean = &#123; //val userInfoFieldName = userInfo.getClass.getDeclaredFields.map(_.getName) val userInfoFieldValueMap = (Map[String, Any]() /: userInfo.getClass.getDeclaredFields) &#123; (a, f) =&gt; f.setAccessible(true) a + (f.getName -&gt; f.get(userInfo)) &#125; val updateUserSqlset = if(userInfo.password == &quot;&quot;) userInfoFieldValueMap.keys.map(a =&gt; &#123;if (a != &quot;password&quot;) a + &quot;=&apos;&quot; + userInfoFieldValueMap(a)&#125;).mkString(&quot;&apos;,&quot;) else userInfoFieldValueMap.keys.map(a =&gt; a + &quot;=&apos;&quot; + userInfoFieldValueMap(a)).mkString(&quot;&apos;,&quot;) val updateUserSqlWhere = &quot;userId = &apos;&quot; + userInfoFieldValueMap(&quot;userId&quot;) + &quot;&apos;&quot; val updateUserSql = s&quot;&quot;&quot; |update userinfo set $updateUserSqlset where $updateUserSqlWhere &quot;&quot;&quot;.stripMargin Sql(updateUserSql).execute &#125; List 列表模式 列表还可以使用模式匹配做拆分，这时列表模式需逐一匹配要拆分的列表表达式 两种形式： 1. List(...) 对列表所有的元素做匹配 1234567scala&gt; val fruit = List(&quot;Apple&quot;,&quot;Orange&quot;,&quot;pears&quot;)fruit: List[String] = List(Apple, Orange, pears)scala&gt; val List(a,b,c) = fruita: String = Appleb: String = Orangec: String = pears 2. :: 操作符 和 Nil 常量组成的模式逐位拆分列表 123456789101112scala&gt; val fruit = List(&quot;Apple&quot;,&quot;Orange&quot;,&quot;pears&quot;)fruit: List[String] = List(Apple, Orange, pears)scala&gt; val List(a,b,c) = fruita: String = Appleb: String = Orangec: String = pearsscala&gt; val a:: b :: rest = fruita: String = Appleb: String = Orangerest: List[String] = List(pears) 在不知道列表长度为多少时，最好使用后一种插入排序算法1. 陪练写法： 1234567def isort(xs: List[Int]): List[Int] = if(xs.isEmpty) Nil else insert(xs.head,isort(xs.tail))def insert(x: Int, xs: List[Int]): List[Int] = if(xs.isEmpty || x &lt;= xs.head) x :: xs else xs.head :: insert(x,xs.tail) 2. // 使用模式匹配拆分列表的方法 123456789def isort(xs: List[Int]) = xs match &#123; case List() =&gt; List() case x :: xs1 =&gt; insert(x,isort(xs1))&#125;def insert(x: Int, xs: List[Int]) = xs match &#123; case List() =&gt; List(x) case y :: ys =&gt; if(x &lt;= y) x :: xs else y :: insert(x,ys)&#125;http://hongjiang.info/scala/Scala 强大的集合数据操作示例//todoscala sort 排序123xmlFileList.map((xml.XML.loadFile(_))).map(parseXML(_)).filter((_.version.compareTo(currentVersion) &gt; 0)).map(a =&gt; (a.version, a)).&lt;strong&gt;sortWith((a,b) =&gt; (a._1).compareTo(b._1) &lt; 0)&lt;/strong&gt;.toMap 如何排序 scala 读写文件//todo scala 变长参数、ListgetFootInfoByUUID(l: _*) getFootInfoByUUID()是一个接受变长参数的方法l中的数据通过语法 _* 转换并传入函数中 scala伴生对象伴生对象可以调用伴生类中的方法，但是反过来不行 scala sbt read file from resources1234val is = getClass.getClassLoader.getResourceAsStream(&quot;remote_app.conf&quot;)val source = scala.io.Source.fromInputStream(is).getLines().map(_.split(&quot;\t&quot;).toList).toList//.foreach(as =&gt; println(as.length))[How to get load file under resources folder in scala sbt](https://stackoverflow.com/questions/40724082/how-to-get-load-file-under-resources-folder-in-scala-sbt) scala 反射如果要用反射的话，需要额外添加依赖 1libraryDependencies += &quot;org.scala-lang&quot; % &quot;scala-reflect&quot; % &quot;2.11.11&quot; scala Jsonplay-json-extensions +22 field case class formatter and more for play-json 大数据场景下case class 必备,是play-json 的升级版跟spray-json也很像 1234567891011121314151617181920package data.creationimport ai.x.play.json.Jsonximport common.HttpClientHelperimport data.beans.LastInfoimport play.api.libs.json.Json/** * Created by xxh on 18-7-30. */object LastInfoHelper &#123; implicit lazy val lastInfoFormat = Jsonx.formatCaseClass[LastInfo] def getLastInfo(url:String,email:String,shoeLastBaseNo:String,basicsize:Int): LastInfo =&#123; val lastArray = new HttpClientHelper().post(url = url, postJsonObj = s&quot;&quot;&quot;&#123;&quot;email&quot;:&quot;$email&quot;,&quot;shoeLastBaseNo&quot;:&quot;$shoeLastBaseNo&quot;,&quot;basicsize&quot;:$basicsize&#125;&quot;&quot;&quot;) val lastJsonArray = Json.parse(lastArray) val lastInfo = lastJsonArray(0).as[LastInfo] lastInfo &#125;&#125; case object vs case class vs object vs classcase class \ case object 都有pattern match 、序列化 、toString 、 equal、方法的好处加持只是 object 还是一个单例对象，不能有构造参数，如果是一个不可变的单里对象用在模式匹配中时可用 new trait不能通过new 关键字去新建一个对象，只能新建一个extends这个trait的匿名类， 123456789101112131415161718192021trait MyTrait&#123;val a = 1val b = 2&#125;//----------------val anonClassMixingInTrait = new Mytrait&#123; def aFunctionInMyClass = &quot;111&quot;&#125;//is the equivalent of:class MyClass extends MyTrait&#123; def aFunctionInMyClass = &quot;111&quot;&#125;val anonClassMixingInTrait = new MyClass 区别是 前者只能在定义该匿名类的时候初始化它，因为他既没有名字，也没有构造函数 12345678910111213141516val t = new MyTrait &#123; val t1 = ... //some expression val t2 = ... //some expression&#125;is the same asval t = new AnyRef with MyTrait &#123; val t1 = ... //some expression val t2 = ... //some expression&#125;is the same asval t = new Object with MyTrait &#123; val t1 = ... //some expression val t2 = ... //some expression&#125; type关键字在scala 字符串中转义一个美元(dollar)$符号加倍 12scala&gt; val a = s&quot;$$a&quot;a: String = $a]]></content>
      <categories>
        <category>编程语言 - scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell笔记]]></title>
    <url>%2F2018%2F12%2F04%2Fshell%E8%84%9A%E6%9C%AC%E8%BF%81%E7%A7%BBgitlab%2F</url>
    <content type="text"><![CDATA[背景gitlab A 上的一部分repo迁移至 gitlab B 在下底层coder,用不了gitlab 管理员的骚操作（这活应该是管理员来干啊） 平时建了很多repo在上面，想用脚本迁移同步过去 实现12345678910111213141516171819202122232425262728293031323334#!/bin/bash# 当前目录baseDir=$(pwd)# 父目录fatherDir=$(dirname $baseDir)#echo $fatherDir#projList=$(ls $baseDir)# 列出当前目录下的全部子目录，包括.. 父目录[Linux Shell 只列出目录的方法](https://blog.csdn.net/DLUTBruceZhang/article/details/9244897)projList=$(ls -l |grep &quot;^d&quot; |awk &apos;&#123;print $9&#125;&apos;)for p in $&#123;projList[@]&#125; do if [ $p != &quot;/home/xxh/data/mycloud&quot; ] #过滤掉父目录,if 字符串比较讲究还不少，用的时候在具体查吧 then #echo &quot;wocao&quot; newP=$fatherDir/gitlabbakmirror/$p #新建repo dir mkdir -p $newP echo $newP git clone git@aigit.epoque.cn:xianhong.xu/$p.git $newP cd $newP # 在新的gitlab上创建repo [原repo库迁移及批量创建gitlab库](https://blog.csdn.net/lets_do/article/details/78110913) # [private token 寻找方法](https://www.safaribooksonline.com/library/view/gitlab-cookbook/9781783986842/ch06s05.html) info=&quot;name=$p&amp;path=$p&amp;wiki_enabled=no&amp;public_jobs=false&amp;public=false&amp;default_branch=master&amp;private_token=WwspxLGvbxAAY_5HQBH9&quot; # 只有ip端口需要替换 curl -d $info &quot;http://172.17.60.240:8020/api/v3/projects&quot; git remote set-url origin git@172.17.60.240:xuxianhong/$p.git #更改remote url -&gt; 新的gitlab git add . # 没什么更改，不执行也行 git commit -m &apos;migrate&apos; git push origin master cd $baseDir else echo $p&quot;,no process&quot; fi done 小计脚本异常处理什么的一概没有，本身是一次性执行快速脚本，所以就不再这里花费太多时间了]]></content>
      <categories>
        <category>服务器端 - shell</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark填坑记]]></title>
    <url>%2F2018%2F12%2F04%2Fspark%E5%A1%AB%E5%9D%91%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[使用spark streaming报错ERROR DFSClient: Failed to close inode xxxxtopic太多造成的，有待进一步了解 需要给checkpoint的目录打时间戳 spark集群环境主机日志文件太多问题： spark集群环境主机日志文件太多，超出了ext3文件系统一级子目录的个数默认为31998(个)，准确地说是32000个，导致集群不正常 解决： 手动情况日志文件， 更改配置： 修改配置文件：/home/mr/spark/conf/spark-defaults.conf使 123spark.worker.cleanup.enabled=true,spark.worker.cleanup.appDataTtl = 259200(3*24*3600), #worker App保存三天 然后重启spark服务和sparkSQL服务。]]></content>
      <categories>
        <category>数据存储处理 - spark</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark的3个API:RDDs/DataFrames/DataSets]]></title>
    <url>%2F2018%2F12%2F04%2Fspark%E7%9A%84%E4%B8%89%E4%B8%AAAPI%2F</url>
    <content type="text"><![CDATA[且谈Apache Spark的API三剑客：RDD、DataFrame和Dataset]]></content>
      <categories>
        <category>数据存储处理 - spark</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>Spark</tag>
        <tag>DataFrame</tag>
        <tag>DataSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql笔记]]></title>
    <url>%2F2018%2F12%2F04%2Fsql%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[sql优化SQL查询的过程理解SQL查询的过程是进行SQL优化的理论依据 第一、单表查询：根据WHERE条件过滤表中的记录，形成中间表（这个中间表对用户是不可见的）；然后根据SELECT的选择列选择相应的列进行返回最终结果。 第二、两表连接查询：对两表求积（笛卡尔积）并用ON条件和连接类型进行过滤形成中间表；然后根据WHERE条件过滤中间表的记录，并根据SELECT指定的列返回查询结果。 笛卡尔（Descartes）乘积又叫直积。假设集合A=a,b，集合B=0,1,2，则两个集合的笛卡尔积为(a,0),(a,1),(a,2),(b,0),(b,1), (b,2)。 可以扩展到多个集合的情况。类似的例子有，如果A表示某学校学生的集合，B表示该学校所有课程的集合，则A与B的笛卡尔积表示所有可能的选课情况。 第三、多表连接查询：先对第一个和第二个表按照两表连接做查询，然后用查询结果和第三个表做连接查询，以此类推，直到所有的表都连接上为止，最终形成一个中间的结果表，然后根据WHERE条件过滤中间表的记录，并根据SELECT指定的列返回查询结果。 ON后面的条件（ON条件）和WHERE条件的区别： ON条件：是过滤两个链接表笛卡尔积形成中间表的约束条件。 WHERE条件：在有ON条件的SELECT语句中是过滤中间表的约束条件。在没有ON的单表查询中，是限制物理表或者中间查询结果返回记录的约束。在两表或多表连接中是限制连接形成最终中间表的返回结果的约束。 从这里可以看出，将WHERE条件移入ON后面是不恰当的。推荐的做法是： ON只进行连接操作，WHERE只过滤中间表的记录。 关键点：表连接查询是先将 几张表 做笛卡尔积 然后在做过滤 sql执行顺序（转侵删）sql不同于其他编程语言，代码并不是按照编码顺序来被处理的， 在sql语言中，第一个被处理的是FROM子句，SELECT子句第一个出现，却被最后一个处理； 每个步骤都会产生一个虚拟表，该虚拟表被用作下一个步骤的输入； 这些虚拟表对调用者不可用，只是最后一步生成的表才会返回给调用者。如果没有在查询中指定某一子句，将跳过相应的步骤 下面的各个逻辑步骤描述适用于SQL server2000 和 2005: FROM : 对from子句的前两个表执行笛卡尔积，生成虚拟表VT1; ON: 对VT1应用ON筛选，只有那些使为真的行才被插入VT2; OUTER(JOIN)：如 果指定了OUTER JOIN（相对于CROSS JOIN 或(INNER JOIN),保留表（preserved table：左外部联接把左表标记为保留表，右外部联接把右表标记为保留表，完全外部联接把两个表都标记为保留表）中未找到匹配的行将作为外部行添加到 VT2,生成VT3.如果FROM子句包含两个以上的表，则对上一个联接生成的结果表和下一个表重复执行步骤1到步骤3，直到处理完所有的表为止。 WHERE：对VT3应用WHERE筛选器。只有使为true的行才被插入VT4. GROUP BY：按GROUP BY子句中的列列表对VT4中的行分组，生成VT5. CUBE|ROLLUP：把超组(Suppergroups)插入VT5,生成VT6. HAVING：对VT6应用HAVING筛选器。只有使为true的组才会被插入VT7. SELECT：处理SELECT列表，产生VT8. DISTINCT：将重复的行从VT8中移除，产生VT9. ORDER BY：将VT9中的行按ORDER BY 子句中的列列表排序，生成游标（VC10). TOP：从VC10的开始处选择指定数量或比例的行，生成表VT11,并返回调用者。 工作中遇到复杂的sql逻辑：复杂的sql语句也是最基本的sql语句连接而成，所以最重要的是先要理清思路和逻辑，弄清自己要查哪几张表，要用哪几个字 段，表之间如何关联，将这些弄清，然后由简单到复杂，从最基本的sql写起，通过找共同点，实现表关联等。 select后是自己需要的字段from后是自己需要查询的多张表或者自己子查询得出的结果集where后是条件 是对from后的结果集进行筛选 多张表关联 最重要的是找共同点 比如通过userid 第一种方式就是通过join管理 第二种方式就是通过where条件子句 比如几个表的userid相等来筛选结果集 在处理复杂的业务查询时，先从逻辑层面理清几张表之间的关系以及自己需要的字段和数据 然后逐步拆分 从最简单的局部sql出发 一步步迭代出复杂的sql语句 这可以看做是写复杂脚本的原则： 由简单到复杂 逐步迭代 得出结果 最重要的还是在工作实践中多加总结 主动接触sql 正确性验证： 正确理解需求，跟同事，产品沟通清楚； 小规模数据测试+多场景数据测试； 同事间代码走查，团队评审代码； QA把关。 表设计Varchar字段类型长度设计:关于数据库Varchar字段类型长度设计问题 规范化－数据库设计（三大范式）12345678910create table xxh.sample( prjnum int not null, prjname varchar(200), emynum int not null, emyname varchar(200), salcategory char(1), salpackage int, primary key(prjnum) ); ibm数据库设计 sql笔记sql union: 将两个sql查询的结果合并到一起group by &amp; having\如果只是执行一条语句，有没有GO都一样如果多条语句之间用GO分隔开就不一样了每个被GO分隔的语句都是一个单独的事务，一个语句执行失败不会影响其它语句执行。例如：首先同时执行下边的语句select * from sysobjects where id=aselect getdate()你会发现会报错，并且不会显示任何结果集而你再执行select * from sysobjects where id=agoselect getdate()go你会发现尽管同样会报错，但结果集中包含select getdate()的结果\ mysql-Every derived table must have its own alias关于这条报错信息，意思是指每个派生出来的表都必须有一个自己的别名。\ _mysql_exceptions.OperationalError: (1248, &apos;Every derived table must have its own alias&apos;)\ 如下两条select语句可以说明这个问题： select count(*) from (select * from A where uuid=&apos;123&apos;) ; // error\ select count(*) from (select * from A where uuid=&apos;123&apos;) as a ; // okCAST 函数CAST 函数用于将某种数据类型的表达式显式转换为另一种数据类型。CAST()函数的参数是一个表达式，它包括用AS 关键字分割的原值和目标数据类型。 12//CAST (expression AS data_type)cast(minute(clttime)/5 AS int) mysql极客时间课程－mysql实战45讲]]></content>
      <categories>
        <category>数据存储处理 - database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop使用]]></title>
    <url>%2F2018%2F12%2F04%2Fsqoop%2F</url>
    <content type="text"><![CDATA[installsqoop install &amp;&amp; test 版本1.4.7的安装文档没有找到，使用1.99.7的文档装了半天 官网下载：sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz 解压 复制template配置文件并修改配置文件 123456789101112131415161718# Set Hadoop-specific environment variables here.#Set path to where bin/hadoop is available#export HADOOP_COMMON_HOME=export HADOOP_COMMON_HOME=/home/xxh/wkspc/hadoop/hadoop-2.7.7#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/home/xxh/wkspc/hadoop/hadoop-2.7.7#set the path to where bin/hbase is availableexport HBASE_HOME=/home/xxh/wkspc/hbase/hbase-2.1.0#Set the path to where bin/hive is availableexport HIVE_HOME=/home/xxh/wkspc/hive/apache-hive-1.2.2-bin#Set the path for where zookeper config dir isexport ZOOCFGDIR=/home/xxh/wkspc/zookeeper/zookeeper0/conf 测试 1234567891011121314151617181920212223242526sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root --password admin123sqoop import \--connect jdbc:mysql://localhost:3306/mysql \--username root \--password admin123 \--table user \--target-dir /sqoop/data_1 \--delete-target-dir \--num-mappers 1 \--fields-terminated-by &quot;\t&quot;# mysql on other computersqoop import \--connect jdbc:mysql://172.17.60.59:3306/mysql \--username root \--password Admin@321 \--table user \--target-dir /sqoop/data_user_59 \--delete-target-dir \--num-mappers 1 \--fields-terminated-by &quot;\t&quot; 数据库导入Sqoop 通mr作业从数据库中导入一个表，这个作业从表中抽取一行行记录，然后将记录写入HDFS 在导入开始之前，Sqoop使用jdbc 检查表，检索出表中所有的列、列的数据类型。这些sql数据类型被映射到java数据类型 导入控制重点在保证数据一致性 sqoop 后台实际上是运行了mr,用yarn 提交管理的 调试的时候发现提交完任务长时间不能执行，很可能是资源不足，也有可能是数据库权限问题先杀掉，同步本地的数据库试试 123# 8088 查下 app id,然后yarn application -kill application_1540448968752_0003 sqoop 同步json数据报错 12345678910111213141516171819Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/json/JSONObject at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:43) at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:785) at org.apache.sqoop.metastore.hsqldb.HsqldbJobStorage.createInternal(HsqldbJobStorage.java:399) at org.apache.sqoop.metastore.hsqldb.HsqldbJobStorage.create(HsqldbJobStorage.java:379) at org.apache.sqoop.tool.JobTool.createJob(JobTool.java:181) at org.apache.sqoop.tool.JobTool.run(JobTool.java:294) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252)Caused by: java.lang.ClassNotFoundException: org.json.JSONObject at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 12 more solve: 1234567By spending some more time reading blogs:I found solution as below:downloaded java-json.jar file from location http://www.java2s.com/Code/Jar/j/Downloadjavajsonjar.htmstored this jar file at location /usr/lib/sqoop/lib/java-json.jar Sqoop import exception java.lang.NoClassDefFoundError: org/json/JSONObject name node is in safe mode solve: 1hdfs dfsadmin -safemode leave name node is in safe mode sqoop 任务 一直僵尸在ACCEPED状态，不动 搜，跟着改了很多配置，最后看到日志中有[warn] error*,报错某某项配置的目录可用空间超过总空间百分之九十，导致伪分布式模式下唯一一个node不能用于是更改了其默认配置，并更改其yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage -&gt; 98.58088 UI上 Active nodes显示为1 最终的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#yarn-site.xml&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/yarnlocaldir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/yarnlogdir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage&lt;/name&gt; &lt;value&gt;98.5&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;# hdfs-site.xmldatanode的dfs.datanode.data.dir 不配置的话，会默认使用/tmp目录，我的系统装在小小的额固态上，左右基本没有空间剩余了，于是改到T级别的机械硬盘上了&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/namenode-namespace&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/datanodespace&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 参考：MapReduce jobs get stuck in Accepted state redshift 表 同步到 hive 一次性同步，整体迁移 123456789101112./bin/sqoop job --delete customer_order./bin/sqoop job --create customer_order \-- import \--connect jdbc:redshift://north-1.redshift.amazonaws.com.cn:59/prod \--driver com.amazon.redshift.jdbc42.Driver \--username boot_last \--table XXX.XXXX \--target-dir /hive/warehouse/XXXX/XXXX \--fields-terminated-by &apos;,&apos; \--lines-terminated-by &apos;\n&apos; \--hive-drop-import-delims \-m 1 没有顺序的数值列，所以无法切分，通过一个map任务来同步，也就是指定 -m 1? 也是不是没办法，比如讲某列进行hash,然后根据hash值来分片同步，使用sqoop同步时hash 就要自定义函数参考sqoop 同步所有列，所以指定了 table 后，不配置column的意思就是同步所有列 redshift jdbc Driver 记得添加 ，从maven上下载最新的jar,放到sqoop home的lib 中? sqoop 直接同步成hive 表]]></content>
      <categories>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH深入]]></title>
    <url>%2F2018%2F12%2F04%2Fssh%E6%B7%B1%E5%85%A5%2F</url>
    <content type="text"><![CDATA[SSH(Secure Shell)Secure Shell（安全外壳协议，简称SSH）是一种加密的网络传输协议，可在不安全的网络中为网络服务提供安全的传输环境。 SSH通过在网络中创建安全隧道来实现SSH客户端与服务器之间的连接[2]。虽然任何网络服务都可以通过SSH实现安全传输，SSH最常见的用途是远程登录系统，人们通常利用SSH来传输命令行界面和远程执行命令 在设计上，SSH是Telnet和非安全shell的替代品。Telnet和Berkeley rlogin、rsh、rexec等协议采用明文传输，使用不可靠的密码，容易遭到监听、嗅探和中间人攻击[4]。SSH旨在保证非安全网络环境（例如互联网）中信息加密完整可靠。 建立连接SSH以非对称加密实现身份验证[2]。身份验证有多种途径， 其中一种方法是使用自动生成的公钥-私钥对来简单地加密网络连接，随后使用密码认证进行登录；此时密钥是自动生成 另一种方法是人工生成一对公钥和私钥，通过生成的密钥进行认证，这样就可以在不输入密码的情况下登录。任何人都可以自行生成密钥。公钥需要放在待访问的计算机之中，而对应的私钥需要由用户自行保管。认证过程基于生成出来的私钥，但整个认证过程中私钥本身不会传输到网络中。 使用 登录远程主机执行命令 SSH也支持隧道协议、端口映射和X11连接。借助SFTP或SCP协议还可以传输文件 秘钥管理在类Unix系统中，已许可登录的公钥通常保存在用户 /home 目录的 ~/.ssh/authorized_keys 文件中[9]，该文件只由SSH使用。当远程机器持有公钥，而本地持有对应私钥时，登录过程不再需要手动输入密码。另外为了额外的安全性，私钥本身也能用密码保护。私钥会保存在固定位置，也可以通过命令行参数指定（例如ssh命令的“-i”选项）。ssh-keygen是生成密钥的工具之一。SSH也支持基于密码的身份验证，此时密钥是自动生成的。若客户端和服务端从未进行过身份验证，SSH未记录服务器端所使用的密钥，那么攻击者可以模仿服务器端请求并获取密码，即中间人攻击。但是密码认证可以禁用，而且SSH客户端在发现新密钥或未知服务器时会向用户发出警告。 所以aws ec2提供的秘钥就是一对秘钥对中的私钥，ec2主机中保存有公钥，通过-i指定私钥去匹配公钥，完成登录验证，也可以将自己机器的公钥放到ec2的/.ssh/authorized_keys 文件中，实现自己机器与ec2的免密登录，因为登录时默认会拿本地的私钥去匹配目标主机的/.ssh/authorized_keys文件中的公钥]]></content>
      <categories>
        <category>服务器端 - shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow2--基本用法]]></title>
    <url>%2F2018%2F12%2F04%2Ftensorflow2--%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Tensorflow 基本使用Tf使用要点 使用图(graph)表示计算任务 在会话(session)－上下文(context)中执行图 使用tensor表示数据 通过变量(Variable)维护状态 使用feed和fetch可以为任意的操作（arbitrary operation）赋值或者从中获取数据 综述TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels]. 一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是 tensorflow::Tensor 实例. 计算图TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op. 例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. TensorFlow 支持 C, C++, Python 编程语言. 目前, TensorFlow 的 Python 库更加易用, 它提供了大量的辅助函数来简化构建图的工作, 这些函数尚未被 C 和 C++ 库支持. 三种语言的会话库 (session libraries) 是一致的. 构建图构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入. TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了. 阅读 Graph 类 文档 来了解如何管理多个图. 不再摘抄了，发现没有多余的字符中文文档 在一个会话中启动图交互式使用]]></content>
      <categories>
        <category>机器学习、深度学习 - tensorflow</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow3--mnist]]></title>
    <url>%2F2018%2F12%2F04%2Ftensorflow3--mnist%2F</url>
    <content type="text"><![CDATA[mnist教程Mnist是tf圈的hello world mnist_softmax.py源码soft_max回归介绍： Softmax回归模型，该模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签 \textstyle y 可以取两个以上的值。Softmax回归模型对于诸如MNIST手写数字分类等问题是很有用的，该问题的目的是辨识10个不同的单个数字。Softmax回归是有监督的，不过后面也会介绍它与深度学习/无监督学习方法的结合。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================&quot;&quot;&quot;A very simple MNIST classifier.See extensive documentation athttps://www.tensorflow.org/get_started/mnist/beginners&quot;&quot;&quot;#Python的每个新版本都会增加一些新的功能，或者对原来的功能作一些改动。Python提供了__future__模块，把下一个新版本的特性导入到当前版本，于是我们就可以在当前版本中测试一些新版本的特性。from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport argparseimport sys#使用tf提供的input_data函数来导入数据from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tf#函数声明FLAGS = Nonedef main(_): # Import data mnist = input_data.read_data_sets(FLAGS.data_dir) # Create the model # 占位 &quot;&quot;&quot; 这里的x和y并不是特定的值，相反，他们都只是一个占位符，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。 输入图片x是一个2维的浮点数张量。这里，分配给它的shape为[None, 784]，其中784是一张展平的MNIST图片的维度。None表示其值大小不定，在这里作为第一个维度值，用以指代batch的大小，意即x的数量不定。输出类别值y_也是一个2维张量，其中每一行为一个10维的one-hot向量,用于代表对应某一MNIST图片的类别。 虽然placeholder的shape参数是可选的，但有了它，TensorFlow能够自动捕捉因数据维度不一致导致的错误。 我们现在为模型定义权重W和偏置b。可以将它们当作额外的输入量，但是TensorFlow有一个更好的处理方式：变量。 一个变量代表着TensorFlow计算图中的一个值，能够在计算过程中使用，甚至进行修改。在机器学习的应用过程中，模型参数一般用Variable来表示。 &quot;&quot;&quot; x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) #现在我们可以实现我们的回归模型了。这只需要一行！我们把向量化后的图片x和权重矩阵W相乘，加上偏置b，然后计算每个分类的softmax概率值 y = tf.matmul(x, W) + b # Define loss and optimizer # y_ = tf.placeholder(tf.float32, [None]) y_ = tf.placeholder(tf.int64, [None]) # The raw formulation of cross-entropy, # # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)), # reduction_indices=[1])) # # can be numerically unstable. # # So here we use tf.losses.sparse_softmax_cross_entropy on the raw # outputs of &apos;y&apos;, and then average across the batch. #可以很容易的为训练过程指定最小化误差用的损失函数，我们的损失函数是目标类别和预测类别之间的交叉熵。 cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y) #我们已经定义好模型和训练用的损失函数，那么用TensorFlow进行训练就很简单了。因为TensorFlow知道整个计算图，它可以使用自动微分法找到对于各个变量的损失的梯度值。TensorFlow有大量内置的优化算法 这个例子中，我们用最速下降法让交叉熵下降，步长为0.5. train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) #新建session sess = tf.InteractiveSession() #变量初始化 tf.global_variables_initializer().run() # Train # 循环训练1000次，每次都是取下100份数据作为输入，运行train_step for _ in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) # Test trained model # 首先让我们找出那些预测正确的标签。tf.argmax 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。 由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值， 而 tf.argmax(y_,1) 代表正确的标签，我们可以用 tf.equal 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。 这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：[True, False, True, True]变为[1,0,1,1]，计算出平均值为0.75。 correct_prediction = tf.equal(tf.argmax(y, 1), y_) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print(sess.run( accuracy, feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels &#125;)) # SAVE THE MODEL # 保存model builder = tf.saved_model.builder.SavedModelBuilder(&quot;/tmp/mnist/model/1620&quot;) builder.add_meta_graph_and_variables( sess, [tf.saved_model.tag_constants.SERVING] ) builder.save()if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser() parser.add_argument( &apos;--data_dir&apos;, type=str, default=&apos;/tmp/tensorflow/mnist/input_data&apos;, help=&apos;Directory for storing input data&apos;) FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) 卷积神经网络//Todo]]></content>
      <categories>
        <category>机器学习、深度学习 - tensorflow</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jep使用]]></title>
    <url>%2F2018%2F12%2F04%2Fusingjep%2F</url>
    <content type="text"><![CDATA[jep 简介whatJep: (Java Embeded Python) Jep takes a different route and embeds CPython in Java using JNI. Long story short, if you need to include CPython modules (such as numpy) Jep is the way to go. 要想使用cPython 模块，用jep加载就对了 whypython 在机器学习界大火。项目内机器学习算法工程师也是用python,lightgbm,tensorflow来开发模型，但是部署模型最好还是用java、scala 顺便说一下业界部署模型的方式： 全程python python 部署模型，提供rest接口，供外部调用，这样数据清洗类的操作可以通过java实现 Jython,JyNI 不能很好的支持基于c的python模块 jep 使用看上去好吊的样子，让我们来用一下试试效果怎么样 https://github.com/ninia/jep 2017年诞生，一直在开发中，很活跃 正片开始： jep wiki pip 安装 jep安装没有报错，但是安装完成后，java 新建Jep()对象时报错，找不到jep1234567891011121314151617181920Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError: no jep in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at jep.MainInterpreter.initialize(MainInterpreter.java:121) at jep.MainInterpreter.getMainInterpreter(MainInterpreter.java:97) at jep.Jep.&lt;init&gt;(Jep.java:232) at jep.Jep.&lt;init&gt;(Jep.java:228) at jep.Jep.&lt;init&gt;(Jep.java:140) at JepTest.JepDemo$.delayedEndpoint$JepTest$JepDemo$1(JepDemo.scala:27) at JepTest.JepDemo$delayedInit$body.apply(JepDemo.scala:9) at scala.Function0$class.apply$mcV$sp(Function0.scala:34) at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) at scala.App$class.main(App.scala:76) at JepTest.JepDemo$.main(JepDemo.scala:9) at JepTest.JepDemo.main(JepDemo.scala) 碰到这个问题的还真是不少，S.O.上类似的问题提示: 没有设置好 LD_LIBRARY_PATH变量 或者是没有设置好 1-Djava.library.path=/usr/local/lib/python2.7/dist-packages/jep/ github jep也有这样的issueBut提到的方法都不好使，只好去看看源码 看setup.py源码： cmdclass={ &#39;setup_java&#39;: setup_java, &#39;build_java&#39;: build_java, &#39;javadoc&#39;: javadoc, &#39;build_jar&#39;: build_jar, &#39;build&#39;: jep_build, &#39;build_ext&#39; : build_ext, &#39;build_scripts&#39;: build_scripts, &#39;install_lib&#39;: jep_install, &#39;clean&#39;: really_clean, &#39;test&#39;: test, }, 所以执行 python3 setup.py install_lib 才对， 执行打印如下： numpy include found at /home/xxh/anaconda3/lib/python3.6/site-packages/numpy/core/include running install_lib running build_py running build_ext ln -sf jep.cpython-36m-x86_64-linux-gnu.so /home/xxh/anaconda3/lib/python3.6/site-packages/jep/libjep.so 如此，如此将libjep.so文件生成后再设置相应的环境变量在idea的run-Edit Configurations中的VM中填入： 1-Djava.library.path=/home/xxh/anaconda3/lib/python3.6/site-packages/jep/ 也可以设置环境变量(划掉，不好使)： 1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/xxh/anaconda3/lib/python3.6/site-packages/jep/ 通过scala调用的python文件中，引用了不在环境变量中的其他python模块时会报错试用中，未完待续。。。版本一直在更新中，所以商用的话要小心]]></content>
      <categories>
        <category>编程语言 - scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>python</tag>
        <tag>jep</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解Docker]]></title>
    <url>%2F2018%2F12%2F04%2F%E4%BA%86%E8%A7%A3Docker%2F</url>
    <content type="text"><![CDATA[Docker What is Docker?我的理解： 在操作系统上新增的一层抽象 解决的痛点：同一台服务器上多应用间的依赖冲突问题 优势：很多，比如将多个应用部署在同一个云主机上，该云主机弹性扩展，这样能够节约不少成本 目前的项目暂时用不到docker 教程]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>微服务</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人人都是产品经理--note]]></title>
    <url>%2F2018%2F12%2F04%2F%E4%BA%BA%E4%BA%BA%E9%83%BD%E6%98%AF%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86-note%2F</url>
    <content type="text"><![CDATA[第一章 写给新手为什么要做产品经理因为世界等我们去改变，创造好的产品，改善坏的产品 我们到底是不是产品经理产品是什么产品是用来解决某个问题的东西，准确的说产品就是要同时解决用户的问题和公司的问题 因为作为产品经理要在用户目标和公司的商业目标之间寻找平衡 产品经理出现源于宝洁 产品经理的出现是为了适应公司发展的需要。随着企业越来越大,产品越来越多,越来越复杂,原来按职能划分部门的组织结构已经无法适应,所以出现了产品管理的矩阵型组织,而此时产品经理的主要职责是规划产品的生命周期 他们真的是在招产品经理吗它更多地侧重产品本身“从无到有”、“从有到优”的过程,更多地涉及了“产品规划、数据分析、用户研究、需求分析、功能设计、项目管理、敏捷方法”等内容,而不是如传统的产品经理那样,去做已经有了产品之后需要做的诸如管理产品、推广和营销产品的事情 产品经理概念进化 行业形态不同:成熟行业VS.新兴行业。 而互联网、软件行业是新兴行业,新兴市场,三天一小变,五天一大变,产品本身在不断取得突破,用户看什么都是新的,所以产品需要推陈出新,尽力先入为主,占领用户,主导用户习惯,这就导致了产品工作的重头戏在前期,从无到有,从有到优,偏重研发类创新。因此,互联网、软件行业的产品经理更重视产品功能本身的规划,需要“对市场发展趋势有敏锐的洞察力和创新意识及良好的分析、研判能力”,要能不断改进产品,要“深入了解业务,挖掘用户的多种需求,不断推出有竞争力的产品”,“制定所负责产品线的发展蓝图和实施路线图”。 实物VS虚拟物 - 生命周期不同：几年VS几个月 软件和互联网产品更推崇敏捷开发，虽然互联网、软件的项目更加不可控,但项目过程本身看起来并没有传统行业那么复杂,更接近“艺术”而不是“技术”,要依靠丰富的经验,于是产品经理也经常兼顾项目管理,这样自己可以在项目完成度和产品质量之间做平衡,对产品无疑也是一件好事。所以我们在招聘广告中看到公司要求产品经理“发起产品研发项目”,“组织资源实施产品,对其效益负责”。 赢利模式不同,单一卖产品赚钱VS多元赢利。 用户心态不同:花钱买VS.免费用。 互联网、软件产品更重视用户体验,相应的,出现了很多产品经理会涉及的工作内容,如交互设计、视觉设计、文案设计等 非典型产品经理有时高科技企业会采取类似的做法,让产品经理专心处理产品在工程和技术方面的问题,而把大部分营销决定交给另外的职能单位来负责。在这种情况下,产品经理可能会变成产品技术/应用方面的专家,他最主要的工作就是支援、协助销售人员,至于了解市场和从市场了解产品利益等工作,则另有他人代劳 不管整天只是写文档、做Demo,还是成了没钱没权的项目经理;或是求完销售求工程师,最后只能欺负客服,到处不招人待见……前辈们似乎也认可——人人都是产品经理,非典型成了多数,也就变成典型。 一线员工眼中的管理管理的能力,其实就是“在资源不足的情况下把事情做成”的能力,这里的资源在产品经理的工作中通常表现为以下几种形式。 第一,信息不足以决策。时间有限,能力有限,每次决策前不可能掌握所有信息,做决定时总是很头疼,我估计是“拍脑袋”拍得太多的原因。 第二,时间不足以安排周密的计划。总是接到3个月、1个月,甚至1个礼拜完成某项目的命令,每次都让我们张大嘴巴说不出话来,应承下来后如何计划?不过一次又一次的实践表明,办法总比困难多。 第三,人员不足以支持工作强度和难度。不但时间不足,人员也不足,就算数量足,能力够不够?能力够了,团队士气高不高?哪个公司不加班,又有多少公司有加班工资?但还得完成任务,难不难?难! 第四,资金不足以自由调配。俗话说钱要花在刀刃上,买机器要钱,招人要钱,产品推广要钱,而花这些钱的前提是公司还得赚钱,每一分钱都恨不得掰成两半用。 我真的想做,怎么入行用户会去想怎么用这个产品,才能带给自己更大的好处,产生更大的效用;而产品经理则习惯于绕过表象,从背后看问题的本质,思考怎么设计这个产品才能更好地平衡用户目标与商业目标 找到自己的位置几个可能的入行切入点你是做开发的,那就经常要参与需求评审,不妨比别人多用点功,每次都预先了解需求,多多思考,然后在评审会上对需求提出自己的合理建议,时间长了大家都会觉得你很有想法,做产品也许不错。关于职位,可以从“需求分析师”切入,这有些像系统分析的工作,比如业务逻辑、流程图,都是你已经很熟悉的,可以在这个过程中慢慢培养商业的感觉,重点体会某个产品功能是为了满足商业上的什么需求而做的。 一个产品经理的-1到3岁]]></content>
      <categories>
        <category>项目管理</category>
      </categories>
      <tags>
        <tag>需求分析</tag>
        <tag>产品</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用]]></title>
    <url>%2F2018%2F12%2F04%2F%E4%BD%BF%E7%94%A8git%2F</url>
    <content type="text"><![CDATA[git 不熟悉但是这种工具性的东西，就得边用边学 两个项目合并方法不一定最快，能用 project a 12git checkout -b &apos;newbranch&apos;git push origin newbranch project b 12345678910111213141516git init//copy .gitignoregit add .git commit -m &apos;b&apos;git remote add origin git@***.gitgit pull origin sparks3git statusgit add .git commit -m &apos;sfs&apos;git push origin master]]></content>
      <categories>
        <category>工欲善其事，必先利其器</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于单元测试]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%85%B3%E4%BA%8E%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[单元测试Java单元测试入门 简介介绍单元测试是先mock一些正常边界异常条件来对接口进行操作，并且期望接口返回什么内容，最后接口实现了之后再重新测试一遍。TDD：Test-Driven Development，测试驱动开发模式——旨在强调在开发功能代码之前，先编写测试代码。开发未动，测试先行。 为什么我们要进行单元测试：最后才修改一个 bug 的代价是在 bug 产生时修改它的代价的10倍。——《快速软件开发》测试任何可能的错误。单元测试不是用来证明您是对的，而是为了证明您没有错。单元测试是编写测试代码，用来检测特定的、明确的、细颗粒的功能。单元测试并不一定保证程序功能是正确的，更不保证整体业务是准备的。单元测试不仅仅用来保证当前代码的正确性，更重要的是用来保证代码修复、改进或重构之后的正确性。 单元测试 定义与特点 单元测试（unit testing）：是指对软件中的最小可测试单元进行检查和验证。 这个定义有点抽象，这里举几个单元测试的特性，大家感受一下：一般是一个函数配几个单元测试、单元测试不应该依赖外部系统、单元测试运行速度很快、单元测试不应该造成测试环境的脏数据、单元测试可以重复运行。 优点 单元测试使得我们可以放心修改、重构业务代码，而不用担心修改某处代码后带来的副作用。 单元测试可以帮助我们反思模块划分的合理性，如果一个单元测试写得逻辑非常复杂、或者说一个函数复杂到无法写单测，那就说明模块的抽象有问题。 单元测试使得系统具备更好的可维护性、具备更好的可读性；对于团队的新人来说，阅读系统代码可以从单元测试入手，一点点开始后熟悉系统的逻辑。 本文要解决的痛点 单测何时写？如果你的团队在坚持TDD的风格，那就是在编码之前写；如果没有，也不建议在全部业务代码编写完成之后再开始补单元测试。单元测试比较（最）合适的时机是：一块业务逻辑写完后，跟着写几个单元测试验证下。单测怎么写？分层单测：数据库操作层、中间件依赖层、业务逻辑层，各自的单元测试各自写，互相不要有依赖。单测运行太慢？dao层测试，使用H2进行测试，做独立的BaseH2Test、独立的test-h2-applicationContext.xml，只对dao的测试service层测试，依赖mockito框架，使用@RunWith(MockitoJUnitRunner.class)注解，就无需加载其他spring bean，具体用法对于依赖外部的中间件（例如redis、diamond、mq），在处理单测的时候要注意分开加载和测试，尤其是与dao的测试分开 作者：杜琪链接：https://www.jianshu.com/p/a8e17afd8c90來源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 使用场景什么是单元测试，在什么场景下使用单元测试合适单元测试到底是什么]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>单元测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用python进行数据处理]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%88%A9%E7%94%A8python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[2 引言－－大饼总结起来数据处理的日常工作分为以下几类，python 都可以轻松解决 与外界进行交互 读写各种格式的文件和数据库 准备 对数据进行清理、修整、整合、规范化。。。 转化对数据集做数学和统计运算以产生新的数据集 如：根据分组变量对一个大表进行聚合 建模和计算将数据跟统计模型、机器学习算法或其他计算工具联系起来 展示 创建交互式的或者静态的图片或文字摘要 来自bit.ly的1.usa.gov数据movie评分数据名字数据3 IPython工具跟普通的python cmd解释器差不多，但是输出可读性强 Tab自动补全提示 变量自动补全（搜索命名空间） 对象方法提示 对象内省 在变量的前面或者后面加一个 ? 就可以将该对象的一些通用信息显示出来： ?? 显示源代码 %run 执行脚本文件 %paste 执行粘贴板上的代码 …这个工具各种好，功能还是很强大的，也证明数据分析师真的很懒，只关心业务 4 NumPy基础：数组和矢量运算高性能科学计算和数据分析的基础包可以很容易的将数据传递给低级语言编写的外部库，外部库也能以numpy数组的形式将数据返回给python Numpy的ndarray:一种多维数组对象ndarray是一个通用的同构数据多维容器，也就是说，其中所有元素必须是相同类型的。每个数据都有一个shape和一个dtype(一个用于说明数组数据类型的对象)]]></content>
      <categories>
        <category>数据分析 - python</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
        <tag>数据处理</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在spark中使用S3]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%9C%A8spark%E4%B8%AD%E4%BD%BF%E7%94%A8S3%2F</url>
    <content type="text"><![CDATA[感受：aws S3 远远没有aws官方吹嘘的那么好用，特别是对hadoop的兼容，简直让人吐血，客观原因：在emr中使用S3 都折腾了好久，现在有个半成品吧，还是花钱买的aws官方支持 build.sbt ××hadoop-aws包与hadoop版本兼容及其敏感×× 1234567891011121314151617181920212223242526272829303132333435name := &quot;my-api-hdfs&quot;version := &quot;1.1.provided&quot;scalaVersion := &quot;2.11.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.core&quot; % &quot;jackson-core&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.core&quot; % &quot;jackson-databind&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.core&quot; % &quot;jackson-annotations&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.module&quot; %% &quot;jackson-module-scala&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.module&quot; % &quot;jackson-module-paranamer&quot; % &quot;2.7.8&quot;libraryDependencies += &quot;org.apache.hadoop&quot; % &quot;hadoop-client&quot; % &quot;3.1.0&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-awslibraryDependencies += &quot;org.apache.hadoop&quot; % &quot;hadoop-aws&quot; % &quot;3.1.0&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3libraryDependencies += &quot;com.amazonaws&quot; % &quot;aws-java-sdk-s3&quot; % &quot;1.11.338&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-corelibraryDependencies += &quot;com.amazonaws&quot; % &quot;aws-java-sdk-core&quot; % &quot;1.11.338&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-kmslibraryDependencies += &quot;com.amazonaws&quot; % &quot;aws-java-sdk-kms&quot; % &quot;1.11.338&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/org.apache.spark/spark-corelibraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.3.0&quot; % &quot;provided&quot;libraryDependencies += &quot;cn.**.aip&quot; %% &quot;cn-***-aip-common-databases&quot; % &quot;1.0.13&quot;mainClass in assembly := Some(&quot;AwsSupportS3&quot;)assemblyJarName in assembly := s&quot;my-api-hdfs1.1.0607.jar&quot; s3连接： scala版： 12345678910111213141516object SparkS3 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(s&quot;sparkS3&quot;).setMaster(&quot;yarn&quot;) val spark = new SparkContext(conf) spark.hadoopConfiguration.set(&quot;fs.s3n.impl&quot;, &quot;com.amazon.ws.emr.hadoop.fs.EmrFileSystem&quot;) spark.hadoopConfiguration.set(&quot;fs.s3n.endpoint&quot;, &quot;s3.cn-north-1.amazonaws.com.cn&quot;) spark.hadoopConfiguration.set(&quot;fs.s3n.awsAccessKeyId&quot;, &quot;××××××××&quot;) spark.hadoopConfiguration.set(&quot;fs.s3n.secret.key&quot;, &quot;××××××××××××HM5&quot;) //读 val testweet = spark.textFile(&quot;s3n://×××××-test-×××/testweet.json&quot;) //写 testweet.saveAsTextFile(s&quot;s3n://xuxianhong-test-data/data/$&#123;DateUtil.currentDate&#125;&quot;) spark.stop() &#125;&#125; java版(还是很感谢aws的支持)： 123456789101112131415161718192021222324252627282930public class AwsSupportS3 &#123; public static void main(String args[])&#123; new AwsSupportS3().awsCodeS3(); &#125; public void awsCodeS3()&#123; AmazonS3 s3client = new AmazonS3Client(new DefaultAWSCredentialsProviderChain()); //Map&lt;String, EC2MetadataUtils.IAMSecurityCredential&gt; credentialMap = EC2MetadataUtils.getIAMSecurityCredentials(); Region region = Region.getRegion(Regions.CN_NORTH_1); s3client.setRegion(region); SparkConf sparkConf = new SparkConf().setAppName(&quot;test app&quot;).setMaster(&quot;yarn&quot;); JavaSparkContext jsc = new JavaSparkContext(sparkConf); jsc.hadoopConfiguration().set(&quot;fs.s3n.impl&quot;,&quot;com.amazon.ws.emr.hadoop.fs.EmrFileSystem&quot;); jsc.hadoopConfiguration().set(&quot;fs.s3n.endpoint&quot;, &quot;s3.cn-north-1.amazonaws.com.cn&quot;); jsc.hadoopConfiguration().set(&quot;fs.s3n.awsAccessKeyId&quot;, &quot;××××&quot;); jsc.hadoopConfiguration().set(&quot;fs.s3n.awsSecretAccessKey&quot;, &quot;×××××××××/×××/××&quot;); JavaRDD&lt;String&gt; resource = jsc.textFile(&quot;s3n://××××-××-data/××/ham.txt&quot;); resource.cache(); String date = DateUtil.currentDate(); resource.saveAsTextFile(&quot;s3n://×××××-×××××-data/data/&quot;+date); List&lt;String&gt; list = resource.collect(); System.out.println(&quot;---------------------&quot;+resource.count()); //System.out.println(list.toString()); jsc.close(); &#125;&#125; 有别的使用再补充]]></content>
      <categories>
        <category>数据存储处理 - spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>S3</tag>
        <tag>aws</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发-java 编程思想]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%B9%B6%E5%8F%91-java%2F</url>
    <content type="text"><![CDATA[并发并发介绍，想要真正编写一些复杂的程序钱，应该学一些专门介绍并发的书籍 并发的多面性使用并发时需要解决的问题有多个，而实现并发的方式也有多种，并且两者之间没有明显的映射关系 并发解决的问题大体上可以分为速度和设计可管理性两种 更快的执行并发通常是提高运行在单处理器上的程序性能直觉上思考在单核心上顺序运行程序应该是最高效的，因为没有上下文切换的代价 但是世界上有讨厌的 阻塞 单处理器系统中性能提高的常见实例就是事件驱动编程 操作进程进程是运行在它自己的地址空间内的自包容的程序 而java所使用的并发系统会共享诸如内存和IO这样的资源,java操作的是代码代表的进程生成的一系列线程 java 可能并没有函数式编程语言适合并发 改进代码设计基本的线程机制并发编程将程序划分为多个分离的独立运行的任务，通过多线程机制，这些独立任务中的每一个都将由执行线程来驱动。线程的调度由操作系统负责 代码 定义任务通过Runnable接口定义任务 Thread class将Runnable对象转变为工作按任务的传统方式是把它提交给一个Thread构造器 executor非常常见的情况是，单个Executor被用来穿件和管理系统所有的任务 在任何线程池中，现有线程在可能的情况下，都会被自动复用 从任务中产生返回值使用Callable接口，而不是Runnable,Callable接口返回一个泛型的Future,可以使用get()方法等待结果返回 休眠sleep 优先级优先调度优先级比较高的线程 书上的例子在我的电脑上不好使 让步如果知道已经完成了在run()方法的循环的一次迭代过程中所需的工作，就可以给线程带哦度机制一个暗示：我的工作已经做得差不多了，可以让别的线程使用CPU了 这个暗示就是使用yield()方法来作出 不能依赖yield,实际上，yield经常被滥用 后台线程所谓后台(daemon)线程,是指在程序运行的时候在后台提供的一种通用服务的线程， 这种线程并不属于程序中不可或缺的部分后台线程退出时，会杀死进程中所有与的后台线程。]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发，java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列化与反序列化]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[调用服务，避免不了序列化 rest+jsonjson真是操蛋，特别是不同的语言，不同的json库间转换，很痛苦,所以需要思考还有没有其他的序列化方式 最好用的scala中的json工具： play-json-extensions +22 field case class formatter and more for play-json 大数据场景下case class 必备,是play-json 的升级版跟spray-json也很像 1234567891011121314151617181920package data.creationimport ai.x.play.json.Jsonximport common.HttpClientHelperimport data.beans.LastInfoimport play.api.libs.json.Json/** * Created by xxh on 18-7-30. */object LastInfoHelper &#123; implicit lazy val lastInfoFormat = Jsonx.formatCaseClass[LastInfo] def getLastInfo(url:String,email:String,shoeLastBaseNo:String,basicsize:Int): LastInfo =&#123; val lastArray = new HttpClientHelper().post(url = url, postJsonObj = s&quot;&quot;&quot;&#123;&quot;email&quot;:&quot;$email&quot;,&quot;shoeLastBaseNo&quot;:&quot;$shoeLastBaseNo&quot;,&quot;basicsize&quot;:$basicsize&#125;&quot;&quot;&quot;) val lastJsonArray = Json.parse(lastArray) val lastInfo = lastJsonArray(0).as[LastInfo] lastInfo &#125;&#125;]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>序列化</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[感悟：项目中使用了微服务架构，相对于单体的应用，整个项目被拆成另外N个小应用，分散在各个服务器上，好处是，项目中的各个部分互不影响(接口不变的情况下)，缺点是，应用过于分散，难以管理、 所以有必要了解一下别人家的微服务架构是怎么设计的 大家都知道，基于单体（Monolith）和微服务（Microservice）架构的争论已经存在多年，正如我们对胖客户端、瘦客户端孰好孰坏的争论一样，有必然的历史演化，也有各自的优缺点。架构师们总是在考虑，我们是要一个中心化、全能多才的单体，还是百花齐放、各自为政的微服务群体，各种基于成本、交互、部署等等的讨论应该不会停下脚步。这里，我们不做过多的深入探讨和介绍，而本文正是这些讨论中的一个很好的例子，供大家思考。 随着系统变得越来越大，最终会达到一个临界点，作为一个单体（monolith）它变得难以管理。每一行代码的添加，都会让系统变得更加难以理解、变更和重用。虽然微服务（microservice）试图解决这些问题，但也带来了额外的复杂性以及集成的成本。 一个经过优化的微服务架构案例 优势: 模块较小，低耦合，提高开发人员开发效率 容易理解和测试每个服务 容易定位问题和解决故障,减少了关联故障 难点： 微服务的模块间需要n多契约，文档，需要时间来维护接口和文档 devops显得很重要 API gateway :lagom]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[掌握HttpResponse]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%8E%8C%E6%8F%A1HttpResponse%2F</url>
    <content type="text"><![CDATA[HTTP请求报文和HTTP响应报文 http 报文是面向文本的，报文中的每一个字段都是一些ASCII码串，各个字段的长度是不确定的。分两种，请求报文和响应报文。 请求报文//ToDo 响应报文http响应由四部分组成，分别是： 状态行、消息报头、空行、 响应正文格式: //[] 状态行格式如下： HTTP-Version Status-Code Reason-Phrase CRLF 其中，HTTP-Version表示服务器HTTP协议的版本；Status-Code表示服务器发回的响应状态代码；Reason-Phrase表示状态代码的文本描述。状态代码由三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。 1xx：指示信息–表示请求已接收，继续处理。2xx：成功–表示请求已被成功接收、理解、接受。3xx：重定向–要完成请求必须进行更进一步的操作。4xx：客户端错误–请求有语法错误或请求无法实现。5xx：服务器端错误–服务器未能实现合法的请求。常见状态代码、状态描述的说明如下。 200 OK：客户端请求成功。400 Bad Request：客户端请求有语法错误，不能被服务器所理解。401 Unauthorized：请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用。403 Forbidden：服务器收到请求，但是拒绝提供服务。404 Not Found：请求资源不存在，举个例子：输入了错误的URL。500 Internal Server Error：服务器发生不可预期的错误。503 Server Unavailable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常，举个例子：HTTP/1.1 200 OK（CRLF）。]]></content>
      <categories>
        <category>服务器端 - web开发</category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>HttpResponse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[攒技能]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%94%92%E6%8A%80%E8%83%BD%2F</url>
    <content type="text"><![CDATA[抬头看天 低头走路分类 项目管理编程语言数据结构服务器端前端数据可视化数据存储处理分布式计算数据分析机器学习、深度学习工欲善其事，必先利其器误入歧途书签数据采集瞎说]]></content>
      <categories>
        <category>书签</category>
      </categories>
      <tags>
        <tag>技能树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[敏捷开发流程]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[定义敏捷不是指某一种具体的方法论、过程或框架，而是一组价值观和原则。符合敏捷价值观和原则的开发方法包括：极限编程（XP），Scrum，精益软件开发（Lean Software Development），动态系统开发方法（DSDM），特征驱动开发（Feature Driver Development），水晶开发（Crystal Clear）等等。所有这些方法都具有以下共同特征： 迭代式开发 ，整个开发周期分为几个迭代周期 - 增量交付 每个迭代周期结束后逐步交付使用，而不是整个开发过程结束的时候一次性交付使用 - 开发团队和用户反馈推动产品开发 持续集成 开发团队自我管理 敏捷实战 需求评审 Story划分 人员划分 方案设计 方案评审 任务拆分 开发 端到端测试 压力测试＋集成测试 关于需求分析： 一部分是项目总体需求分析时确定的，另一部分是随着项目的发展，新增的业务需求 关于方案设计：在项目需求评审会议后，划分了story、确定了人员分配后，根据项目的总体框架进行的方案设计 主要分 1 概述 41.1 目的 41.2 术语、定义和略缩词 41.3 参考资料 42 业务逻辑 43 主题表设计 74 性能设计 95 平滑升级 然后做方案评审 团队内部任务拆分 开发 测试 压力测试 和 集成测试]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>敏捷开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[数据分析基础线性代数微积分]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言数据结构研究的问题包括数据在计算机中存储、组织、传递和转换的过程及方法，这些也是构成与支撑算法的基础。我们在编写计算机程序来解决应用问题时，最终都要归结并落实到这两个问题上 “程序 = 数据结构 + 算法” 算法及其复杂度计算机与算法算法定义所谓算法，就是在特定计算模型下，在信息处理过程中为了解决某一类问题而设计的一个指令序列。比如，对于“过直线上某一点作垂直线”这一问题，绳索及奴隶就构成了一个特定的计算模型，古埃及人基于这一模型所设计的算法一.1，就是在这一计算模型下解决这一问题的一个算法。 算法性能的分析与评价三个层次 合法性，语法正确，能够执行 不同算法的效率,设计和使用适宜的数据结构 软件工程，全局考虑 如果将借助计算机解决实际问题比作书法，那么在第一层次上就相当于学习点、横、竖、撇和捺等基本笔画，而第二层次则相当于学习如何将不同的笔画按照一定的间架结构组成有意义的不同汉字。然而这还远远不够，作为一幅完整的书法作品，还需要在汉字之间形成大小、粗细、疏密、浓淡、奇正及枯润等方面的搭配与呼应，也就是要讲求章法。在利用计算机解决实际问题的过程中，同样存在这样一个更高层次的问题：倘若软件的规模大到任何个人或少数人都不足以开发出来地步，而且在其生命期内软件也需要很多人的协作才能得以维护，则需要进一步考虑一些更为全局性的问题，这些问题都属于软件工程学的范畴， 时间复杂度及其度量T(n)称作算法的时间复杂度。 每一次基本操作都可以在常数时间内完成，因此如果根据算法所需执行的基本操作次数来表示 T(n)，就可以更加客观地反映算法的效率。 空间复杂度衡量算法性能的另一个重要方面，就是算法所需使用的存储空间量，即算法空间复杂度观察结论一.1 就渐进复杂度的意义而言，在任何一个算法的任何一次运行过程中，其实际占用的存储空间都不会多于其间执行的基本操作次数。 #### 算法复杂度及其分析 O(1)⎯⎯取非极端元素O(logn)⎯⎯进制转换O(n)⎯⎯数组求和O(n^2)⎯⎯起泡排序O(2^r)⎯⎯幂函数 计算模型可解性有效可解##### 下界 递归栈与队列栈栈是存放对象的一种特殊容器，在插入与删除对象时，这种结构遵循后进先出（Last-in-first-out，LIFO）的原则入栈（Push）和退栈（Pop）。 栈 ADT java.util.Stack push()、pop()、peek()（功能等价于 top()）、getSize()以及 empty()（功能等价于 isEmpty()）。在遇到空栈时，方法 pop()和 peek()都会报意外错 ExceptionStackEmpty。 实现 JVM中的栈]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志打印规范]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%97%A5%E5%BF%97%E6%89%93%E5%8D%B0%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[背景近期想要通过分析日志来查看应用的性能指标，才发现日志根本没有规范，真是罪过 日志规范最佳日志实践（v2.0日志的优化是一件需要持续不断投入精力的事，需要不断从错误中学习，现在应用分析要求是在应用执行时间维度上给出分析指标：那么这次最初的日志规范就是时间、地点、人物(ID)、事情 未完待续…]]></content>
      <categories>
        <category>服务器端 - 日志</category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识机器学习]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01%2F</url>
    <content type="text"><![CDATA[神经网络神经网络是一种数学模型，是存在于计算机的神经系统，由大量的神经元相连接并进行计算，在外界信息的基础上，改变内部的结构，常用来对输入和输出间复杂的关系进行建模。 神经网络由大量的节点和之间的联系构成，负责传递信息和加工信息，神经元也可以通过训练而被强化。 rel 生物神经网络现代科学还不能完全解释生物学习，记忆的过程，也就无从解释生物神经元建立相互关系的过程 人工神经网络目的：虽然不知道生物神经网络的内部原理，但是想人工模仿出一个有相同功能的神经网络 首先, 替代掉生物神经网络的, 就是已经成体系的人工神经网络. 所有神经元之间的连接都是固定不可更换的, 这也就是说, 在人工神经网络里, 没有凭空产生新联结这回事. 人工神经网络典型的一种学习方式就是, 我已经知道吃到糖果时, 手会如何动, 但是我想让神经网络学着帮我做这件动动手的事情. 所以我预先准备好非常多吃糖的学习数据, 然后将这些数据一次次放入这套人工神经网络系统中, 糖的信号会通过这套系统传递到手. 然后通过对比这次信号传递后, 手的动作是不是”讨糖”动作, 来修改人工神经网络当中的神经元强度. 这种修改在专业术语中叫做”误差反向传递”, 也可以看作是再一次将传过来的信号传回去, 看看这个负责传递信号神经元对于”讨糖”的动作到底有没有贡献, 让它好好反思与改正, 争取下次做出更好的贡献. 这样看来, 人工神经网络和生物神经网络的确不是一回事. 二者间的区别 人工神经网络靠的是正向和反向传播来更新神经元, 从而形成一个好的神经系统, 本质上, 这是一个能让计算机处理和优化的数学模型. 而生物神经网络是通过刺激, 产生新的联结, 让信号能够通过新的联结传递而形成反馈. 虽然现在的计算机技术越来越高超, 不过我们身体里的神经系统经过了数千万年的进化, 还是独一无二的, 迄今为止, 再复杂, 再庞大的人工神经网络系统也不能替代我们的小脑袋. 我们应该感到自豪, 也应该珍惜上天的这份礼物. 梯度下降optimization(优化问题)机器学习是一种优化问题的方法手段准确的说是梯度下降中的一员optimization: Newton’s method(牛顿法) Least Squares method( 最小二乘法) Gradient Descent(梯度下降法) 误差方程误差方程用来计算预测出来的和我们实际中值有多大差别，在预测数值的问题中，我们常用平方差来代替 cost = (predicted - real)^2 = (Wx-y)^2 优化初始值：(W-0)^2 梯度线躺平的点，其W参数为最优解 ##### 全局最优and局部最优 神经网络黑盒我们 把输入进行数字化，特征化，放入神经网络黑盒中，黑盒中进行了一系列的特征转化，最终得到我们的输出]]></content>
      <categories>
        <category>机器学习、深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[一开始接触用python 写爬虫用的是bs4 request urllib2 这些库，简单爬取网页简直不要太简单自己在上家写的那些找不到了，逻辑是 Python 爬虫脚本 爬取解析网页，提取有效字段，整理写入csv文件供市场部门使用]]></content>
      <categories>
        <category>数据采集 - 爬虫</category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>Http</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用Linux shell收集应用日志文件]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%94%A8shell%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[场景：应用的日志文件存储在N台不同的服务器上，现在要统一收集到同一台机器上做分析，用Linux shell 脚本实现设计： 从一台服务器上收集数据（ftp get） 从多台服务器上收集（循环 ftp get） 定时从多台机器上按照一定规则收集（定时器） 思路1：读配置文件，去指定服务器的指定目录下载文件1234567891011121314151617181920212223242526272829303132#!/bin/bash#dirname 获取当前文件的路径：全路径时为绝对路径 相对路径时 为相对路径，所有要用 pwd获取绝对路径basedir=$(cd $(dirname $0); pwd)cd $basedirconfigFile=$basedir/ftphost.confhomedir=$(awk -F &apos;=&apos; &apos;/^homedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)ftphomedir=$(awk -F &apos;=&apos; &apos;/^ftphomedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)grep -i -E -w &quot;ftp|sftp&quot; $configFile | awk -v hdir=$homedir -v fdir=$ftphomedir -F &quot;,&quot; &#123; protocol=$1 ip=$2 port=$3 user=$4 password=$5 ftp -n&lt;&lt;FTPDopen $ipuser $user $passwordbinarypassivelcd $hdirpromptif [[ ! -d $fdir ]];then mkdir $fdirficd $fdirput $2closequitFTPD 更新1234567891011121314151617181920212223242526272829303132333435363738394041424344#! /bin/bash#dirname 获取当前文件的路径：全路径时为绝对路径 相对路径时 为相对路径，所有要用 pwd获取绝对路径basedir=$(cd $(dirname $0); pwd)echo $basedircd $basedirconfigFile=$basedir/ftphost.confftpscriptfile=$basedir/ftpscript.shhomedir=$(awk -F &apos;=&apos; &apos;/^homedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)ftphomedir=$(awk -F &apos;=&apos; &apos;/^ftphomedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)echo &quot;homedir: &quot;$homedirecho &quot;ftphomedir&quot; $ftphomedirecho &quot;#ftp远程获取需要的文件&quot; &gt;&gt;$ftpscriptfilegrep -i -E -w &quot;ftp|sftp&quot; $configFile | awk -v hdir=$homedir -v fdir=$ftphomedir -F &quot;,&quot; &apos;&#123; # protocol=$1 # ip=$2 # port=$3 # user=$4 # password=$5 print &quot;ftp -n&lt;&lt;FTPD&quot; print &quot;open &quot;ip print &quot;user &quot; user &quot; &quot; password print &quot;binary&quot; print &quot;passive&quot; print &quot;cd &quot; fdir &#125;&apos; &gt;&gt;$ftpscriptfile echo &quot;prompt&quot; &gt;&gt;$ftpscriptfile echo &quot;if [[ ! -d \$3 ]];then&quot; &gt;&gt;$ftpscriptfile echo &quot; mkdir \$3&quot; &gt;&gt;$ftpscriptfile echo &quot;fi &quot; &gt;&gt;$ftpscriptfile echo &quot;cd \$3&quot; &gt;&gt;$ftpscriptfile echo &quot;if [[ ! -d \$4 ]];then&quot; &gt;&gt;$ftpscriptfile echo &quot; mkdir \$4&quot; &gt;&gt;$ftpscriptfile echo &quot;fi &quot; &gt;&gt;$ftpscriptfile echo &quot;cd \$4&quot; &gt;&gt;$ftpscriptfile echo &quot;lcd \$1&quot; &gt;&gt;$ftpscriptfile echo &quot;put \$2&quot; &gt;&gt;$ftpscriptfile echo &quot;close&quot; &gt;&gt;$ftpscriptfile echo &quot;quit&quot; &gt;&gt;$ftpscriptfile echo &quot;FTPD&quot; &gt;&gt;$ftpscriptfile 12345678910111213141516171819202122232425262728293031 1 filterAndGet() 2 &#123; 3 srcfile=$1 4 key=$2 5 value=$3 6 tgtfile=$4 7 shift 4 8 if [[ $# -gt 0 ]];then 9 ids=$*10 else11 ids=012 fi1314 awk -F &quot;,&quot; &apos;BEGIN &#123;15 split(&apos;&quot;\&quot;$ids&quot;\&quot;&apos;, indexs,&quot; &quot;)16 &#125;&#123;17 if($&apos;$key&apos;==&quot;&apos;$value&apos;&quot;) &#123;18 for (i=1;i&lt;=length(indexs);i++)&#123;19 if(indexs[i]==9)&#123;20 system(&quot;\\echo -n $(\\date -d \&quot;2000-01-01 +&quot; $indexs[i] &quot; seconds\&quot; +\&quot;%Y-%m-%d %H:%M:%S\&quot;)&quot;)21 printf(&quot;.%03d,&quot;,$28)22 &#125;23 else24 printf(&quot;%s,&quot;,$indexs[i])25 &#125;26 printf(&quot;\n&quot;);27 &#125;28 &#125;&apos; $srcfile &gt;&gt; $tgtfile29 &#125;filterAndGet $1 39 0 $2/a_$&#123;day&#125;.csv 9 14 15 52 2 3 5 10 11 53 54 55 56 57 58 59 88 16 6 7 8 21 35 34 23 86 17 19 20 思路2：配置文件中配置多个IP，循环去取文件思路3：增加文件过滤规则，定时器规则]]></content>
      <categories>
        <category>数据采集 - 日志收集</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[始终－程序员的自我修养]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A7%8B%E7%BB%88%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%2F</url>
    <content type="text"><![CDATA[原文 计算机发展史计算机三大件：中央处理器cpu、内存、IO控制芯片 计算机硬件的房展很大程度上归功于CPU技术的更新，CPU核心频率的不多提升 北桥芯片：用来协调CPU内存显卡之间的高速内存交换（内存的频率远远低于CPU的频率） 不过连接在北桥上的设备还是速度一级棒的，相对于键盘、硬盘来讲，所以CPU不会直接带这些低俗设备玩，把这些设备交给南桥来管理，然后通过内部总线来沟通 i7后，cpu将北桥集成在内 操作系统 计算机科学领域的任何问题，都可以通过增加一个间接的中间层来解决 操作系统就是有很多任务要交给计算机去做的多个用户用户与计算机硬件间的中间层 操作系统经历了，多道程序、分时系统、多任务系统 在多任务系统中，操作系统接管了所有硬件资源并持有对硬件控制的最高权限。在操作系统中执行的程序，都以进程的方式运行在更低的权限中。所有的硬件资源，由操作系统根据进程的优先级以及进程的运行状况进行统一的调配。 进程与线程操作系统是以进程为单位去分配空间和执行的 每个线程有自己独立的栈，同时多个线程共享进程空间中的数据 线程安全竞争如果有多个线程尝试对同一份数据进行写入操作，那么最终结果可能回事不可预期的 原子性单条指令是原子性的，是不可被打断的，但是单条指令能力有限，我们只能通过同步和锁机制实现多条指令的原子性 同步与锁 在这里，同步是一种规则，而锁则是实现这种规则的具体方法。 所谓同步，指的是多线程程序里，多个线程不得同时对某一共享变量进行访问。锁是实现同步的一种具体方案——准确地说，这是一种非常强的方案。锁有多种形式，最符合直觉的锁是所谓的互斥量（Mutex）。具体来说，线程在访问某个共享变量的时候，必须先获取锁；如果获取不到锁，那么就必须等待（或者进行其他操作，总之不许访问这个变量）；在结束对这个变量的访问之后，持有锁的线程应当释放。 值得一提的是，所作为一种同步手段，是非常强的。但是，这种强，仅限于逻辑层面。在实际情况中，编译器优化、CPU 动态调度，都有可能打破锁对于同步的保护。这时候，这些优化就变成了过度优化。 过度优化对线程安全的破坏线程安全不好控制，所以基于actor的akka必火]]></content>
      <categories>
        <category>计算机硬件、操作系统</category>
      </categories>
      <tags>
        <tag>程序员的自我修养</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[《算法》基础在计算机科学领域,我们用算法这个词来描述一种有现的解决问题的方法。算法是计算机科学的基础,是这个领域研究的核心。 #### 基础编程模型 我们学习算法的方法是用 Java 编程语言编写的程序来实现算法。这样做是出于以下原因: 程序是对算法精确、优雅和完全的描述; 可以通过运行程序来学习算法的各种性质; 可以在应用程序中直接使用这些算法。相比用自然语言描述算法,这些是重要而巨大的优势。 java 程序的基本结构]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库清洗规则]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%84%9A%E5%9E%8B%E6%95%B0%E6%8D%AE%E3%80%81%E6%A5%A6%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E6%96%B9%E6%A1%88%E7%A8%BF%2F</url>
    <content type="text"><![CDATA[脚型数据、楦数据清洗方案稿背景脚型数据、楦数据均以入库（待确认） 需要对数据做最基本，根据具体业务需要的数据清洗 ## 前提 数据仓库中的脚数据、楦数据的数据一致性校验完成 ## 清洗规则 明确数据的业务范围明确数据的语义两个表各字段的意义 数据中存在的错误界定及处理方法表级别错误主要包括数据记录不合理的重复、数据记录的缺失。 依赖数据入库操作时数据一致性校验的完成 #### 行级别错误 对于数据中存在的错误，处理的原则是尽最大可能去发现并修复，对于不能修复的错误，则需要给出警示。 对于行级错误的处理，从找出异常数据入手。 对于每个字段，发现异常数据可以通过以下（不限于）几种方式： 对于标量，如果离群则记为异常。 取值不在字典表范围内。 不符合数据的语义（在当前语义上无法解释）。 与其它字段的信息不一致。 异常数据处理流程： 首先判断是不是错误，如果不是错误，那我们只需要加以记录，这可以帮助我们更深入的认识数据； 如果是错误，试着去分析错误产生的原因，推测出正确值；如果能推测出正确值，那么推算出正确值。 如果不能推测出正确值，那么将其置为空值（空值至少不会提供错误的信息，其不良影响要弱于错误的信息所来带来的）。 对数据做规范化、统一化不规范、不统一的数据可能并不影响信息表达的准确性，却对数据分析和数据处理有很大的影响。 我们所要做的有两方面： 含义相同的字段，采用统一的规范的形式来表达。 目前主要包括：- 日期字段格式的统一，全部采用标准Unix时间戳表示。 对于同一字段，相同的信息，采用相同的表达。 主要针对字符型的字段。比如：北京、北京市、【北京】都是表达相同的含义，我们需要将其统一；男女与0,1,2的映射 ３． 空值处理、null值处理]]></content>
      <categories>
        <category>datawarehouse</category>
      </categories>
      <tags>
        <tag>datawarehouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行话]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%A1%8C%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[胶水代码： 在传统的开发模式中，我们很容易陷入“胶水代码”的陷阱里。所谓的“胶水代码”，顾名思义，就是仅仅用来保持用户界面数据、状态同步的函数调用的集合体。这些函数调用扯不断，理还乱，并且使代码变的非常冗长、易出错、不易维护。 编程模型：当面对一个新问题时。通常的想法是通过分析，不断的转化和转换，得到本质相同的熟悉的、或抽象的简单的一个问题，这就是化归思想。把初始的问题或对象称为原型，把化归后的相对定向的模拟化或理想化的对象称为模型。 编程模型，简单的可以理解他就是模板，遇到相似的问题就可以方便的依模板解决，这样就简化了编程问题。不同的编程环境和不同的应用对象有不同的编程模型。原子操作：原子（atom）本意是“不能被进一步分割的最小粒子”，而原子操作（atomic operation）意为”不可被中断的一个或一系列操作” 。为什么我们需要MQ消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过写和检索出入列队的针对应用程序的数据（消息）来通信，而无需专用连接来链接它们。消息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信，直接调用通常是用于诸如远程过程调用的技术。排队指的是应用程序通过队列来通信。队列的使用除去了接收和发送应用程序同时执行的要求为什么我们需要MQ 何为流如何理解编程语言中「流」（stream）的概念如何形象的理解计算机中“流”的概念？ 数据规整(munge/munging/wrangling)指的是将非结构化和散乱数据处理为结构化或整洁形式的整个过程]]></content>
      <categories>
        <category>瞎说</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跳板机的使用]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%B7%B3%E6%9D%BF%E6%9C%BA%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[跳板机是什么跳板机属于内控堡垒机范畴，是一种用于单点登陆的主机应用系统。2000年左右，高端行业用户为了对运维人员的远程登录进行集中管理，会在机房里部署跳板机。跳板机就是一台服务器，维护人员在维护过程中，首先要统一登录到这台服务器上，然后从这台服务器再登录到目标设备进行维护。但跳板机并没有实现对运维人员操作行为的控制和审计，使用跳板机过程中还是会有误操作、违规操作导致的操作事故，一旦出现操作事故很难快速定位原因和责任人。PS:风险高，业界偏向于使用堡垒机 跳板机原理端口转发 Jsch 使用通过跳板机访问数据库 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147import com.jcraft.jsch.JSch;import com.jcraft.jsch.Session;import com.jcraft.jsch.UIKeyboardInteractive;import com.jcraft.jsch.UserInfo;import javax.swing.*;import java.awt.*;import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class PortForwardingL4Aip &#123; public static void main(String[] arg)&#123; int lport; String rhost; int rport; try&#123; JSch jsch=new JSch(); //跳板机信息设置 Session session=jsch.getSession(&quot;跳板机用户名&quot;, &quot;跳板机ip&quot;, 跳板机ssh端口); session.setPassword(&quot;跳板机用户密码&quot;); //跳板机本地转发端口 lport = 9527; //通过跳板机访问的服务器host rhost = &quot;rhost&quot;; //通过跳板机访问的服务器待转发端口 rport = 61539; // username and password will be given via UserInfo interface. //MyUserInfo实现了接口UserInfo，主要是为获得运行执行的用户信息提供接口。大部分实现方法中，没有做实质性的工作，只是输出一下trace信息，帮助判断哪个方法被执行过。 //通过 session.setPassword(&quot;跳板机用户密码&quot;); 设置了密码，不使用UserInfo接口 //UserInfo ui=new MyUserInfo(); //session.setUserInfo(ui); //不需要讨厌的提示 session.setConfig(&quot;StrictHostKeyChecking&quot;, &quot;no&quot;); session.connect(); int assinged_port=session.setPortForwardingL(lport, rhost, rport); System.out.println(&quot;localhost:&quot;+assinged_port+&quot; -&gt; &quot;+rhost+&quot;:&quot;+rport); // 数据库连接 Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;); Connection conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:9527/sync?user=xu_foot_last&quot;); Statement statement = conn.createStatement(); ResultSet resultSet = statement.executeQuery(&quot;show databases&quot;); resultSet.first(); System.out.print(resultSet.getString(&quot;Database&quot;)); &#125; catch(Exception e)&#123; System.out.println(e); &#125; &#125; public static class MyUserInfo implements UserInfo, UIKeyboardInteractive&#123; public String getPassword()&#123; return passwd; &#125; public boolean promptYesNo(String str)&#123; Object[] options=&#123; &quot;yes&quot;, &quot;no&quot; &#125;; int foo=JOptionPane.showOptionDialog(null, str, &quot;Warning&quot;, JOptionPane.DEFAULT_OPTION, JOptionPane.WARNING_MESSAGE, null, options, options[0]); return foo==0; &#125; String passwd; JTextField passwordField=(JTextField)new JPasswordField(20); public String getPassphrase()&#123; return null; &#125; public boolean promptPassphrase(String message)&#123; return true; &#125; public boolean promptPassword(String message)&#123; Object[] ob=&#123;passwordField&#125;; int result= JOptionPane.showConfirmDialog(null, ob, message, JOptionPane.OK_CANCEL_OPTION); if(result==JOptionPane.OK_OPTION)&#123; passwd=passwordField.getText(); return true; &#125; else&#123; return false; &#125; &#125; public void showMessage(String message)&#123; JOptionPane.showMessageDialog(null, message); &#125; final GridBagConstraints gbc = new GridBagConstraints(0,0,1,1,1,1, GridBagConstraints.NORTHWEST, GridBagConstraints.NONE, new Insets(0,0,0,0),0,0); private Container panel; public String[] promptKeyboardInteractive(String destination, String name, String instruction, String[] prompt, boolean[] echo)&#123; panel = new JPanel(); panel.setLayout(new GridBagLayout()); gbc.weightx = 1.0; gbc.gridwidth = GridBagConstraints.REMAINDER; gbc.gridx = 0; panel.add(new JLabel(instruction), gbc); gbc.gridy++; gbc.gridwidth = GridBagConstraints.RELATIVE; JTextField[] texts=new JTextField[prompt.length]; for(int i=0; i&lt;prompt.length; i++)&#123; gbc.fill = GridBagConstraints.NONE; gbc.gridx = 0; gbc.weightx = 1; panel.add(new JLabel(prompt[i]),gbc); gbc.gridx = 1; gbc.fill = GridBagConstraints.HORIZONTAL; gbc.weighty = 1; if(echo[i])&#123; texts[i]=new JTextField(20); &#125; else&#123; texts[i]=new JPasswordField(20); &#125; panel.add(texts[i], gbc); gbc.gridy++; &#125; if(JOptionPane.showConfirmDialog(null, panel, destination+&quot;: &quot;+name, JOptionPane.OK_CANCEL_OPTION, JOptionPane.QUESTION_MESSAGE) ==JOptionPane.OK_OPTION)&#123; String[] response=new String[prompt.length]; for(int i=0; i&lt;prompt.length; i++)&#123; response[i]=texts[i].getText(); &#125; return response; &#125; else&#123; return null; // cancel &#125; &#125; &#125;&#125; Scala 版本已集成到database依赖中]]></content>
      <categories>
        <category>服务器端 - 跳板机</category>
      </categories>
      <tags>
        <tag>跳板机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸡汤每天了解一下]]></title>
    <url>%2F2018%2F12%2F04%2F%E9%B8%A1%E6%B1%A4%E6%AF%8F%E5%A4%A9%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[自我暗示，自己跟自己聊天，时常给自己心里暗示，告诉自己能行，多看些励志的演讲，把自己平时听到的能给自己浑身一震的话记录下来，没事就读一读，我把我积累的一些比较好的话跟题主分享一下吧。 人生的目标不应是祈求风平浪静，而是要造一艘大船，破浪前行。 人生两大悲剧：一是万念俱灰，另一是踌躇满志却只想不做。 食物的价值取决于它的稀缺程度，而不是重要性。——《穷人缺什么》 在过程中打败自己，在结果上打败别人。 我们一生一共能拥有多少次改变自己的机会。 人生如同一场戏，既然都是唱，都要花费同样的力气，还不如选个大舞台，好角色，痛痛快快演一场。 我不想成为社会所规定的样子，父母所期盼的样子，我要成为自己喜欢的样子。 觉得自己做得到和做不到，其实只在一念之间。自己要先看得起自己，别人才会看得起你。 要走窄门，因为引到灭亡，那门是宽的，路是大的，去的人也多。引到永生，那门是窄的，路是小的，找着的人也少。 不被嘲笑的梦想，是不值得被实现的。 最发光的梦想，往往是坚持以后才得到的。 总是有人要赢的，那为什么不能是我呢。——科比 什么样的选择决定了什么样的生活。 伴随着你的成长，总有人会站出来告诉你世界就是这个样子的，你只能去接受它，循规蹈矩地去生活，不要总是企图挑战既定的规则，找一份工作，成家立业，赚一些钱。可是生活要远比你想想得丰富，你只需要牢记一个简单的道理，你周围的世事，所谓的生活都是由一些不比你聪明的人创造的，你可以创造属于你自己事物，你可以去改变这个世界。——乔布斯 轻松的道路往往会越走越艰难，而艰难的道路往往会越走越轻松。 贫居闹市无人问，富在深山有远亲。 人生就像是鸡蛋，如果是外力推着你走，那么你一定会被打碎，如果内力推动着我们，我们会获得新的人生。 我努力奋斗的原因是，我不想把世界让给那些我所鄙视的人。 成功不是实现了梦想，而是捍卫梦想到最后一刻。 能成功的不是出拳最大力的那一个，而是能经受住拳头最多的那一个。 人们不是嘲笑你的梦想，而是嘲笑你的实力。 只有那些疯狂到以为自己能够改变世界的人，才能真正地改变世界。 世界上只有错位的人，而没有无用的人。 能否自律是将富人，穷人和中产阶级区分开来的首要因素。 生活不能等别人来安排，要自己去争取和奋斗。而不论其结果是喜是悲，但可以慰藉的是，你总不枉在这世界上活了一场。——《平凡的世界》 最发光的梦想，往往是坚持以后才得到的。——饶雪漫 我们要在未来的痛苦面前，毫不畏缩，坚持到神志丧失的时刻。——王小波 我或许败北，或许迷失自己，或许哪里也抵达不了，或许我已失去一切，任凭怎么挣扎也只能徒呼奈何，或许我只是徒然掬一把废墟灰烬，唯我一人蒙在鼓里，或许这里没有任何人把赌注下在我身上。无所谓。有一点是明确的：至少我有值得等待有值得寻求的东西。——村上春树 为无为，事无事，味无味，大小多少，报怨以德，图难于其易，为大于其细；天下难事，必作于易；天下大事，必作于细[链接](https://www. zhihu. com/question/21515151/answer/41371992) 转侵删]]></content>
      <categories>
        <category>瞎说</category>
      </categories>
      <tags>
        <tag>鸡汤</tag>
        <tag>每天</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FTP笔记]]></title>
    <url>%2F2018%2F12%2F04%2FFTP%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[FTP作为文件传输的应用，需要保证数据的完整性，即使用TCP可靠传输协议 FTP传输都是通过明文方式，包括用户名和密码；如需传输的安全性，可采用SFTP FTP使用客户服务器方式，大致工作流程如下： 客户端通过TCP三次握手与服务器的21端口建立控制连接 服务器对客户端进行身份验证，授予客户端相应的操作权限 客户端请求数据传输建立数据连接 数据传输完成后断开数据连接 退出FTP服务器断开控制连接 FTP客户端与服务器之间需要建立两个TCP连接，控制连接和数据连接。控制连接，用于传输控制信息，其在整个会话期间保持连接； 数据连接用于传输数据，在客户端发起数据传输请求后建立数据连接，在结束传输后断开连接，不必一直存在。 使用两个连接的好处是使协议更加简单和容易实现，且在传输的过程中，便于利用控制连接中断等操作 java: 参考1,参考21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package ftp;import org.apache.commons.net.ftp.FTPClient;import org.apache.commons.net.ftp.FTPReply;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;/** * Created by on 2016/9/22. */public class FtpUtil &#123; public static boolean uploadFile(String url, int port, String userName, String password, String path, String fileName, InputStream input) &#123; boolean success = false; FTPClient ftpClient = new FTPClient(); int reply; try &#123; ftpClient.connect(url, port); ftpClient.login(userName, password); reply = ftpClient.getReplyCode(); if (!FTPReply.isPositiveCompletion(reply)) &#123; ftpClient.disconnect(); return success; &#125; ftpClient.changeWorkingDirectory(path); ftpClient.setBufferSize(1024); ftpClient.setControlEncoding(&quot;utf-8&quot;); ftpClient.setFileType(FTPClient.BINARY_FILE_TYPE); ftpClient.storeFile(fileName, input); input.close(); ftpClient.logout(); success = true; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (ftpClient.isConnected())&#123; try &#123; ftpClient.disconnect(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return success; &#125; public static void main(String args[])&#123; try &#123; FileInputStream in = new FileInputStream(&quot;D:/FtpUtil.java&quot;); boolean flag = uploadFile(&quot;127.0.01&quot;,21,&quot;root&quot;,&quot;root123&quot;,&quot;D:/&quot;,&quot;FileUtil.java.java&quot;,in); System.out.println(&quot;flag: &quot; + flag); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; scala:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294package webapp.utils.ftpimport org.apache.commons.net.ftp.&#123;FTP, FTPReply, FTPClient&#125;import org.apache.commons.net.PrintCommandListenerimport java.io._import scala.util.&#123;Failure, Success, Try&#125;import scala.Someclass FtpFileSystemAccessor(server: String, port: String, user: String, password: String) extends FtpAccessor &#123; def using(f: AnyRef =&gt; Unit): Unit = &#123; getConnection match &#123; case Some(ftp: FTPClient) =&gt; try &#123; f(ftp) &#125; catch &#123; case e: IOException =&gt; log.error(s&quot;Could not connect to server due to $&#123;e.getMessage&#125;&quot;, e) case e: Throwable =&gt; log.error(e.getMessage, e) &#125; finally &#123; close(ftp) &#125; case _ =&gt; throw new Exception(&quot;Ftp type error&quot;) &#125; &#125; protected def getConnection: Some[AnyRef] = &#123; val ftp = new FTPClient() if (port == &quot;&quot;) &#123; ftp.connect(server) &#125; else &#123; ftp.connect(server, port.toInt) &#125; val reply = ftp.getReplyCode ftp.addProtocolCommandListener(new PrintCommandListener(new PrintWriter(System.out))) //打开调试信息 if (!FTPReply.isPositiveCompletion(reply)) &#123; throw new Exception(&quot;Ftp server refused connection&quot;) &#125; if (!ftp.login(user, password)) &#123; ftp.logout() throw new Exception(&quot;FTP server refused login.&quot;) &#125; Some(ftp) &#125; protected def close(ftp: AnyRef): Unit = &#123; ftp match &#123; case ftp: FTPClient =&gt; if (ftp.isConnected) &#123; Try(ftp.noop()) Try(ftp.logout()) Try(ftp.disconnect()) &#125; case _ =&gt; throw new Exception(&quot;Ftp type error&quot;) &#125; &#125; def makeDir(remote: String): String = &#123; var checkedRemotePath: String = null using &#123; case ftp: FTPClient =&gt; checkedRemotePath = makeDir(ftp, remote) case _ =&gt; &#125; checkedRemotePath &#125; private def makeDir(ftp: FTPClient, remote: String): String = &#123; def remotePathVerified(path: String): String = path.take(1) match &#123; case &quot;.&quot; =&gt; remotePathVerified(path.drop(1)) case &quot;/&quot; =&gt; path.drop(1) case _ =&gt; path &#125; val checkedRemotePath = remotePathVerified(remote) checkedRemotePath.split(&apos;/&apos;).init.foldLeft(&quot;.&quot;)((dir, a) =&gt; &#123; ftp.makeDirectory(dir + &quot;/&quot; + a) dir + &quot;/&quot; + a &#125;) checkedRemotePath &#125; def makeDirectoryRecursive(pathname: String): Boolean = &#123; def directoryHelperFTP(ftpClient: FTPClient, dir: File, res: List[String] = Nil): List[String] = &#123; val path = dir.getPath.replace(&quot;\\&quot;, &quot;/&quot;) Try(ftpClient.getStatus(path)) match &#123; case Success(_) =&gt; res case Failure(_) =&gt; directoryHelperFTP(ftpClient, dir.getParentFile, path :: res) &#125; &#125; var isSuccess:Boolean = false using &#123; case ftp: FTPClient =&gt; isSuccess = directoryHelperFTP(ftp, new File(pathname)).forall(dir =&gt; ftp.makeDirectory(pathname)) case _ =&gt; &#125; isSuccess &#125; override def deleteDirectory(remote: String): Int = &#123; var isDeleteDirectorySuccess: Int = 0 using &#123; case ftp: FTPClient =&gt; ftp.setFileType(FTP.BINARY_FILE_TYPE) val remoteWithoutPoint: String = remote.take(1) match &#123; case &quot;.&quot; =&gt; remote.drop(1) case _ =&gt; remote &#125; isDeleteDirectorySuccess = deleteDirectory(ftp, remoteWithoutPoint) case _ =&gt; &#125; isDeleteDirectorySuccess &#125; private def deleteDirectory(ftp: FTPClient, remoteWithoutPoint: String): Int = &#123; ftp.listFiles(s&quot;./$remoteWithoutPoint&quot;).foreach(f =&gt; &#123; if (f.isDirectory) &#123; val filePath = s&quot;$remoteWithoutPoint/$&#123;f.getName&#125;&quot; deleteDirectory(ftp, filePath) ftp.rmd(s&quot;./$filePath&quot;) ftp.changeWorkingDirectory(s&quot;./$filePath/$&#123;f.getName&#125;&quot;) &#125; else if (f.isFile) &#123; ftp.deleteFile(s&quot;./$remoteWithoutPoint/$&#123;f.getName&#125;&quot;) &#125; &#125;) ftp.rmd(s&quot;./$remoteWithoutPoint&quot;) &#125; override def delete(pathname: String): Unit = &#123; using &#123; case ftp: FTPClient =&gt; ftp.deleteFile(pathname) case _ =&gt; &#125; &#125; def isExists(file: String, filter: String =&gt; Boolean = f =&gt; true): Boolean = &#123; var rtnValue: Boolean = false using &#123; case ftp: FTPClient =&gt; var index = file.lastIndexOf(&apos;/&apos;) if (index &lt; 0) index = file.lastIndexOf(&apos;\\&apos;) val parentPath = file.substring(0, index) val fileName = file.substring(index + 1, file.length) val result = ftp.listNames(parentPath) val files = result.filter(filter) println(result.mkString(&quot;,&quot;)) val fileNames = files.toList.map(x =&gt; x.split(&quot; &quot;).last) rtnValue = fileNames.contains(fileName) case _ =&gt; &#125; rtnValue &#125; override def download(src: String, dst: String, timeout: Int): Unit = &#123; using &#123; case ftp: FTPClient =&gt; ftp.setFileType(FTP.BINARY_FILE_TYPE) ftp.enterLocalPassiveMode() ftp.setBufferSize(1024 * 1024) ftp.setDataTimeout(timeout) //设置数据连接超时,在非正常断链情况下可抛出超时异常,避免长时间阻塞 download(ftp, src, dst) case _ =&gt; &#125; &#125; private def download(ftp: FTPClient, src: String, dst: String): Boolean = &#123; ftp.setFileType(FTP.BINARY_FILE_TYPE) var index = src.lastIndexOf(&apos;/&apos;) if (index == -1) index = src.lastIndexOf(&apos;\\&apos;) val fileName = src.substring(index + 1, src.length) val localFile = new File(dst + &quot;/&quot; + fileName) val localStream = new FileOutputStream(localFile) val isDownloadSuccessful = ftp.retrieveFile(src, localStream) localStream.close() ftp.noop() // check that control connection is working OK ftp.logout() isDownloadSuccessful &#125; /** * 递归遍历和下载指定目录下面指定后缀名的文件 * relativePath 相对路径，用来记录在ftp内当前目录的相对路径。在调用时该参数设置�?&quot;，后面递归调用时会设置为非空值�? * srcDir 需要遍历的目录 * baseDstDir 目标目录，必须是绝对路径 * ext 文件的扩展名 */ override def downloadByExt(srcDir: String, baseDstDir: String, ext: String): Unit = &#123; using &#123; case ftp: FTPClient =&gt; downloadByExt(ftp, &quot;&quot;, srcDir, baseDstDir, ext) case _ =&gt; &#125; &#125; private def downloadByExt(ftp: FTPClient, relativePath: String, srcDir: String, baseDstDir: String, ext: String): Unit = &#123; val array = srcDir.split(&quot;/&quot;).toList array.foreach(x =&gt; if (x != &quot;&quot;) ftp.changeWorkingDirectory(x)) ftp.listFiles.foreach(f =&gt; &#123; if (f.isFile) &#123; if (f.getName.endsWith(ext)) &#123; val dst = baseDstDir + &quot;/&quot; + relativePath val src = f.getName val file = new File(dst + &quot;/&quot; + f.getName) if (!file.exists) &#123; if (!file.getParentFile.exists) file.getParentFile.mkdirs download(ftp, src, dst) &#125; &#125; &#125; else if (f.isDirectory) &#123; val relativeDir = if (relativePath == &quot;&quot;) f.getName else relativePath + &quot;/&quot; + f.getName downloadByExt(ftp, relativeDir, f.getName, baseDstDir, ext) &#125; &#125;) ftp.changeToParentDirectory() &#125; override def upload(localStream: InputStream, remote: String): Boolean = &#123; var result = false using &#123; case ftp: FTPClient =&gt; ftp.setFileType(FTP.BINARY_FILE_TYPE) val remotePath = makeDir(remote) result = ftp.storeFile(s&quot;./$remotePath&quot;, localStream) localStream.close() case _ =&gt; &#125; result &#125; override def upload(local: String, remote: String): Boolean = &#123; val input = new FileInputStream(local) upload(input, remote) &#125; def list(parentPath: String, filter: String =&gt; Boolean = f =&gt; true): Array[String] = &#123; var result: Array[String] = Array.empty using &#123; case ftp: FTPClient =&gt; result = ftp.listFiles(parentPath).map(_.getName).filter(filter) case _ =&gt; &#125; result &#125; def listFiles(parentPath: String): Array[String] = &#123; var result: Array[String] = Array.empty using &#123; case ftp: FTPClient =&gt; val files = ftp.listFiles(parentPath).filter(_.isFile) result = files.map(_.getName) case _ =&gt; &#125; result &#125; def listDirectories(parentPath: String): Array[String] = &#123; var result: Array[String] = Array.empty using &#123; case ftp: FTPClient =&gt; val files = ftp.listDirectories(parentPath) result = files.map(_.getName) case _ =&gt; &#125; result &#125; def rename(src:String, dst:String):Boolean = &#123; var result: Boolean = true using &#123; case ftp: FTPClient =&gt; ftp.rename(src,dst) result = true case _ =&gt; &#125; result &#125;&#125;object FtpFileSystemAccessor &#123; def apply(ftpInfo: FtpServerConfiguration): FtpFileSystemAccessor = new FtpFileSystemAccessor(ftpInfo.ip, ftpInfo.port, ftpInfo.user, ftpInfo.password)&#125;]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>笔记</tag>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow1--morvan笔记]]></title>
    <url>%2F2018%2F12%2F04%2FTensorflow1--morvan%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[tensorflow 简介TensorFlow是Google开发的一款神经网络的Python外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库.TensorFlow 让我们可以先绘制计算结构图, 也可以称是一系列可人机交互的计算操作, 然后把编辑好的Python文件 转换成 更高效的C++, 并在后端进行计算. TensorFlow 无可厚非地能被认定为 神经网络中最好用的库之一. 它擅长的任务就是训练深度神经网络.通过使用TensorFlow我们就可以快速的入门神经网络, 大大降低了深度学习（也就是深度神经网络）的开发成本和开发难度. TensorFlow 的开源性, 让所有人都能使用并且维护, 巩固它. 使它能迅速更新, 提升. tensorflow 安装123sudo apt-get install python-pip python-devpip install tensorflow 第一次执行pip安装时还报错了，重新运行pip安装，成功 拟合曲线机器学习其实是计算机不断尝试模拟已知的，训练的数据，他能自己知道自己拟合的数据离真实的数据差距有多远，然后不断的改进自己拟合的参数，提高拟合的相似度，最终得到的便是拟合曲线，比如其表达式为：y=ax+b 拟合参数如果红色曲线的表达式为：y = a*x + b 其中x代表inputs, y代表outputs, a和b是神经网络训练的参数. 模型训练好了以后,a和b的值将会被确定, 比如 a=0.5, b=2,当我们再输入x=3时, 我们的模型就会输出 0.5*3 + 2 的结果. 模型通过学习数据, 得到能表达数据的参数, 然后对我们另外给的数据所作出预测. tensorflow 基础架构tensorflow 实例:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3# 接着, 我们用 tf.Variable 来创建描述 y 的参数.# 我们可以把 y_data = x_data*0.1 + 0.3# 想象成 y=Weights * x + biases,# 然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3.接着,# 我们用 tf.Variable 来创建描述 y 的参数.# 我们可以把 y_data = x_data*0.1 + 0.3 想象成 y=Weights * x + biases,# 然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3.#---create model---Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biases#------#---calculate loss---loss = tf.reduce_mean(tf.square(y-y_data))#------#---传播误差 ---optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)#------#---train---init = tf.global_variables_initializer() # 替换成这样就好sess = tf.Session()sess.run(init) # Very importantfor step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases))#------#---save model---saver = tf.train.Saver()save_path = saver.save(sess, &quot;my_net/save_net.ckpt&quot;)print(&quot;Save to path: &quot;, save_path)print(&quot;-------------------------------------------------------&quot;)#------#---restore and train---saver.restore(sess, &quot;my_net/save_net.ckpt&quot;)for step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases)) 计算图纸tensorflow 先定义神经网络的结构，然后再把数据放入结构中去运算和training 因为tensorflow是采用数据流图(data flow graphs) 来计算，所以首先我们得创建一个数据流图，然后再将我们的数据(数据以张量(tensor)的形式存在)放在数据流图中计算，节点(Nodes)在图中表示数学操作，图中的线(edges)则表示在节点间相互联系的多维数据数组，即张量，训练模型时tensor会不断从数据流图中的一个节点flow到另一个节点，这也是tensorflow的由来 Tensor(张量)张量有很多种： 零阶张量为纯量或标量(scalar)也就是一个数值，如1 一阶张量为向量(vector),[1,2,3] 二阶张量为矩阵(matrix),[[1,2,3],[3,4,5],[4,5,6]] 三阶 SessionSession 是 Tensorflow 为了控制,和输出文件的执行的语句. 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分. ps:让我联想到了spark的两种算子，而session类似于sc，而run是acton算子 123456789101112131415161718192021import tensorflow as tf# create two matrixesmatrix1 = tf.constant([[3,3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1,matrix2)# method 1sess = tf.Session()result = sess.run(product)print(result)sess.close()# [[12]]# method 2with tf.Session() as sess: result2 = sess.run(product) print(result2)# [[12]] Variable 变量在 Tensorflow 中，定义了某字符串是变量，它才是变量，这一点是与 Python 所不同的。 定义语法： state = tf.Variable() 特别像新建了一个变量对象 特别注意的是新建了变量一定要init: 一定要定义 init = tf.initialize_all_variables() placeholder这一次我们会讲到 Tensorflow 中的 placeholder , placeholder 是 Tensorflow 中的占位符，暂时储存变量. Tensorflow 如果想要从外部传入data, 那就需要用到 tf.placeholder(), 然后以这种形式传输数据 sess.run(**, feed_dict={input: *}). 1234567891011import tensorflow as tf#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)# mul = multiply 是将input1和input2 做乘法运算，并输出为 outputouput = tf.multiply(input1, input2)#接下来, 传值的工作交给了 sess.run() , 需要传入的值放在了feed_dict=&#123;&#125; 并一一对应每一个 input. placeholder 与 feed_dict=&#123;&#125; 是绑定在一起出现的with tf.Session() as sess: print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;)) 激励函数激励函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经系统。激励函数的实质是非线性方程。 Tensorflow 的神经网络 里面处理较为复杂的问题时都会需要运用激励函数 activation function tensorflow 实例 升级+matplot绘图 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&quot;&quot;&quot;Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.&quot;&quot;&quot;from __future__ import print_functionimport tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltdef add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# Make up some real datax_data = np.linspace(-1,1,300)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape)y_data = np.square(x_data) - 0.5 + noise# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1])# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediciton and real dataloss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split(&apos;.&apos;)[1]) &lt; 12 and int((tf.__version__).split(&apos;.&apos;)[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)# plot the real datafig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data, y_data)plt.ion()plt.show()for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to visualize the result and improvement try: ax.lines.remove(lines[0]) except Exception: pass prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;) # plot the prediction lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw=5)plt.pause(0.1)]]></content>
      <categories>
        <category>机器学习、深度学习 - tensorflow</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UsingZookeeper]]></title>
    <url>%2F2018%2F12%2F04%2FUsingZookeeper%2F</url>
    <content type="text"><![CDATA[welcomeApache ZooKeeper is an effort to develop and maintain an open-source server which enables highly reliable distributed coordination.(zookeeper 致力于开发和维护高可靠性的分布式协调服务) ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.All of these kinds of services are used in some form or another by distributed applications.(作为分布式应用的一部分被使用)Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable.(解决分布式应用中的不可避免的问题：异常失败（部分失败）和竞态条件)Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage.Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed. source code readingidea zookeeper ant 第一步 123git clone git@github.com:apache/zookeeper.gitcd zookeeperant eclipse 第二步 idea 中打开 zookeeper project file -&gt; new -&gt; project from existing source -&gt; 选中目录 -&gt; 选择eclipse -&gt; ok 参考 进度： 读了一部分，作为源码注释在project中了zookeeper-notes,从zkServer.sh进入 开发环境搭建：参考:Hadoop安装与伪分布式部署 安装 zookeeper 提供的API: create group , join group, delete group 都比较简单，我跟着文档码了一遍例子 zookeeper 服务 zookeeper 是一个高可用的调度服务，(在用zookeeper 的过程中，发现看不懂日志中的一些名词，相信这部分会介绍这些概念) data model(数据模型) summary znode: 存储的data、ACL(Access Control List) 数据的访问是原子性的 可以通过path 定位znode,不支持相对路径 Ephemeral Znodes 我们已经知道，znode有两种类型：ephemeral和persistent。在创建znode时，我们指定znode的类型，并且在之后不会再被修改。 当创建znode的客户端的session结束后，ephemeral类型的znode将被删除。persistent类型的znode在创建以后，就与客户端没什么联系了，除非主动去删除它，否则他会一直存在。 Ephemeral znode没有任何子节点。虽然Ephemeral znode绑定了客户端session，但是对任何其他客户端都是可见的，当然是在他们的ACL策略下允许访问的情况下。 当我们在创建分布式系统时，需要知道分布式资源是否可用。Ephemeral znode就是为这种场景应运而生的。正如我们之前讲述的例子中，使用Ephemeral znode来实现一个成员关系管理，任何一个客户端进程任何时候都可以知道其他成员是否可用。Znode的序号 如果在创建znode时，我们使用排序标志的话，ZooKeeper会在我们指定的znode名字后面增加一个数字。我们继续加入相同名字的znode时，这个数字会不断增加。 这个序号的计数器是由这些排序znode的父节点来维护的。如果我们请求创建一个znode，指定命名为/a/b-，那么ZooKeeper会为我们创建一个名字为/a/b-3的znode。 我们再请求创建一个名字为/a/b-的znode，ZooKeeper会为我们创建一个名字/a/b-5的znode。ZooKeeper给我们指定的序号是不断增长的。 Java API中的create()的返回结果就是znode的实际名字。那么序号用来干什么呢？当然是用来排序用的！后面《A Lock Service》中我们将讲述如何使用znode的序号来构建一个share lock。观察者模式 watches 观察模式可以使客户端在某一个znode发生变化时得到通知。观察模式有ZooKeeper服务的某些操作启动，并由其他的一些操作来触发。例如，一个客户端对一个znode进行了exists操作，来判断目标znode是否存在，同时在znode上开启了观察模式。 如果znode不存在，这exists将返回false。如果稍后，另外一个客户端创建了这个znode，观察模式将被触发，将znode的创建事件通知之前开启观察模式的客户端。 我们将在以后详细介绍其他的操作和触发。观察模式只能被触发一次。如果要一直获得znode的创建和删除的通知，那么就需要不断的在znode上开启观察模式。 在上面的例子中，如果客户端还继续需要获得znode被删除的通知，那么在获得创建通知后，客户端还需要继续对这个znode进行exists操作，再开启一次观察模式。在《A Configuration Service》中，有一个例子将讲述如何使用观察模式在集群中更新配置。operations(操作) base: create delete exits getACL setACL getChildren getData setData sync(Synchronizes a client’s view of a znode with ZooKeeper) 调用delete和setData操作时，我们必须指定一个znode版本号（version number） muti update 批量更新Implementation(实现)Consistency(数据一致性)Sessions(会话) READEDIT 实战：配置服务：使用zookeeper实现一个配置服务，数据分析师可以通过zookeeper共享一个通用的配置服务。从表面上看，zookeeper 可以理解为一个配置数据的高可用服务，可以为应用提供检索和更新配置数据服务。我们可以使用zk的watcher模式实现一个活动的配置服务，当配置数据发生变化时，可以通知与配置相关的客户端 设计： 模型：当master跟新数据，其他的worker也随之将数据更新，就像hdfs的namenode 使用znode存储一个string类型的数据作为value,用znode的path来表示key 实现一个client,这个client可以在任何时候对数据进行更新操作 class ActiveKeyValueStore: 1234567891011121314151617181920212223242526272829303132package usingzookeeper.configurationservice;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs;import org.apache.zookeeper.data.Stat;import usingzookeeper.ConnectionWatcher;import java.nio.charset.Charset;/** * Created by xxh on 17-12-13. */public class ActiveKeyValueStore extends ConnectionWatcher &#123; private static final Charset CHARSET = Charset.forName(&quot;UTF-8&quot;); public void write(String path,String value) throws InterruptedException,KeeperException&#123; Stat stat = zk.exists(path,false); if (stat == null)&#123; zk.create(path,value.getBytes(CHARSET), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125;else &#123; zk.setData(path,value.getBytes(CHARSET),-1); &#125; &#125; public String read(String path, Watcher watcher) throws KeeperException, InterruptedException &#123; byte[] data = zk.getData(path,watcher,null); return new String(data,CHARSET); &#125;&#125; class ConfigUpdater 123456789101112131415161718192021222324252627282930313233343536package usingzookeeper.configurationservice;import org.apache.zookeeper.KeeperException;import java.io.IOException;import java.util.Random;import java.util.concurrent.TimeUnit;/** * Created by xxh on 17-12-13. */public class ConfigUpdater &#123; public static final String PATH = &quot;/config&quot;; private ActiveKeyValueStore store; private Random random = new Random(); public ConfigUpdater(String hosts) throws IOException, InterruptedException &#123; store = new ActiveKeyValueStore(); store.connect(hosts); &#125; public void run() throws KeeperException, InterruptedException &#123; while (true)&#123; String value = random.nextInt(100) + &quot;&quot;; store.write(PATH,value); System.out.printf(&quot;Set %s to %s\n&quot;,PATH,value); TimeUnit.SECONDS.sleep(random.nextInt(10)); &#125; &#125; public static void main(String[] args) throws Exception&#123; ConfigUpdater configUpdater = new ConfigUpdater(&quot;localhost&quot;); configUpdater.run(); &#125;&#125; class ConfigUpdater: 123456789101112131415161718192021222324252627282930313233343536package usingzookeeper.configurationservice;import org.apache.zookeeper.KeeperException;import java.io.IOException;import java.util.Random;import java.util.concurrent.TimeUnit;/** * Created by xxh on 17-12-13. */public class ConfigUpdater &#123; public static final String PATH = &quot;/config&quot;; private ActiveKeyValueStore store; private Random random = new Random(); public ConfigUpdater(String hosts) throws IOException, InterruptedException &#123; store = new ActiveKeyValueStore(); store.connect(hosts); &#125; public void run() throws KeeperException, InterruptedException &#123; while (true)&#123; String value = random.nextInt(100) + &quot;&quot;; store.write(PATH,value); System.out.printf(&quot;Set %s to %s\n&quot;,PATH,value); TimeUnit.SECONDS.sleep(random.nextInt(10)); &#125; &#125; public static void main(String[] args) throws Exception&#123; ConfigUpdater configUpdater = new ConfigUpdater(&quot;localhost&quot;); configUpdater.run(); &#125;&#125; 组成员管理服务：]]></content>
      <categories>
        <category>数据存储处理</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[团队协作指南(转载)]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%9B%A2%E9%98%9F%E5%8D%8F%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[工作或者是生活，自己或者是团队实事求是的面对自我，面对现实，挑战、机遇、自己心中，团队中多种不同的想法，拿到台面上客观的思考讨论，跟现实，跟自己，跟团队达成一个共识，确立好一个大概的方向，大家向着这个方向一起努力，以求尽快达成目标，我觉得这是至关重要的 这里有个人分享的自己的创业经历，这边记录下，转侵删 背景 5月份，自己从某巨头互联网大厂离职，之后和伙伴一起尝试通过远程协作创业，到今天也快满2个月了。因为第一次正式尝试纯远程的工作方式，一开始的工作节奏非常混乱，幸好，通过不断的踩坑和逐步的调整，一切都开始慢慢趋于稳定。这里把我们曾经遇到的困难和解决的方法在这里进行分享，这样即使我们最后因为各方面原因，导致没有达成最理想的结果，也算给其他同学留下一些经验参考吧。 关于团队 先介绍下我们吧。目前我们核心成员只有我和另外一个大学同学两个人。15年那会，我们都还在学校，而又正值互联网融资热，当时就商量着要不要一起直接就成立工作室创业。但是最后考虑各方面经验积累和视野可能不够，最后约定， 大家先工作两三年，如果到时候还有创业的欲望，到时候再一起出来做。之后他去了某 知名游戏发行公司做游戏策划，而我去了某巨头互联网公司做开发。 16年底的时候，他选择先出来了。一个人从 0 到1，拉起了一个小团队做独立游戏。虽然这两年独立游戏概念很热，但是却掩盖不了 市场环境残酷的现实。初次创业，经验的稚嫩，资金的缺乏，加上市场前景的暗淡，导致团队成员的信心越来越弱，最后其他团队成员包括投资人一个个都选择了离开。那个 游戏 也成为了他一个人的坚持。所幸，一直一个人坚持到今天，上了QQ空间的小游戏平台，得到了首页推荐，也慢慢开始有了一些正向的回报，希望努力不会被辜负吧。 而我，因为很早就有自己创业的欲望。所以为了积攒经验，从大二开始，就开始进入创业团队做开发工作。前前后后经历的团队的创世人有 上市公司高管，有大厂多年工作经验出来创业的资深员工，有 大学老师兼职出来创业，也有在学校就慢慢拉起团队进行创业的师兄。经历的业务从社交、电商，h5营销工具再到O2O服务。虽然创业人的背景和从事的业务不同，管理方式和风格各异，但是大家都同样过的艰难，只能靠着资本一轮又一轮的输血来维持生存。当毕业的时候，面临第一份正式工作的抉择的时候，出于对创业这件事情的未来的迷茫以及对大厂的敬畏，最后还是选择去了某巨头公司。转眼间，又过了几年，看着好友选择在外面直面风雨，困难的精彩着。而自己，在大平台的包裹下，似乎开始慢慢忘记了什么是真实的市场，也明显感觉到，自己似乎正在变得越来越不适合创业这件事情了。所以，虽然那里有着可爱又聪明的同事，不错的待遇和未来可期的上升空间，在纠结了大半年之后，还是在3年这个时间节点选择了离开，去直面真实的市场和风雨。 关于方向 对于两个都有着多年的创业欲望的人来说，可能在过去的数年里在脑海里面都有构思过N多个创业方向。当大家选择正式一起做的时候，这时候就面临到了第一个矛盾，到底从哪个方向进行切入。这时候，我们选择了对整件事情进行梳理，得到了下面几点共识： 1、首先，大家有自己的想法这件事情本身是我们认可和鼓励的，正是因为大家都有属于自己的创业想法，同时又有行动力，才走到了一起。 2、假设后续需要吸纳新成员，我们也是 偏向于有自己个人想法的同事，当之后一个群体里面的人都有个人的想法，如果没有一个好的机制，让大家能够达成一致的意见，各自发挥自己的力量，团队最终会一盘散沙。 3、没有谁能确保自己一定是对的，也没有谁能够确保自己的眼界一定是全面的，每个人的想法或者点子其实都非常重要。 4、所有的创业方向，都会面临残酷的市场这个共同的挑战，不管选择往哪个方向走，目标都是为了让团队能够最大化的发挥自己的价值，同时可以让大家过的更好。 5、任何一个点子，进入到执行过程的时候，一定会遇到各种困难，整个过程一定要有充分的沟通。 基于上面几点共识，最后我们选择确定第一个方向的方式是，一起商量好一些要点，花一周的时间去各自将自己的想法形成文档，然后再互相评测，PK。看最后哪个方向 最适合当前的我们去做。就这样，我们筛选出了第一个尝试的方向。 关于协作 远程合作这件事情，对于双方，其实都是第一次尝试。之所以选择这种方式，是因为我们非常确认，如果大家都能够足够自觉和理想化的投入到工作当中。不拘泥地点的办公方式，是最高效的。同时由于 我们人少，大家又都是 对事情有足够的热爱的，是可以去做这个尝试的。 当我们具体开始工作的时候，发现事情好像并没有那么简单。最开始的时候，我们选择不拘泥任何形式的方式，大家商量好最近一两周的目标，然后各自去做就好了。当突然 从一个时间，地点受到严格限制的工作环境，进入到一个完全自由的工作环境，整个生活节奏变得混乱不堪。在刚开始的那几周，我整个人都是白天黑夜颠倒的，晚上的工作状态非常好，经常工作到四五点，然后经历一晚上的亢奋状态，白天又昏昏欲睡，只能补觉，最后形成了一个非常不好的恶性循环。当大家的工作生活节奏没有保持一致的时候，实时的沟通都成为了一件奢侈的事情。 那几周混乱的远程工作尝试，非常的不成功。一方面，个人的生物钟紊乱了，同时，大家没有进行有效的沟通，猜疑和不信任也随之产生。最后，我们特意找了一个周末的时间，去盘点整件事情。最后，达成了新的一套方案： 1、大家还是需要保障在固定时间段的工作时长，我们目前的固定工作时间计划是一周6天，早上9-12，下午14-17。其他的时间自由调配。 2、为了确保大家不在早上睡懒觉，8-9点需要通过微信小程序打卡，不管任何理由，缺少一次，扣除500 3、因为是远程，为了让大家充分理解各自的进度，减少猜疑。每天早上9点，开语音晨会，对前一天的进度进行盘点，同时各自定下当天的任务。同时，每天结束工作之前，需要填写当天的工作日报。 4、所有会议文档，工作纪要，工作文档都通过google在线文档进行。所有的沟通过程都通过在线文档实时的记录，同时双方实时确认，以最大程度的避免沟通的信息丢失。 5、需要长时间讨论的内容，尽量放到晚上，不占用 正常的固定工作时间 事实证明，做了这次调整之后，整件事情的推进开始变得稳定有序，大家的生物钟开始稳定，对事情的执行进度以及各自遇到的困难也各自有了比较清晰的了解。 关于未来 作为草根创业，资源少，相对也包袱轻。我们可以大胆的去尝试我们认为好的事情，遇到障碍也可以比较轻便的的进行调整。经过几个月的需求分析、竞品分析、立项计划，如今已经确定了比较满意的方向，现阶段，我们最重要的目标是，将我们的立项产品化，并且赖以生存。后续也会不定期的将我们做过的的尝试和遇到的困难通过文章分享出来，方法总比困难多，共勉。]]></content>
      <categories>
        <category>管理</category>
      </categories>
      <tags>
        <tag>团队</tag>
        <tag>管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[掌握post请求]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%8E%8C%E6%8F%A1post%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[应用rest层使用的是spray库，之前使用GET REQUEST比较多，现在用到POST REQUEST了，现在记录总结下 HTTP/1.1 协议规定的 HTTP 请求方法有 OPTIONS、GET、HEAD、POST、PUT、DELETE、TRACE、CONNECT 这几种。其中 POST 一般用来向服务端提交数据，本文主要讨论 POST 提交数据的几种方式。我们知道，HTTP 协议是以 ASCII 码传输，建立在 TCP/IP 协议之上的应用层规范。规范把 HTTP 请求分为三个部分：状态行、请求头、消息主体。类似于下面这样： 123&lt;method&gt; &lt;request-URL&gt; &lt;version&gt;&lt;headers&gt;&lt;entity-body&gt; 协议规定 POST 提交的数据必须放在消息主体（entity-body）中，但协议并没有规定数据必须使用什么编码方式。实际上，开发者完全可以自己决定消息主体的格式，只要最后发送的 HTTP 请求满足上面的格式就可以。但是，数据发送出去，还要服务端解析成功才有意义。一般服务端语言如 php、python 等，以及它们的 framework，都内置了自动解析常见数据格式的功能。服务端通常是根据请求头（headers）中的 Content-Type 字段来获知请求中的消息主体是用何种方式编码，再对主体进行解析。所以说到 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分。下面就正式开始介绍它们。POST 提交数据方式有四种： application/x-www-form-urlencoded multipart/form-data application/json text/xml spray 如何 handle 这些 request 1. application/x-www-form-urlencoded 这应该是最常见的 POST 提交数据的方式了。浏览器的原生 表单，如果不设置 enctype 属性，那么最终就会以 application/x-www-form-urlencoded 方式提交数据。请求类似于下面这样（无关的请求头在本文中都省略掉了）： 123POST http://www.example.com HTTP/1.1Content-Type: application/x-www-form-urlencoded;charset=utf-8title=test&amp;sub%5B%5D=1&amp;sub%5B%5D=2&amp;sub%5B%5D=3 首先，Content-Type 被指定为 application/x-www-form-urlencoded；其次，提交的数据按照 key1=val1&amp;key2=val2 的方式进行编码，key 和 val 都进行了 URL 转码。大部分服务端语言都对这种方式有很好的支持。例如 PHP 中，$_POST[‘title’] 可以获取到 title 的值，$_POST[‘sub’] 可以得到 sub 数组。很多时候，我们用 Ajax 提交数据时，也是使用这种方式。例如 JQuery 和 QWrap 的 Ajax，Content-Type 默认值都是「application/x-www-form-urlencoded;charset=utf-8」。 spray handle 12345678910111213141516171819202122232425path(&quot;uploadMeasurementItems&quot;) &#123; post &#123; detach() &#123; entity(as[FormData]) &#123; measurementItems =&gt; &#123; complete(s&quot;&quot;&quot;--token: $&#123;measurementItems.fields(0) _2&#125;&quot;&quot;&quot;) /*if (measurementItems.token == AuthorityConfig.authorityToken)&#123; new MeasurementItemsService(measurementItems).upload2Redshift match &#123; case true =&gt; complete(200,&quot;ok,upload measurementItems success&quot;) case false =&gt; complete(500,&quot;inner server error&quot;) &#125; //Todo: 解析数据，粗略保证数据正确性，如检测data不为空，本地暂存，定时批量解析数据并连接redshift入库 //complete(&quot;measurementItems type: &quot; + measurementItems.flag + &quot;measurementItems data: &quot; + measurementItems.measurementItem) &#125; else &#123; complete(401,&quot;token error,access denied&quot;) &#125;*/ &#125; &#125; &#125; &#125; &#125; 重点在与 FormData class源码： 1234567891011121314151617181920package spray.httpimport spray.http.HttpHeaders._sealed trait HttpForm &#123; type FieldType def fields: Seq[FieldType]&#125;/** * Model for `application/x-www-form-urlencoded` form data. */case class FormData(fields: Seq[(String, String)]) extends HttpForm &#123; // TODO: better FormData(query: Uri.Query)? type FieldType = (String, String)&#125;object FormData &#123; val Empty = FormData(Seq.empty) def apply(fields: Map[String, String]): FormData = this(fields.toSeq)&#125; 根据注释也可以看到 case class FormData Model for application/x-www-form-urlencoded form data于是 entity-body 中的多个 key-value 转换成有一个名为 fields 的Seq(String,String)类型字段,那么我们就可以通过处理field字段来获得我们想要的参数 2. multipart/form-data在同一个源代码文件中还定义了multipart/form-data的handle case class 123456789101112131415161718/** * Model for `multipart/form-data` content as defined in RFC 2388. * All parts must contain a Content-Disposition header with a type form-data * and a name parameter that is unique */case class MultipartFormData(fields: Seq[BodyPart]) extends HttpForm &#123; // TODO: `fields: BodyPart*` is probably better type FieldType = BodyPart def get(partName: String): Option[BodyPart] = fields.find(_.name.exists(_ == partName))&#125;object MultipartFormData &#123; val Empty = MultipartFormData(Seq.empty) def apply(fields: Map[String, BodyPart]): MultipartFormData = this&#123; fields.map &#123; case (key, value) ⇒ value.copy(headers = `Content-Disposition`(&quot;form-data&quot;, Map(&quot;name&quot; -&gt; key)) +: value.headers) &#125;(collection.breakOut) &#125;&#125; 根据源码我们也可以看到，我们的entity-body 被转换成了fields: Seq[BodyPart]，还定义了 get 方法 这种方式一般用来上传文件，各大服务端语言对它也有着良好的支持。上面提到的这两种 POST 数据的方式，都是浏览器原生支持的，而且现阶段标准中原生 表单也只支持这两种方式（通过 元素的 enctype 属性指定，默认为 application/x-www-form-urlencoded。其实 enctype 还支持 text/plain，不过用得非常少）。随着越来越多的 Web 站点，尤其是 WebApp，全部使用 Ajax 进行数据交互之后，我们完全可以定义新的数据提交方式，给开发带来更多便利。 3. application/json application/json 这个 Content-Type 作为响应头大家肯定不陌生。实际上，现在越来越多的人把它作为请求头，用来告诉服务端消息主体是序列化后的 JSON 字符串。由于 JSON 规范的流行，除了低版本 IE 之外的各大浏览器都原生支持 JSON.stringify，服务端语言也都有处理 JSON 的函数，使用 JSON 不会遇上什么麻烦。JSON 格式支持比键值对复杂得多的结构化数据，这一点也很有用。记得我几年前做一个项目时，需要提交的数据层次非常深，我就是把数据 JSON 序列化之后来提交的。不过当时我是把 JSON 字符串作为 val，仍然放在键值对里，以 x-www-form-urlencoded 方式提交。 之前用AJAX都是使用json来交换信息 这种方案，可以方便的提交复杂的结构化数据，特别适合 RESTful 的接口。各大抓包工具如 Chrome 自带的开发者工具、Firebug、Fiddler，都会以树形结构展示 JSON 数据，非常友好。 12345678910111213141516171819202122232425path(&quot;uploadMeasurementItems&quot;/&quot;another&quot;) &#123; post &#123; detach() &#123; entity(as[MeasurementItems]) &#123; measurementItems =&gt; &#123; //complete(s&quot;&quot;&quot;--token: $&#123;measurementItems.fields(0)&#125;&quot;&quot;&quot;) if (measurementItems.token == AuthorityConfig.authorityToken)&#123; new MeasurementItemsService(measurementItems).upload2Redshift match &#123; case true =&gt; complete(200,&quot;ok,upload measurementItems success&quot;) case false =&gt; complete(500,&quot;inner server error&quot;) &#125; //Todo: 解析数据，粗略保证数据正确性，如检测data不为空，本地暂存，定时批量解析数据并连接redshift入库 //complete(&quot;measurementItems type: &quot; + measurementItems.flag + &quot;measurementItems data: &quot; + measurementItems.measurementItem) &#125; else &#123; complete(401,&quot;token error,access denied&quot;) &#125; &#125; &#125; &#125; &#125; &#125; 当然这种方式要定义自定义 case class 的 unmashaller 1234567object MeasurementItemsJsonImplicits extends DefaultJsonProtocol &#123; implicit val CustomerInfoJson = jsonFormat9(CustomerInfo) implicit val MeasurementItemDetailJson = jsonFormat2(MeasurementItemDetail) implicit val MeasurementItemInfoJson = jsonFormat2(MeasurementItemInfo) implicit val AnalysisReportJson = jsonFormat6(AnalysisReport) implicit val MeasurementItemsJson = jsonFormat9(MeasurementItems)&#125; ４.text/xml暂时没用过 参考：四种常见的 POST 提交数据方式]]></content>
      <categories>
        <category>服务器端 - web开发</category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>HttpPost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说的好]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%AF%B4%E7%9A%84%E5%A5%BD%2F</url>
    <content type="text"><![CDATA[说的好初级程序员升级指南转侵删作者：暗灭链接：https://www.zhihu.com/question/33578621/answer/451931102来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 先不说高级。就只说初级程序员经常容易犯的错误，把这些错误改正了，你离中级就不远了。初级程序员经常犯的错误集锦 命名不规范 日志不规范 拒绝写接口和假数据 不写单元测试 盲目集成 逻辑不清 不做方案 不关注性能 害怕重构 做出来就好，不考虑优雅的方案 不考虑未来需求的变化 遇到问题的时候不会试错 不会写伪代码 不做数据量的预估 提交代码不规范 不喜欢打Tag 不遵守发布流程 不知道Bug修复的优先级 总喜欢手动修改线上代码 不做数据备份 不做自测 不尽力模仿真实数据，测试数据很随意 不抽取公共代码 不认真听需求讲解 不看验收标准 不主动推进项目进度 遇到难题不主动反馈 一 命名不规范 命名很随意，当时写代码特别High，什么奇奇怪怪的命名都有的：xiaonaigou,xxxx,j1,jl,llst. 完全意识不到命名规范的价值和意义。 二 日志不规范日志？那是什么鬼东西，能吃么？ 曾经有一个从文思海辉出来的小伙伴，三年后端工程师经验，出了问题不知道怎么解决。 只好重启。 找我来协助，问他， 怎么错了？ 不知道。 日志呢？ 没有。 晕，那怎么解决问题，神仙也搞不定啊。 后来才知道，他们解决问题都是本地改代码然后直接部署，重新访问看错误消失没，没有消失就继续在本地改源码。三 拒绝写接口和假数据一个菜鸡不可怕，可怕的是菜鸡遇到菜鸡。曾经有一个项目中的两个菜鸡，一个前端一个后端，他们很欢快的调接口，根本不写文档 ，两个人效率特别高。 直到有一天，发现项目可能做不完了，需要另外两个前端菜鸡协助一下。 新来的两个菜鸡要获取后端的数据，不知道接口的Url地址，不知道Get还是Post，不知道发送的参数和返回值。就这样写！ 我压根没想到可以这么写代码，两个菜鸡很开心！拍手称快：通了，通了，通了！ 我说你们通什么呢？他们说接口终于通了！原来他们两个参考之间的页面，硬生生的一次一次不停的尝试，就这样把接口猜出来了！ 这就是编程的乐趣吗？还有不写假数据。曾经有一个马姓小哥，对赵姓小哥信誓旦旦的说：3天，给我3天时间 ，我把真数据给你。 于是赵姓小哥信以为真。就这样，3天又3天，3天又3天，3天又3天，3天又3天，3天又3天。 整整一个半月，赵姓小哥都没有拿到全部的数据！ 四 不写单元测试 确切来说，是不按TDD的方式开发。在现在IDE这么强大的情况下，先写单元测试的习惯，不仅仅是代码的严谨性，也是效率的代名词啊。 可是很多菜鸡理解不了单元测试的价值，没关系，等到代码重构，需求变更的时候，就哭都哭不出来了！ 好的单元测试，你的逻辑必然会清楚。 五 先集成，再测试，再放弃。很多时候，菜鸡在引入第三方的库，框架，接口或者是服务的时候，最喜欢的事情就是直接和自己原有的代码集成在一起。结果 是什么呢？突然间不能用了，跑不起来了，不知道问题出在哪了，根本分不清倒底是第三方的问题还是自己的问题。好的方法是什么？先跑通官方提供的Demo，再想办法一点一点加上自己的业务。 六 理不清楚逻辑，边做边猜前端在这里的问题特别多，做支付，不清楚支付的流程，分不清楚定义，总以为前端就是接口处理好数据展示好拉倒。很多菜鸡都会有这种习惯，这样不好，先把逻辑处理好，弄清楚流程，再去动手才好。 七 不做方案不做方案代表什么含义呢？就是完全凭直觉行走啊。跟闭上眼逛窑子一样。写代码的好习惯应该是先在脑袋里把所有的需求细节过一遍，实现细节拿出来。上个月就有一个张姓小菜鸡，做一个匿名评论的功能。基本上没有什么经验，脑子也不好使，给出的方式是什么你们猜得到么？用户刷新一次就往用户表里插入一条数据，密码默认昵称随机。不多说了都是泪，我见过太多让人目瞪狗呆的方案了，看着满屏的代码，你怎么帮他调错调优，最好的方式就是全部重写。做方案的好处太多了。 八 不关注性能不关注性能也是新人很容易犯的错。什么是性能呢。对后端来说就是TPS和响应时间，对前端来说就是响应时间。很多新人程序员的习惯就是把东西做出来，然后再优化。最后就是东西做出来了，优化留给别人了。对性能的关注也是晋升中级程序员最关键的技能点。在写代码的时候，有经验的工程师已经知道了这个方法这个函数这个功能点的性能怎么样，瓶颈在哪里。 九 害怕重构“程序员最大的勇气就是看自己三个月之前写的代码。”其实重构并不应该是在几个月之后重构，最好的方式是实时重构。写一天代码，70%的时间都放到重构上都不过份。而新人呢，磕磕跘跘的完成一个功能，就跟多米诺骨牌做成的大黄蜂一样，你敢动一下他的代码试试？他会跟你拼命。你让他自己动一行代码试试？不重构在某种程度上也意味着你的代码实现无法重塑。 十 做出来就好，不考虑优雅的方案有个词叫做最佳实践，其实编码规范和最佳实践，是编程功底的重要体现。优雅方案可以认为是最佳实践的升级版，它和上面说到的不断的重构是相辅相成的。不好的方案是什么呢？硬编码居多，没有可扩展性，用很丑陋的方式完成了功能。上次他们去做了一个关于试听课的方案，一个人能试听多少节课，正常的逻辑应该是在用户的表里加一个字段来表示。需求是写着邀请几个人，可以试听多少节课，所以他们判断试听多少节课就直接在通过邀请人的表里查询去做。完全没考虑到以后如果我变换了试听课的判断条件怎么办？实际上这是应该拆解成两部分，一个是试听课的产生条件，这是一个独立的模块，加一个是试听课的确认。像这种例子太多了，也和不做方案，不考虑扩展性有关系。就是接下来要说的。 十一 不考虑未来需求的变化工程师的水准，其实可以分成以下几个阶段（马丹我找不到之前在哪个答案里写过了）： 面向功能编程 面向性能编程 面向未来编程工程师拿到需求的第一件事，应该聚集在以下几个问题：第一 哪些需求是我之前完成过的第二 哪些需求是有可能变化的第三 有几种方案，分别支持什么样的需求变化但是差一点的程序员就考虑不到那么远，一个是对业务不熟悉，判断不出来哪些需求可能会产生变化，一个是对可选的方案掌握的不多，根本就没有什么可选的余地，还有就是没有这种思维习惯，分不清楚哪些是现在要完成的，哪些是未来可能会支持或者是变动的。十二 遇到问题的时候不会试错这也是新手常见的问题。很多时候新人会遇到问题，解决不了，去找一个有经验的工程师，这个有经验的工程师呢，大概也未曾遇到这种情况，但是他解决问题的思路清楚啊。一会儿试试这个，一会儿删删那段代码，很快就跑通了。解决问题是一个很见功底的技术点，而且是有很多方法论的，之前总结过一些，简单列举过来： 寻找正确的代码 理清楚正确的执行顺序 重现错误 最小化错误产生的场景 修改代码到一个已知的错误类型等等等。解决问题就是一个分析推理的过程，而在这里呢，背后的功底就是你知道很多哪些是肯定不会错的小公理，然后再挨个去定位可能产生错误的环节，分解流程是最基础的工作。十三 不会写伪代码 伪代码是什么呢？就是自然语言啊。其实编程只有 三种逻辑控制块，顺序，循环，判断所以你只要用自然语言来描述出来，先做什么，再做什么，什么时候循环，什么时候判断，代码写出来的问题就不大。这是一个先写伪代码再写细节的过程。你不要上来就开始平铺写代码（我之前讲过优雅代码之道，有兴趣的可以加群听一下，重点讲了怎么写出来优雅代码）。平铺代码是最菜的方式，好的代码是有结构的，有不同的抽像层级。第一步，干嘛。第二步，干嘛。第三步，干嘛。先把这个列清楚，这是伪代码的第一级。然后变成注释，这是第二级。删掉注释变成函数名，这是第三级。所以说，好的程序员写代码是不需要注释的，不是说让你把注释删掉，而是让你完成这三步升华的过程。写的好的代码，命名规范，你看到的真的是一首诗， 是一种编程语言，是在用语言来描述一件功能的完成，这种编程艺术的工业感很爽快，你看那些不爽的代码，简直了。。]]></content>
      <categories>
        <category>瞎说</category>
      </categories>
      <tags>
        <tag>摘抄</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发测试集群环境搭建]]></title>
    <url>%2F2018%2F12%2F04%2F%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[通过虚拟机搭建集群环境 虚拟机环境安装配置 下载安装VMare，官网下载vmare安装文件安装，本次在ubuntu 上安装使用，跟windows并无太大区别，秘钥靠网友（cnblogs上有） 下载centos的iso镜像文件并安装,安装ubuntu系统一直失败报错，我怀疑还是我的办公电脑的显卡比较奇葩造成的，所以直接下载centos的minimal版本安装一切正常 配置一台matrix并clone,安装好一台后，安装java,clone 修改hostname,/etc/hosts文件 集群服务器间免密 搞了将近两天， 最后发现原因是 我给私钥的权限太大，造成验证不通过 （我给的777，应该是700） zookeeper安装Zookeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储，但是 Zookeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控你存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理，后面将会详细介绍 Zookeeper 能够解决的一些典型问题，这里先介绍一下，Zookeeper 的操作接口和简单使用示例。 根据官网的指引下载zookeeper安装包，解压 在集群各个机器上新建dataDir目录，并在其中新建文件myid，文件中只有一个字–id，如1 cp conf目录下zoo_sample.cfg文件为zoo.cfg，并修改,zookeeper默认从这里读取配置文件1234567891011121314151617181920212223242526272829303132333435 # The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/root/data/zookeeper# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1#logDirdataLogDir=/root/logs/zookeeperserver.1=0.0.0.0:2888:3888server.2=ake:2888:3888server.3=angela:2888:3888server.4=houyi:2888:3888server.5=houzi:2888:3888 其中dataDir为zookeeper存放数据的目录，最后的集群配置：server.id=ip/hostname:该服务器跟leader服务器交换数据的端口:leader服务器挂掉重新选举leader的端口 将在一台机器上配置好的zookeeper，连同配置文件分发到集群的各台机器上，注意配置文件中当前机器的ip/hostname要改成0.0.0.0,不然选举leader时会有异常refstackoverflow 各台服务器中的zookeeper的目录最好一致，在配置文件限定的时间内同时执行zkServer.sh脚本启动服务 123456 /root/zookeeper/zookeeper-3.4.10/bin/zkServer.sh startvim /root/zookeeper/zookeeper-3.4.10/conf/zoo.cfg#停止服务/root/zookeeper/zookeeper-3.4.10/bin/zkServer.sh stop＃查看服务状态/root/zookeeper/zookeeper-3.4.10/bin/zkServer.sh status 默认启动脚本的目录下会生成zookeeper.out文件，日志都记录在这个文件中 正常的status长这个样子 123 ZooKeeper JMX enabled by defaultUsing config: /root/zookeeper/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower ref: IBM CentOS 7 虚拟机搭建zookeeper集群 ZooKeeper深入浅出 KAFKA集群安装 kafka官网下载安装包 解压，并修改配置文件中zookeeper.connect hostname等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# see kafka.server.KafkaConfig for additional details and defaults############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=3# Switch to enable topic deletion or not, default value is false#delete.topic.enable=true############################# Socket Server Settings ############################## The address the socket server listens on. It will get the value returned from# java.net.InetAddress.getCanonicalHostName() if not configured.# FORMAT:# listeners = listener_name://host_name:port# EXAMPLE:# listeners = PLAINTEXT://your.host.name:9092listeners=PLAINTEXT://houyi:9092hostname=houyiport=9092# Hostname and port the broker will advertise to producers and consumers. If not set,# it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value# returned from java.net.InetAddress.getCanonicalHostName().#advertised.listeners=PLAINTEXT://your.host.name:9092# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL# The number of threads that the server uses for receiving requests from the network and sending responses to the networknum.network.threads=3# The number of threads that the server uses for processing requests, which may include disk I/Onum.io.threads=8# The send buffer (SO_SNDBUF) used by the socket serversocket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket serversocket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)socket.request.max.bytes=104857600############################# Log Basics ############################## A comma seperated list of directories under which to store log fileslog.dirs=/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.# This value is recommended to be increased for installations with data dirs located in RAID array.num.recovery.threads.per.data.dir=1############################# Internal Topic Settings ############################## The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot;# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync# the OS cache lazily. The following configurations control the flush of data to disk.# There are a few important trade-offs here:# 1. Durability: Unflushed data may be lost if you are not using replication.# 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.# 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.# The settings below allow one to configure the flush policy to flush data after a period of time or# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disk#log.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flush#log.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can# be set to delete segments after a period of time, or after a given size has accumulated.# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens# from the end of the log.# The minimum age of a log file to be eligible for deletion due to agelog.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining# segments don&apos;t drop below log.retention.bytes. Functions independently of log.retention.hours.#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according# to the retention policieslog.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181# Timeout in ms for connecting to zookeeperzookeeper.connection.timeout.ms=6000############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.# The default value for this is 3 seconds.# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.group.initial.rebalance.delay.ms=0 将解压后的文件分发到集群中的各个机器中，并修改相应的brokerid,保证brokerid唯一 分别在集群的各台机器上执行启动脚本，并测试kafka的可用性 12345bin/kafka-server-start.sh config/server.properties &amp;bin/kafka-topics.sh --create --zookeeper yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181 --replication-factor 3 --partitions 3 --topic topicTestbin/kafka-topics.sh --list --zookeeper yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181bin/kafka-console-producer.sh --broker-list yase:9092,ake:9092,angela:9092,houyi:9092,houzi:9092 --topic topicTestbin/kafka-console-consumer.sh --zookeeper yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181 --topic topicTest --from-beginning hadoop 安装 官网下载binary文件，这里选择了比较稳定的2.7版本 解压缩，配置slaves core-site.xml hdfs-site.xml 等配置文件 将解压后的整个文件分发到集群的各个节点，路径于yase，也就是master节点保持一致，在yase上执行启动脚本时会按照slaves中的配置，分别远程启动slave的datanode 需要注意的是core-site.xml中的临时文件目录设置时，不要加file:// ,不然格式化时会出错 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/hdpspk/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://yase:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12bin/hdfs namenode -formatsbin/start-all.sh ref:hadoop2.7安装 spark安装 照旧，下载解压，配置，分发，master节点脚本开启服务 ref:spark22安装 使用Amazon EC2,ssh 连接总是会自己断开，很讨厌解决办法: 使用客户端工具，例如securecrt连接linux服务器，有的会出现过一段时间没有任何操作，客户端与服务器就断开了连接。造成这个的原因，主要是因为客户端与服务器之间存在路由器，防火墙以及为了本身的安全性，在超过特定的时间后就会把空闲连接断开。或者是服务器端设置了断开空闲连接。那么解决的方法有两种，一是从服务器着手，一是在客户端工具上下手。 服务器端]]></content>
      <categories>
        <category>数据存储处理 - 环境</category>
      </categories>
      <tags>
        <tag>集群</tag>
        <tag>环境</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web Note]]></title>
    <url>%2F2018%2F12%2F04%2FWeb-Note%2F</url>
    <content type="text"><![CDATA[数据结构 队列 集合 链表、数组 字典、关联数组 栈 树 二叉树 完全二叉树 平衡二叉树 二叉查找树（BST） 红黑树 B-，B+，B*树 LSM 树 BitSet 常用算法 排序、查找算法 选择排序 冒泡排序 插入排序 快速排序 归并排序 希尔排序 堆排序 计数排序 桶排序 基数排序 二分查找 Java 中的排序工具 布隆过滤器 字符串比较 KMP 算法 深度优先、广度优先 贪心算法 回溯算法 剪枝算法 动态规划 朴素贝叶斯 推荐算法 最小生成树算法 最短路径算法 并发 多线程 线程安全 一致性、事务 事务 ACID 特性 事务的隔离级别 MVCC 锁 Java中的锁和同步类 公平锁 &amp; 非公平锁 悲观锁 乐观锁 &amp; CAS ABA 问题 CopyOnWrite容器 RingBuffer 可重入锁 &amp; 不可重入锁 互斥锁 &amp; 共享锁 死锁 操作系统 计算机原理 CPU 多级缓存 进程 线程 协程 Linux 设计模式 设计模式的六大原则 23种常见设计模式 应用场景 单例模式 责任链模式 MVC IOC AOP UML 微服务思想 康威定律 运维 &amp; 统计 &amp; 技术支持 常规监控 APM 统计分析 持续集成(CI/CD) Jenkins 环境分离 自动化运维 Ansible puppet chef 测试 TDD 理论 单元测试 压力测试 全链路压测 A/B 、灰度、蓝绿测试 虚拟化 KVM Xen OpenVZ 容器技术 Docker 云技术 OpenStack DevOps 文档管理 中间件 Web Server Nginx OpenResty Apache Httpd Tomcat 架构原理 调优方案 Jetty 缓存 本地缓存 客户端缓存 服务端缓存 Web缓存 Memcached Redis 架构 回收策略 Tair 消息队列 消息总线 消息的顺序 RabbitMQ RocketMQ ActiveMQ Kafka Redis 消息推送 ZeroMQ 定时调度 单机定时调度 分布式定时调度 RPC Dubbo Thrift gRPC 数据库中间件 Sharding Jdbc 日志系统 日志搜集 配置中心 API 网关 网络 协议 OSI 七层协议 TCP/IP HTTP HTTP2.0 HTTPS 网络模型 Epoll Java NIO kqueue 连接和短连接 框架 零拷贝（Zero-copy） 序列化(二进制协议) Hessian Protobuf 数据库 基础理论 数据库设计的三大范式 MySQL 原理 InnoDB 优化 索引 聚集索引, 非聚集索引 复合索引 自适应哈希索引(AHI) explain NoSQL MongoDB Hbase 搜索引擎 搜索引擎原理 Lucene Elasticsearch Solr sphinx 性能 性能优化方法论 容量评估 CDN 网络 连接池 性能调优 大数据 流式计算 Storm Flink Kafka Stream 应用场景 Hadoop HDFS MapReduce Yarn Spark 安全 web 安全 XSS CSRF SQL 注入 Hash Dos 脚本注入 漏洞扫描工具 验证码 DDoS 防范 用户隐私信息保护 序列化漏洞 加密解密 对称加密 哈希算法 非对称加密 服务器安全 数据安全 数据备份 网络隔离 内外网分离 登录跳板机 授权、认证 RBAC OAuth2.0 双因素认证（2FA） 单点登录(SSO) 常用开源框架 开源协议 日志框架 Log4j、Log4j2 Logback ORM 网络框架 Web 框架 Spring 家族 工具框架 分布式设计 扩展性设计 稳定性 &amp; 高可用 硬件负载均衡 软件负载均衡 限流 应用层容灾 跨机房容灾 容灾演练流程 平滑启动 数据库扩展 读写分离模式 分片模式 服务治理 服务注册与发现 服务路由控制 分布式一致 CAP 与 BASE 理论 分布式锁 分布式一致性算法 PAXOS Zab Raft Gossip 两阶段提交、多阶段提交 幂等 分布式一致方案 分布式 Leader 节点选举 TCC(Try/Confirm/Cancel) 柔性事务 分布式文件系统 唯一ID 生成 全局唯一ID 一致性Hash算法 设计思想 &amp; 开发模式 DDD(Domain-driven Design - 领域驱动设计) 命令查询职责分离(CQRS) 贫血，充血模型 Actor 模式 响应式编程 Reactor RxJava Vert.x DODAF2.0 Serverless Service Mesh 项目管理 架构评审 重构 代码规范 代码 Review RUP 看板管理 SCRUM 敏捷开发 极限编程（XP） 结对编程 FMEA管理模式 通用业务术语 技术趋势 政策、法规 法律 严格遵守刑法253法条 架构师素质 团队管理 招聘 资讯 行业资讯 公众号列表 博客 团队博客 个人博客 综合门户、社区 问答、讨论类社区 行业数据分析 专项网站 其他类 推荐参考书 在线电子书 纸质书 开发方面 架构方面 技术管理方面 基础理论 工具方面 大数据方面 技术资源 开源资源 手册、文档、教程 在线课堂 会议、活动 常用APP 找工作 工具 代码托管 文件服务 综合云服务商 VPS （Toc generated by simple-php-github-toc ） 数据结构队列 《java队列——queue详细分析》 非阻塞队列：ConcurrentLinkedQueue(无界线程安全)，采用CAS机制（compareAndSwapObject原子操作）。 阻塞队列：ArrayBlockingQueue(有界)、LinkedBlockingQueue（无界）、DelayQueue、PriorityBlockingQueue，采用锁机制；使用 ReentrantLock 锁。 《LinkedList、ConcurrentLinkedQueue、LinkedBlockingQueue对比分析》 集合 《Java Set集合的详解》 链表、数组 《Java集合详解–什么是List》 字典、关联数组 《Java map 详解 - 用法、遍历、排序、常用API等》 栈 《java数据结构与算法之栈（Stack）设计与实现》 《Java Stack 类》 《java stack的详细实现分析》 Stack 是线程安全的。 内部使用数组保存数据，不够时翻倍。 树二叉树每个节点最多有两个叶子节点。 《二叉树》 完全二叉树 《完全二叉树》 叶节点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树。 平衡二叉树左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 《浅谈数据结构-平衡二叉树》 《浅谈算法和数据结构: 八 平衡查找树之2-3树》 二叉查找树（BST）二叉查找树（Binary Search Tree），也称有序二叉树（ordered binary tree）,排序二叉树（sorted binary tree）。 《浅谈算法和数据结构: 七 二叉查找树》 红黑树 《最容易懂得红黑树》 添加阶段后，左旋或者右旋从而再次达到平衡。 《浅谈算法和数据结构: 九 平衡查找树之红黑树》 B-，B+，B*树MySQL是基于B+树聚集索引组织表 《B-树，B+树，B*树详解》 《B-树，B+树与B*树的优缺点比较》 B+ 树的叶子节点链表结构相比于 B- 树便于扫库，和范围检索。LSM 树 LSM（Log-Structured Merge-Trees）和 B+ 树相比，是牺牲了部分读的性能来换取写的性能(通过批量写入)，实现读写之间的。Hbase、LevelDB、Tair（Long DB）、nessDB 采用 LSM 树的结构。LSM可以快速建立索引。 《LSM树 VS B+树》 B+ 树读性能好，但由于需要有序结构，当key比较分散时，磁盘寻道频繁，造成写性能。 LSM 是将一个大树拆分成N棵小树，先写到内存（无寻道问题，性能高），在内存中构建一颗有序小树（有序树），随着小树越来越大，内存的小树会flush到磁盘上。当读时，由于不知道数据在哪棵小树上，因此必须遍历（二分查找）所有的小树，但在每颗小树内部数据是有序的。 《LSM树（Log-Structured Merge Tree）存储引擎》 极端的说，基于LSM树实现的HBase的写性能比MySQL高了一个数量级，读性能低了一个数量级。 优化方式：Bloom filter 替代二分查找；compact 小数位大树，提高查询性能。 Hbase 中，内存中达到一定阈值后，整体flush到磁盘上、形成一个文件（B+数），HDFS不支持update操作，所以Hbase做整体flush而不是merge update。flush到磁盘上的小树，定期会合并成一个大树。 BitSet经常用于大规模数据的排重检查。 《Java Bitset类》 《Java BitSet（位集）》 常用算法 《常见排序算法及对应的时间复杂度和空间复杂度》 排序、查找算法 《常见排序算法及对应的时间复杂度和空间复杂度》 选择排序 《Java中的经典算法之选择排序（SelectionSort）》 每一趟从待排序的记录中选出最小的元素，顺序放在已排好序的序列最后，直到全部记录排序完毕。 冒泡排序 《冒泡排序的2种写法》 相邻元素前后交换、把最大的排到最后。 时间复杂度 O(n²) 插入排序 《排序算法总结之插入排序》 快速排序 《坐在马桶上看算法：快速排序》 一侧比另外一次都大或小。 归并排序 《图解排序算法(四)之归并排序》 分而治之，分成小份排序，在合并(重建一个新空间进行复制)。 希尔排序TODO 堆排序 《图解排序算法(三)之堆排序》 排序过程就是构建最大堆的过程，最大堆：每个结点的值都大于或等于其左右孩子结点的值，堆顶元素是最大值。 计数排序 《计数排序和桶排序》 和桶排序过程比较像，差别在于桶的数量。 桶排序 《【啊哈！算法】最快最简单的排序——桶排序》 《排序算法（三）：计数排序与桶排序》 桶排序将[0,1)区间划分为n个相同的大小的子区间，这些子区间被称为桶。 每个桶单独进行排序，然后再遍历每个桶。 基数排序按照个位、十位、百位、…依次来排。 《排序算法系列：基数排序》 《基数排序》 二分查找 《二分查找(java实现)》 要求待查找的序列有序。 时间复杂度 O(logN)。 《java实现二分查找-两种方式》 while + 递归。Java 中的排序工具 《Arrays.sort和Collections.sort实现原理解析》 Collections.sort算法调用的是合并排序。 Arrays.sort() 采用了2种排序算法 – 基本类型数据使用快速排序法，对象数组使用归并排序。 布隆过滤器常用于大数据的排重，比如email，url 等。核心原理：将每条数据通过计算产生一个指纹（一个字节或多个字节，但一定比原始数据要少很多），其中每一位都是通过随机计算获得，在将指纹映射到一个大的按位存储的空间中。注意：会有一定的错误率。优点：空间和时间效率都很高。缺点：随着存入的元素数量增加，误算率随之增加。 《布隆过滤器 – 空间效率很高的数据结构》 《大量数据去重：Bitmap和布隆过滤器(Bloom Filter)》 《基于Redis的布隆过滤器的实现》 基于 Redis 的 Bitmap 数据结构。 《网络爬虫：URL去重策略之布隆过滤器(BloomFilter)的使用》 使用Java中的 BitSet 类 和 加权和hash算法。 字符串比较KMP 算法KMP：Knuth-Morris-Pratt算法（简称KMP）核心原理是利用一个“部分匹配表”，跳过已经匹配过的元素。 《字符串匹配的KMP算法》 深度优先、广度优先 《广度优先搜索BFS和深度优先搜索DFS》 贪心算法 《算法：贪婪算法基础》 《常见算法及问题场景——贪心算法》 回溯算法 《 五大常用算法之四：回溯法》 剪枝算法 《α-β剪枝算法》 动态规划 《详解动态规划——邹博讲动态规划》 《动态规划算法的个人理解》 朴素贝叶斯 《带你搞懂朴素贝叶斯分类算法》 P(B|A)=P(A|B)P(B)/P(A) 《贝叶斯推断及其互联网应用1》 《贝叶斯推断及其互联网应用2》 推荐算法 《推荐算法综述》 《TOP 10 开源的推荐系统简介》 最小生成树算法 《算法导论–最小生成树（Kruskal和Prim算法）》 最短路径算法 《Dijkstra算法详解》 并发Java 并发 Java 并发知识合集 JAVA并发知识图谱 多线程 《40个Java多线程问题总结》 线程安全 《Java并发编程——线程安全及解决机制简介》 一致性、事务事务 ACID 特性 《数据库事务ACID特性》 事务的隔离级别 未提交读：一个事务可以读取另一个未提交的数据，容易出现脏读的情况。 读提交：一个事务等另外一个事务提交之后才可以读取数据，但会出现不可重复读的情况（多次读取的数据不一致），读取过程中出现UPDATE操作，会多。（大多数数据库默认级别是RC，比如SQL Server，Oracle），读取的时候不可以修改。 可重复读： 同一个事务里确保每次读取的时候，获得的是同样的数据，但不保障原始数据被其他事务更新（幻读），Mysql InnoDB 就是这个级别。 序列化：所有事物串行处理（牺牲了效率） 《理解事务的4种隔离级别》 数据库事务的四大特性及事务隔离级别 《MySQL的InnoDB的幻读问题 》 幻读的例子非常清楚。 通过 SELECT … FOR UPDATE 解决。 《一篇文章带你读懂MySQL和InnoDB》 图解脏读、不可重复读、幻读问题。 MVCC 《【mysql】关于innodb中MVCC的一些理解》 innodb 中 MVCC 用在 Repeatable-Read 隔离级别。 MVCC 会产生幻读问题（更新时异常。） 《轻松理解MYSQL MVCC 实现机制》 通过隐藏版本列来实现 MVCC 控制，一列记录创建时间、一列记录删除时间，这里的时间 每次只操作比当前版本小（或等于）的 行。 锁Java中的锁和同步类 《Java中的锁分类》 主要包括 synchronized、ReentrantLock、和 ReadWriteLock。 《Java并发之AQS详解》 《Java中信号量 Semaphore》 有数量控制 申请用 acquire，申请不要则阻塞；释放用 release。 《java开发中的Mutex vs Semaphore》 简单的说 就是Mutex是排它的，只有一个可以获取到资源， Semaphore也具有排它性，但可以定义多个可以获取的资源的对象。 公平锁 &amp; 非公平锁公平锁的作用就是严格按照线程启动的顺序来执行的，不允许其他线程插队执行的；而非公平锁是允许插队的。 《公平锁与非公平锁》 默认情况下 ReentrantLock 和 synchronized 都是非公平锁。ReentrantLock 可以设置成公平锁。 悲观锁悲观锁如果使用不当（锁的条数过多），会引起服务大面积等待。推荐优先使用乐观锁+重试。 《【MySQL】悲观锁&amp;乐观锁》 乐观锁的方式：版本号+重试方式 悲观锁：通过 select … for update 进行行锁(不可读、不可写，share 锁可读不可写)。 《Mysql查询语句使用select.. for update导致的数据库死锁分析》 mysql的innodb存储引擎实务锁虽然是锁行，但它内部是锁索引的。 锁相同数据的不同索引条件可能会引起死锁。 《Mysql并发时经典常见的死锁原因及解决方法》 乐观锁 &amp; CAS 《乐观锁的一种实现方式——CAS》 和MySQL乐观锁方式相似，只不过是通过和原值进行比较。 ABA 问题由于高并发，在CAS下，更新后可能此A非彼A。通过版本号可以解决，类似于上文Mysql 中提到的的乐观锁。 《Java CAS 和ABA问题》 《Java 中 ABA问题及避免》 AtomicStampedReference 和 AtomicStampedReference。 CopyOnWrite容器可以对CopyOnWrite容器进行并发的读，而不需要加锁。CopyOnWrite并发容器用于读多写少的并发场景。比如白名单，黑名单，商品类目的访问和更新场景，不适合需要数据强一致性的场景。 《JAVA中写时复制(Copy-On-Write)Map实现》 实现读写分离，读取发生在原始数据上，写入发生在副本上。 不用加锁，通过最终一致实现一致性。 《聊聊并发-Java中的Copy-On-Write容器》 RingBuffer 《线程安全的无锁RingBuffer的实现【一个读线程，一个写线程】》 可重入锁 &amp; 不可重入锁 《可重入锁和不可重入锁》 通过简单代码举例说明可重入锁和不可重入锁。 可重入锁指同一个线程可以再次获得之前已经获得的锁。 可重入锁可以用户避免死锁。 Java中的可重入锁：synchronized 和 java.util.concurrent.locks.ReentrantLock 《ReenTrantLock可重入锁（和synchronized的区别）总结》 synchronized 使用方便，编译器来加锁，是非公平锁。 ReenTrantLock 使用灵活，锁的公平性可以定制。 相同加锁场景下，推荐使用 synchronized。 互斥锁 &amp; 共享锁互斥锁：同时只能有一个线程获得锁。比如，ReentrantLock 是互斥锁，ReadWriteLock 中的写锁是互斥锁。共享锁：可以有多个线程同时或的锁。比如，Semaphore、CountDownLatch 是共享锁，ReadWriteLock 中的读锁是共享锁。 《ReadWriteLock场景应用》 死锁 《“死锁”四个必要条件的合理解释》 互斥、持有、不可剥夺、环形等待。 Java如何查看死锁？ JConsole 可以识别死锁。 java多线程系列：死锁及检测 jstack 可以显示死锁。 操作系统计算机原理 《操作系统基础知识——操作系统的原理，类型和结构》 CPU多级缓存典型的 CPU 有三级缓存，距离核心越近，速度越快，空间越小。L1 一般 32k，L2 一般 256k，L3 一般12M。内存速度需要200个 CPU 周期，CPU 缓存需要1个CPU周期。 《从Java视角理解CPU缓存和伪共享》 进程TODO 线程 《线程的生命周期及状态转换详解》 协程 《终结python协程—-从yield到actor模型的实现》 线程的调度是由操作系统负责，协程调度是程序自行负责 与线程相比，协程减少了无谓的操作系统切换. 实际上当遇到IO操作时做切换才更有意义，（因为IO操作不用占用CPU），如果没遇到IO操作，按照时间片切换. Linux 《Linux 命令大全》 设计模式设计模式的六大原则 《设计模式的六大原则》 开闭原则：对扩展开放,对修改关闭，多使用抽象类和接口。 里氏替换原则：基类可以被子类替换，使用抽象类继承,不使用具体类继承。 依赖倒转原则：要依赖于抽象,不要依赖于具体，针对接口编程,不针对实现编程。 接口隔离原则：使用多个隔离的接口,比使用单个接口好，建立最小的接口。 迪米特法则：一个软件实体应当尽可能少地与其他实体发生相互作用，通过中间类建立联系。 合成复用原则：尽量使用合成/聚合,而不是使用继承。 23种常见设计模式 《设计模式》 《23种设计模式全解析》 应用场景 《细数JDK里的设计模式》 结构型模式： 适配器：用来把一个接口转化成另一个接口，如 java.util.Arrays#asList()。 桥接模式：这个模式将抽象和抽象操作的实现进行了解耦，这样使得抽象和实现可以独立地变化，如JDBC； 组合模式：使得客户端看来单个对象和对象的组合是同等的。换句话说，某个类型的方法同时也接受自身类型作为参数，如 Map.putAll，List.addAll、Set.addAll。 装饰者模式：动态的给一个对象附加额外的功能，这也是子类的一种替代方式，如 java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap。 享元模式：使用缓存来加速大量小对象的访问时间，如 valueOf(int)。 代理模式：代理模式是用一个简单的对象来代替一个复杂的或者创建耗时的对象，如 java.lang.reflect.Proxy 创建模式: 抽象工厂模式：抽象工厂模式提供了一个协议来生成一系列的相关或者独立的对象，而不用指定具体对象的类型，如 java.util.Calendar#getInstance()。 建造模式(Builder)：定义了一个新的类来构建另一个类的实例，以简化复杂对象的创建，如：java.lang.StringBuilder#append()。 工厂方法：就是 一个返* 回具体对象的方法，而不是多个，如 java.lang.Object#toString()、java.lang.Class#newInstance()。 原型模式：使得类的实例能够生成自身的拷贝、如：java.lang.Object#clone()。 单例模式：全局只有一个实例，如 java.lang.Runtime#getRuntime()。 行为模式： 责任链模式：通过把请求从一个对象传递到链条中下一个对象的方式，直到请求被处理完毕，以实现对象间的解耦。如 javax.servlet.Filter#doFilter()。 命令模式：将操作封装到对象内，以便存储，传递和返回，如：java.lang.Runnable。 解释器模式：定义了一个语言的语法，然后解析相应语法的语句，如，java.text.Format，java.text.Normalizer。 迭代器模式：提供一个一致的方法来顺序访问集合中的对象，如 java.util.Iterator。 中介者模式：通过使用一个中间对象来进行消息分发以及减少类之间的直接依赖，java.lang.reflect.Method#invoke()。 空对象模式：如 java.util.Collections#emptyList()。 观察者模式：它使得一个对象可以灵活的将消息发送给感兴趣的对象，如 java.util.EventListener。 模板方法模式：让子类可以重写方法的一部分，而不是整个重写，如 java.util.Collections#sort()。 《Spring-涉及到的设计模式汇总》 《Mybatis使用的设计模式》 单例模式 《单例模式的三种实现 以及各自的优缺点》 《单例模式－－反射－－防止序列化破坏单例模式》 使用枚举类型。 责任链模式TODO MVC 《MVC 模式》 模型(model)－视图(view)－控制器(controller) IOC 《理解 IOC》 《IOC 的理解与解释》 正向控制：传统通过new的方式。反向控制，通过容器注入对象。 作用：用于模块解耦。 DI：Dependency Injection，即依赖注入，只关心资源使用，不关心资源来源。 AOP 《轻松理解AOP(面向切面编程)》 《Spring AOP详解》 《Spring AOP的实现原理》 Spring AOP使用的动态代理，主要有两种方式：JDK动态代理和CGLIB动态代理。 《Spring AOP 实现原理与 CGLIB 应用》 Spring AOP 框架对 AOP 代理类的处理原则是：如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类；如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类 UML 《UML教程》 微服务思想 《微服务架构设计》 《微服务架构技术栈选型手册》 康威定律 《微服务架构的理论基础 - 康威定律》 定律一：组织沟通方式会通过系统设计表达出来，就是说架构的布局和组织结构会有相似。 定律二：时间再多一件事情也不可能做的完美，但总有时间做完一件事情。一口气吃不成胖子，先搞定能搞定的。 定律三：线型系统和线型组织架构间有潜在的异质同态特性。种瓜得瓜，做独立自治的子系统减少沟通成本。 定律四：大的系统组织总是比小系统更倾向于分解。合久必分，分而治之。 《微服务架构核⼼20讲》 运维 &amp; 统计 &amp; 技术支持常规监控 《腾讯业务系统监控的修炼之路》 监控的方式：主动、被动、旁路(比如舆情监控) 监控类型： 基础监控、服务端监控、客户端监控、监控、用户端监控 监控的目标：全、块、准 核心指标：请求量、成功率、耗时 《开源还是商用？十大云运维监控工具横评》 Zabbix、Nagios、Ganglia、Zenoss、Open-falcon、监控宝、 360网站服务监控、阿里云监控、百度云观测、小蜜蜂网站监测等。 《监控报警系统搭建及二次开发经验》 命令行监控工具 《常用命令行监控工具》 top、sar、tsar、nload 《20个命令行工具监控 Linux 系统性能》 《JVM性能调优监控工具jps、jstack、jmap、jhat、jstat、hprof使用详解》 APMAPM — Application Performance Management 《Dapper，大规模分布式系统的跟踪系统》 CNCF OpenTracing，中文版 主要开源软件，按字母排序 Apache SkyWalking CAT CNCF jaeger Pinpoint Zipkin 《开源APM技术选型与实战》 主要基于 Google的Dapper（大规模分布式系统的跟踪系统） 思想。 统计分析 《流量统计的基础：埋点》 常用指标：访问与访客、停留时长、跳出率、退出率、转化率、参与度 《APP埋点常用的统计工具、埋点目标和埋点内容》 第三方统计：友盟、百度移动、魔方、App Annie、talking data、神策数据等。 《美团点评前端无痕埋点实践》 所谓无痕、即通过可视化工具配置采集节点，在前端自动解析配置并上报埋点数据，而非硬编码。 持续集成(CI/CD) 《持续集成是什么？》 《8个流行的持续集成工具》 Jenkins 《使用Jenkins进行持续集成》 环境分离开发、测试、生成环境分离。 《开发环境、生产环境、测试环境的基本理解和区》 自动化运维Ansible 《Ansible中文权威指南》 《Ansible基础配置和企业级项目实用案例》 puppet 《自动化运维工具——puppet详解》 chef 《Chef 的安装与使用》 测试TDD 理论 《深度解读 - TDD（测试驱动开发）》 基于测试用例编码功能代码，XP（Extreme Programming）的核心实践. 好处：一次关注一个点，降低思维负担；迎接需求变化或改善代码的设计；提前澄清需求；快速反馈； 单元测试 《Java单元测试之JUnit篇》 小谈UT 《JUnit 4 与 TestNG 对比》 TestNG 覆盖 JUnit 功能，适用于更复杂的场景。 《单元测试主要的测试功能点》 模块接口测试、局部数据结构测试、路径测试 、错误处理测试、边界条件测试 。 压力测试 《Apache ab 测试使用指南》 《大型网站压力测试及优化方案》 《10大主流压力/负载/性能测试工具推荐》 《真实流量压测工具 tcpcopy应用浅析》 《nGrinder 简易使用教程》 全链路压测 《京东618：升级全链路压测方案，打造军演机器人ForceBot》 《饿了么全链路压测的探索与实践》 《四大语言，八大框架｜滴滴全链路压测解决之道》 《全链路压测经验》 A/B 、灰度、蓝绿测试 《技术干货 | AB 测试和灰度发布探索及实践》 《nginx 根据IP 进行灰度发布》 《蓝绿部署、A/B 测试以及灰度发布》 虚拟化 《VPS的三种虚拟技术OpenVZ、Xen、KVM优缺点比较》 KVM 《KVM详解，太详细太深入了，经典》 《【图文】KVM 虚拟机安装详解》 Xen 《Xen虚拟化基本原理详解》 OpenVZ 《开源Linux容器 OpenVZ 快速上手指南》 容器技术Docker 《几张图帮你理解 docker 基本原理及快速入门》 《Docker 核心技术与实现原理》 《Docker 教程》 云技术OpenStack 《OpenStack构架知识梳理》 DevOps 《一分钟告诉你究竟DevOps是什么鬼？》 《DevOps详解》 文档管理 Confluence-收费文档管理系统 GitLab? Wiki 中间件Web ServerNginx 《Ngnix的基本学习-多进程和Apache的比较》 Nginx 通过异步非阻塞的事件处理机制实现高并发。Apache 每个请求独占一个线程，非常消耗系统资源。 事件驱动适合于IO密集型服务(Nginx)，多进程或线程适合于CPU密集型服务(Apache)，所以Nginx适合做反向代理，而非web服务器使用。 《nginx与Apache的对比以及优缺点》 nginx只适合静态和反向代理，不适合处理动态请求。 OpenResty 官方网站 《浅谈 OpenResty》 通过 Lua 模块可以在Nginx上进行开发。 Apache Httpd 官方网站 Tomcat架构原理 《TOMCAT原理详解及请求过程》 《Tomcat服务器原理详解》 《Tomcat 系统架构与设计模式,第 1 部分: 工作原理》 《四张图带你了解Tomcat系统架构》 《JBoss vs. Tomcat: Choosing A Java Application Server》 Tomcat 是轻量级的 Serverlet 容器，没有实现全部 JEE 特性（比如持久化和事务处理），但可以通过其他组件代替，比如Srping。 Jboss 实现全部了JEE特性，软件开源免费、文档收费。 调优方案 《Tomcat 调优方案》 启动NIO模式（或者APR）；调整线程池；禁用AJP连接器（Nginx+tomcat的架构，不需要AJP）； 《tomcat http协议与ajp协议》 《AJP与HTTP比较和分析》 AJP 协议（8009端口）用于降低和前端Server（如Apache，而且需要支持AJP协议）的连接数(前端)，通过长连接提高性能。 并发高时，AJP协议优于HTTP协议。 Jetty 《Jetty 的工作原理以及与 Tomcat 的比较》 《jetty和tomcat优势比较》 架构比较:Jetty的架构比Tomcat的更为简单。 性能比较：Jetty和Tomcat性能方面差异不大，Jetty默认采用NIO结束在处理I/O请求上更占优势，Tomcat默认采用BIO处理I/O请求，Tomcat适合处理少数非常繁忙的链接，处理静态资源时性能较差。 其他方面：Jetty的应用更加快速，修改简单，对新的Servlet规范的支持较好;Tomcat 对JEE和Servlet 支持更加全面。 缓存 《缓存失效策略（FIFO 、LRU、LFU三种算法的区别）》 本地缓存 《HashMap本地缓存》 《EhCache本地缓存》 堆内、堆外、磁盘三级缓存。 可按照缓存空间容量进行设置。 按照时间、次数等过期策略。 《Guava Cache》 简单轻量、无堆外、磁盘缓存。 《Nginx本地缓存》 《Pagespeed—懒人工具，服务器端加速》 客户端缓存 《浏览器端缓存》 主要是利用 Cache-Control 参数。 《H5 和移动端 WebView 缓存机制解析与实战》 服务端缓存Web缓存 nuster - nuster cache varnish - varnish cache squid - squid cache Memcached 《Memcached 教程》 《深入理解Memcached原理》 采用多路复用技术提高并发性。 slab分配算法： memcached给Slab分配内存空间，默认是1MB。分配给Slab之后 把slab的切分成大小相同的chunk，Chunk是用于缓存记录的内存空间，Chunk 的大小默认按照1.25倍的速度递增。好处是不会频繁申请内存，提高IO效率，坏处是会有一定的内存浪费。 《Memcached软件工作原理》 《Memcache技术分享：介绍、使用、存储、算法、优化、命中率》 《memcache 中 add 、 set 、replace 的区别》 区别在于当key存在还是不存在时，返回值是true和false的。 《memcached全面剖析》 Redis 《Redis 教程》 《redis底层原理》 使用 ziplist 存储链表，ziplist是一种压缩链表，它的好处是更能节省内存空间，因为它所存储的内容都是在连续的内存区域当中的。 使用 skiplist(跳跃表)来存储有序集合对象、查找上先从高Level查起、时间复杂度和红黑树相当，实现容易，无锁、并发性好。 《Redis持久化方式》 RDB方式：定期备份快照，常用于灾难恢复。优点：通过fork出的进程进行备份，不影响主进程、RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。缺点：会丢数据。 AOF方式：保存操作日志方式。优点：恢复时数据丢失少，缺点：文件大，回复慢。 也可以两者结合使用。 《分布式缓存–序列3–原子操作与CAS乐观锁》 架构 《Redis单线程架构》 回收策略 《redis的回收策略》 Tair 官方网站 《Tair和Redis的对比》 特点：可以配置备份节点数目，通过异步同步到备份节点 一致性Hash算法。 架构：和Hadoop 的设计思想类似，有Configserver，DataServer，Configserver 通过心跳来检测，Configserver也有主备关系。 几种存储引擎: MDB，完全内存性，可以用来存储Session等数据。 Rdb（类似于Redis），轻量化，去除了aof之类的操作，支持Restfull操作 LDB（LevelDB存储引擎），持久化存储，LDB 作为rdb的持久化，google实现，比较高效，理论基础是LSM(Log-Structured-Merge Tree)算法，现在内存中修改数据，达到一定量时（和内存汇总的旧数据一同写入磁盘）再写入磁盘，存储更加高效，县比喻Hash算法。 Tair采用共享内存来存储数据，如果服务挂掉（非服务器），重启服务之后，数据亦然还在。 消息队列 《消息队列-推/拉模式学习 &amp; ActiveMQ及JMS学习》 RabbitMQ 消费者默认是推模式（也支持拉模式）。 Kafka 默认是拉模式。 Push方式：优点是可以尽可能快地将消息发送给消费者，缺点是如果消费者处理能力跟不上，消费者的缓冲区可能会溢出。 Pull方式：优点是消费端可以按处理能力进行拉去，缺点是会增加消息延迟。 《Kafka、RabbitMQ、RocketMQ等消息中间件的对比 —— 消息发送性能和区别》 消息总线消息总线相当于在消息队列之上做了一层封装，统一入口，统一管控、简化接入成本。 《消息总线VS消息队列》 消息的顺序 《如何保证消费者接收消息的顺序》 RabbitMQ支持事务，推拉模式都是支持、适合需要可靠性消息传输的场景。 《RabbitMQ的应用场景以及基本原理介绍》 《消息队列之 RabbitMQ》 《RabbitMQ之消息确认机制（事务+Confirm）》 RocketMQJava实现，推拉模式都是支持，吞吐量逊于Kafka。可以保证消息顺序。 《RocketMQ 实战之快速入门》 《RocketMQ 源码解析》 ActiveMQ纯Java实现，兼容JMS，可以内嵌于Java应用中。 《ActiveMQ消息队列介绍》 Kafka高吞吐量、采用拉模式。适合高IO场景，比如日志同步。 官方网站 《各消息队列对比，Kafka深度解析，众人推荐，精彩好文！》 《Kafka分区机制介绍与示例》 Redis 消息推送生产者、消费者模式完全是客户端行为，list 和 拉模式实现，阻塞等待采用 blpop 指令。 《Redis学习笔记之十：Redis用作消息队列》 ZeroMQ TODO 定时调度单机定时调度 《linux定时任务cron配置》 《Linux cron运行原理》 fork 进程 + sleep 轮询 《Quartz使用总结》 《Quartz源码解析 —- 触发器按时启动原理》 《quartz原理揭秘和源码解读》 定时调度在 QuartzSchedulerThread 代码中，while()无限循环，每次循环取出时间将到的trigger，触发对应的job，直到调度器线程被关闭。 分布式定时调度 《这些优秀的国产分布式任务调度系统，你用过几个？》 opencron、LTS、XXL-JOB、Elastic-Job、Uncode-Schedule、Antares 《Quartz任务调度的基本实现原理》 Quartz集群中，独立的Quartz节点并不与另一其的节点或是管理节点通信，而是通过相同的数据库表来感知到另一Quartz应用的 《Elastic-Job-Lite 源码解析》 《Elastic-Job-Cloud 源码解析》 RPC 《从零开始实现RPC框架 - RPC原理及实现》 核心角色：Server: 暴露服务的服务提供方、Client: 调用远程服务的服务消费方、Registry: 服务注册与发现的注册中心。 《分布式RPC框架性能大比拼 dubbo、motan、rpcx、gRPC、thrift的性能比较》 Dubbo 官方网站 dubbo实现原理简单介绍 ** SPI ** TODO Thrift 官方网站 《Thrift RPC详解》 支持多语言，通过中间语言定义接口。 gRPC服务端可以认证加密，在外网环境下，可以保证数据安全。 官方网站 《你应该知道的RPC原理》 数据库中间件Sharding Jdbc 官网 日志系统日志搜集 《从零开始搭建一个ELKB日志收集系统》 《用ELK搭建简单的日志收集分析系统》 《日志收集系统-探究》 配置中心 Apollo - 携程开源的配置中心应用 Spring Boot 和 Spring Cloud 支持推、拉模式更新配置 支持多种语言 《基于zookeeper实现统一配置管理》 《 Spring Cloud Config 分布式配置中心使用教程》 servlet 3.0 异步特性可用于配置中心的客户端 《servlet3.0 新特性——异步处理》 API 网关主要职责：请求转发、安全认证、协议转换、容灾。 《API网关那些儿》 《谈API网关的背景、架构以及落地方案》 《使用Zuul构建API Gateway》 《Spring Cloud Gateway 源码解析》 《HTTP API网关选择之一Kong介绍》 网络协议OSI 七层协议 《OSI七层协议模型、TCP/IP四层模型学习笔记》 TCP/IP 《深入浅出 TCP/IP 协议》 《TCP协议中的三次握手和四次挥手》 HTTP 《http协议详解(超详细)》 HTTP2.0 《HTTP 2.0 原理详细分析》 《HTTP2.0的基本单位为二进制帧》 利用二进制帧负责传输。 多路复用。 HTTPS 《https原理通俗了解》 使用非对称加密协商加密算法 使用对称加密方式传输数据 使用第三方机构签发的证书，来加密公钥，用于公钥的安全传输、防止被中间人串改。 《八大免费SSL证书-给你的网站免费添加Https安全加密》 网络模型 《web优化必须了解的原理之I/o的五种模型和web的三种工作模式》 五种I/O模型：阻塞I/O，非阻塞I/O，I/O复用、事件(信号)驱动I/O、异步I/O，前四种I/O属于同步操作，I/O的第一阶段不同、第二阶段相同，最后的一种则属于异步操作。 三种 Web Server 工作方式：Prefork(多进程)、Worker方式(线程方式)、Event方式。 《select、poll、epoll之间的区别总结》 select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。 select 有打开文件描述符数量限制，默认1024（2048 for x64），100万并发，就要用1000个进程、切换开销大；poll采用链表结构，没有数量限制。 select，poll “醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，通过回调机制节省大量CPU时间；select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，而epoll只要一次拷贝。 poll会随着并发增加，性能逐渐下降，epoll采用红黑树结构，性能稳定，不会随着连接数增加而降低。 《select，poll，epoll比较 》 在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。 《深入理解Java NIO》 NIO 是一种同步非阻塞的 IO 模型。同步是指线程不断轮询 IO 事件是否就绪，非阻塞是指线程在等待 IO 的时候，可以同时做其他任务 《BIO与NIO、AIO的区别》 《两种高效的服务器设计模型：Reactor和Proactor模型》 Epoll 《epoll使用详解（精髓）》 Java NIO 《深入理解Java NIO》 《Java NIO编写Socket服务器的一个例子》 kqueue 《kqueue用法简介》 连接和短连接 《TCP/IP系列——长连接与短连接的区别》 框架 《Netty原理剖析》 Reactor 模式介绍。 Netty 是 Reactor 模式的一种实现。 零拷贝（Zero-copy） 《对于 Netty ByteBuf 的零拷贝(Zero Copy) 的理解》 多个物理分离的buffer，通过逻辑上合并成为一个，从而避免了数据在内存之间的拷贝。 序列化(二进制协议)Hessian 《Hessian原理分析》Binary-RPC;不仅仅是序列化 Protobuf 《Protobuf协议的Java应用例子》Goolge出品、占用空间和效率完胜其他序列化类库，如Hessian；需要编写 .proto 文件。 《Protocol Buffers序列化协议及应用》 * 关于协议的解释；缺点：可读性差; 《简单的使用 protobuf 和 protostuff》 protostuff 的好处是不用写 .proto 文件，Java 对象直接就可以序列化。 数据库基础理论数据库设计的三大范式 《数据库的三大范式以及五大约束》 第一范式：数据表中的每一列（每个字段）必须是不可拆分的最小单元，也就是确保每一列的原子性； 第二范式（2NF）：满足1NF后，要求表中的所有列，都必须依赖于主键，而不能有任何一列与主键没有关系，也就是说一个表只描述一件事情； 第三范式：必须先满足第二范式（2NF），要求：表中的每一列只与主键直接相关而不是间接相关，（表中的每一列只能依赖于主键）； MySQL原理 《MySQL的InnoDB索引原理详解》 《MySQL存储引擎－－MyISAM与InnoDB区别》 两种类型最主要的差别就是Innodb 支持事务处理与外键和行级锁 《myisam和innodb索引实现的不同》 InnoDB 《一篇文章带你读懂Mysql和InnoDB》 优化 《MySQL36条军规》 《MYSQL性能优化的最佳20+条经验》 《SQL优化之道》 《mysql数据库死锁的产生原因及解决办法》 《导致索引失效的可能情况》 《 MYSQL分页limit速度太慢优化方法》 原则上就是缩小扫描范围。 索引聚集索引, 非聚集索引 《MySQL 聚集索引/非聚集索引简述》 《MyISAM和InnoDB的索引实现》 MyISAM 是非聚集，InnoDB 是聚集 复合索引 《复合索引的优点和注意事项》 自适应哈希索引(AHI) 《InnoDB存储引擎——自适应哈希索引》 explain 《MySQL 性能优化神器 Explain 使用分析》 NoSQLMongoDB MongoDB 教程 《Mongodb相对于关系型数据库的优缺点》 优点：弱一致性（最终一致），更能保证用户的访问速度；内置GridFS，支持大容量的存储；Schema-less 数据库，不用预先定义结构；内置Sharding；相比于其他NoSQL，第三方支持丰富；性能优越； 缺点：mongodb不支持事务操作；mongodb占用空间过大；MongoDB没有如MySQL那样成熟的维护工具，这对于开发和IT运营都是个值得注意的地方； Hbase 《简明 HBase 入门教程（开篇）》 《深入学习HBase架构原理》 《传统的行存储和（HBase）列存储的区别》 《Hbase与传统数据库的区别》 空数据不存储，节省空间，且适用于并发。 《HBase Rowkey设计》 rowkey 按照字典顺序排列，便于批量扫描。 通过散列可以避免热点。 搜索引擎搜索引擎原理 《倒排索引–搜索引擎入门》 Lucene 《Lucene入门简介》 Elasticsearch 《Elasticsearch学习，请先看这一篇！》 《Elasticsearch索引原理》 Solr 《 Apache Solr入门教程》 《elasticsearch与solr比较》 sphinx 《Sphinx 的介绍和原理探索》 性能性能优化方法论 《15天的性能优化工作，5方面的调优经验》 代码层面、业务层面、数据库层面、服务器层面、前端优化。 《系统性能优化的几个方面》 容量评估 《联网性能与容量评估的方法论和典型案例》 《互联网架构，如何进行容量设计？》 评估总访问量、评估平均访问量QPS、评估高峰QPS、评估系统、单机极限QPS CDN 网络 《CDN加速原理》 《国内有哪些比较好的 CDN？》 连接池 《主流Java数据库连接池比较与开发配置实战》 性能调优 《九大Java性能调试工具，必备至少一款》 大数据流式计算Storm 官方网站 《最详细的Storm入门教程》 Flink 《Flink之一 Flink基本原理介绍》 Kafka Stream 《Kafka Stream调研：一种轻量级流计算模式》 应用场景例如： 广告相关实时统计； 推荐系统用户画像标签实时更新； 线上服务健康状况实时监测； 实时榜单； 实时数据统计。 Hadoop 《用通俗易懂的话说下hadoop是什么,能做什么》 《史上最详细的Hadoop环境搭建》 HDFS 《【Hadoop学习】HDFS基本原理》 MapReduce 《用通俗易懂的大白话讲解Map/Reduce原理》 《 简单的map-reduce的java例子》 Yarn 《初步掌握Yarn的架构及原理》 Spark 《Spark(一): 基本架构及原理》 安全web 安全XSS 《xss攻击原理与解决方法》CSRF 《CSRF原理及防范》 SQL 注入 《SQL注入》 Hash Dos 《邪恶的JAVA HASH DOS攻击》 利用JsonObjet 上传大Json，JsonObject 底层使用HashMap；不同的数据产生相同的hash值，使得构建Hash速度变慢，耗尽CPU。 《一种高级的DoS攻击-Hash碰撞攻击》 《关于Hash Collision DoS漏洞：解析与解决方案》 脚本注入 《上传文件漏洞原理及防范》 漏洞扫描工具 《DVWA》 W3af OpenVAS详解 验证码 《验证码原理分析及实现》 《详解滑动验证码的实现原理》 滑动验证码是根据人在滑动滑块的响应时间，拖拽速度，时间，位置，轨迹，重试次数等来评估风险。 《淘宝滑动验证码研究》 DDoS 防范 《学习手册：DDoS的攻击方式及防御手段》 《免费DDoS攻击测试工具大合集》 用户隐私信息保护 用户密码非明文保存，加动态salt。 身份证号，手机号如果要显示，用 “*” 替代部分字符。 联系方式在的显示与否由用户自己控制。 TODO 《个人隐私包括哪些》 《在互联网上，隐私的范围包括哪些？》 《用户密码保存》 序列化漏洞 《Lib之过？Java反序列化漏洞通用利用分析》 加密解密对称加密 《常见对称加密算法》 DES、3DES、Blowfish、AES DES 采用 56位秘钥，Blowfish 采用1到448位变长秘钥，AES 128，192和256位长度的秘钥。 DES 秘钥太短（只有56位）算法目前已经被 AES 取代，并且 AES 有硬件加速，性能很好。 哈希算法 《常用的哈希算法》 MD5 和 SHA-1 已经不再安全，已被弃用。 目前 SHA-256 是比较安全的。 《基于Hash摘要签名的公网URL签名验证设计方案》 非对称加密 《常见非对称加密算法》 RSA、DSA、ECDSA(螺旋曲线加密算法) 和 RSA 不同的是 DSA 仅能用于数字签名，不能进行数据加密解密，其安全性和RSA相当，但其性能要比RSA快。 256位的ECC秘钥的安全性等同于3072位的RSA秘钥。 《区块链的加密技术》 服务器安全 《Linux强化论：15步打造一个安全的Linux服务器》 数据安全数据备份TODO 网络隔离内外网分离TODO 登录跳板机在内外环境中通过跳板机登录到线上主机。 《搭建简易堡垒机》 授权、认证RBAC 《基于组织角色的权限设计》 《权限系统与RBAC模型概述》 《Spring整合Shiro做权限控制模块详细案例分析》 OAuth2.0 《理解OAuth 2.0》 《一张图搞定OAuth2.0》 双因素认证（2FA）2FA - Two-factor authentication，用于加强登录验证 常用做法是 登录密码 + 手机验证码（或者令牌Key，类似于与网银的 USB key） 【《双因素认证（2FA）教程》】(http://www.ruanyifeng.com/blog/2017/11/2fa-tutorial.html) 单点登录(SSO) 《单点登录原理与简单实现》 CAS单点登录框架 常用开源框架开源协议 《开源协议的选择》 如何选择一个开源软件协议 日志框架Log4j、Log4j2 《log4j 详细讲解》 《log4j2 实际使用详解》 《Log4j1,Logback以及Log4j2性能测试对比》 Log4J 异步日志性能优异。 Logback 《最全LogBack 详解、含java案例和配置说明》 ORM 《ORM框架使用优缺点》 主要目的是为了提高开发效率。 MyBatis： 《mybatis缓存机制详解》 一级缓存是SqlSession级别的缓存，缓存的数据只在SqlSession内有效 二级缓存是mapper级别的缓存，同一个namespace公用这一个缓存，所以对SqlSession是共享的；使用 LRU 机制清理缓存，通过 cacheEnabled 参数开启。 《MyBatis学习之代码生成器Generator》 网络框架TODO Web 框架Spring 家族Spring Spring 简明教程 Spring Boot 官方网站 《Spring Boot基础教程》 Spring Cloud Spring Boot 中文索引站 Spring Cloud 中文文档 《Spring Cloud基础教程》 工具框架 《Apache Commons 工具类介绍及简单使用》 《Google guava 中文教程》 分布式设计扩展性设计 《架构师不可不知的十大可扩展架构》 总结下来，通用的套路就是分布、缓存及异步处理。 《可扩展性设计之数据切分》 水平切分+垂直切分 利用中间件进行分片如，MySQL Proxy。 利用分片策略进行切分，如按照ID取模。 《说说如何实现可扩展性的大型网站架构》 分布式服务+消息队列。 《大型网站技术架构（七）–网站的可扩展性架构》 稳定性 &amp; 高可用 《系统设计：关于高可用系统的一些技术方案》 可扩展：水平扩展、垂直扩展。 通过冗余部署，避免单点故障。 隔离：避免单一业务占用全部资源。避免业务之间的相互影响 2. 机房隔离避免单点故障。 解耦：降低维护成本，降低耦合风险。减少依赖，减少相互间的影响。 限流：滑动窗口计数法、漏桶算法、令牌桶算法等算法。遇到突发流量时，保证系统稳定。 降级：紧急情况下释放非核心功能的资源。牺牲非核心业务，保证核心业务的高可用。 熔断：异常情况超出阈值进入熔断状态，快速失败。减少不稳定的外部依赖对核心服务的影响。 自动化测试：通过完善的测试，减少发布引起的故障。 灰度发布：灰度发布是速度与安全性作为妥协，能够有效减少发布故障。 《关于高可用的系统》 设计原则：数据不丢(持久化)；服务高可用(服务副本)；绝对的100%高可用很难，目标是做到尽可能多的9，如99.999%（全年累计只有5分钟）。 硬件负载均衡 《转！！负载均衡器技术Nginx和F5的优缺点对比》 主要是和F5对比。 《软/硬件负载均衡产品 你知多少？》 软件负载均衡 《几种负载均衡算法》 轮寻、权重、负载、最少连接、QoS 《DNS负载均衡》 配置简单，更新速度慢。 《Nginx负载均衡》 简单轻量、学习成本低；主要适用于web应用。 《借助LVS+Keepalived实现负载均衡 》 配置比较负载、只支持到4层，性能较高。 《HAProxy用法详解 全网最详细中文文档》 支持到七层（比如HTTP）、功能比较全面，性能也不错。 《Haproxy+Keepalived+MySQL实现读均衡负载》 主要是用户读请求的负载均衡。 《rabbitmq+haproxy+keepalived实现高可用集群搭建》 限流 《谈谈高并发系统的限流》 计数器：通过滑动窗口计数器，控制单位时间内的请求次数，简单粗暴。 漏桶算法：固定容量的漏桶，漏桶满了就丢弃请求，比较常用。 令牌桶算法：固定容量的令牌桶，按照一定速率添加令牌，处理请求前需要拿到令牌，拿不到令牌则丢弃请求，或进入丢队列，可以通过控制添加令牌的速率，来控制整体速度。Guava 中的 RateLimiter 是令牌桶的实现。 Nginx 限流：通过 limit_req 等模块限制并发连接数。 应用层容灾 《防雪崩利器：熔断器 Hystrix 的原理与使用》 雪崩效应原因：硬件故障、硬件故障、程序Bug、重试加大流量、用户大量请求。 雪崩的对策：限流、改进缓存模式(缓存预加载、同步调用改异步)、自动扩容、降级。 Hystrix设计原则： 资源隔离：Hystrix通过将每个依赖服务分配独立的线程池进行资源隔离, 从而避免服务雪崩。 熔断开关：服务的健康状况 = 请求失败数 / 请求总数，通过阈值设定和滑动窗口控制开关。 命令模式：通过继承 HystrixCommand 来包装服务调用逻辑。 《缓存穿透，缓存击穿，缓存雪崩解决方案分析》 《缓存击穿、失效以及热点key问题》 主要策略：失效瞬间：单机使用锁；使用分布式锁；不过期； 热点数据：热点数据单独存储；使用本地缓存；分成多个子key； 跨机房容灾 《“异地多活”多机房部署经验谈》 通过自研中间件进行数据同步。 《异地多活（异地双活）实践经验》 注意延迟问题，多次跨机房调用会将延时放大数倍。 建房间专线很大概率会出现问题，做好运维和程序层面的容错。 不能依赖于程序端数据双写，要有自动同步方案。 数据永不在高延迟和较差网络质量下，考虑同步质量问题。 核心业务和次要业务分而治之，甚至只考虑核心业务。 异地多活监控部署、测试也要跟上。 业务允许的情况下考虑用户分区，尤其是游戏、邮箱业务。 控制跨机房消息体大小，越小越好。 考虑使用docker容器虚拟化技术，提高动态调度能力。 容灾技术及建设经验介绍 容灾演练流程 《依赖治理、灰度发布、故障演练，阿里电商故障演练系统的设计与实战经验》 常见故障画像 案例：预案有效性、预案有效性、故障复现、架构容灾测试、参数调优、参数调优、故障突袭、联合演练。 平滑启动 平滑重启应用思路 1.端流量（如vip层）、2. flush 数据(如果有)、3, 重启应用 《JVM安全退出（如何优雅的关闭java服务）》推荐推出方式：System.exit，Kill SIGTERM；不推荐 kill-9；用 Runtime.addShutdownHook 注册钩子。 《常见Java应用如何优雅关闭》Java、Srping、Dubbo 优雅关闭方式。 数据库扩展读写分离模式 《Mysql主从方案的实现》 《搭建MySQL主从复制经典架构》 《Haproxy+多台MySQL从服务器(Slave) 实现负载均衡》 《DRBD+Heartbeat+Mysql高可用读写分离架构》 DRDB 进行磁盘复制，避免单点问题。 《MySQL Cluster 方式》 分片模式 《分库分表需要考虑的问题及方案》 中间件： 轻量级：sharding-jdbc、TSharding；重量级：Atlas、MyCAT、Vitess等。 问题：事务、Join、迁移、扩容、ID、分页等。 事务补偿：对数据进行对帐检查;基于日志进行比对;定期同标准数据来源进行同步等。 分库策略：数值范围；取模；日期等。 分库数量：通常 MySQL 单库 5千万条、Oracle 单库一亿条需要分库。 《MySql分表和表分区详解》 分区：是MySQL内部机制，对客户端透明，数据存储在不同文件中，表面上看是同一个表。 分表：物理上创建不同的表、客户端需要管理分表路由。 服务治理服务注册与发现 《永不失联！如何实现微服务架构中的服务发现？》 客户端服务发现模式：客户端直接查询注册表，同时自己负责负载均衡。Eureka 采用这种方式。 服务器端服务发现模式：客户端通过负载均衡查询服务实例。 《SpringCloud服务注册中心比较:Consul vs Zookeeper vs Etcd vs Eureka》 CAP支持：Consul（CA）、zookeeper（cp）、etcd（cp） 、euerka（ap） 作者认为目前 Consul 对 Spring cloud 的支持比较好。 《基于Zookeeper的服务注册与发现》 优点：API简单、Pinterest，Airbnb 在用、多语言、通过watcher机制来实现配置PUSH，能快速响应配置变化。 服务路由控制 《分布式服务框架学习笔记4 服务路由》 原则：透明化路由 负载均衡策略：随机、轮询、服务调用延迟、一致性哈希、粘滞连接 本地路由有限策略：injvm(优先调用jvm内部的服务)，innative(优先使用相同物理机的服务),原则上找距离最近的服务。 配置方式：统一注册表；本地配置；动态下发。 分布式一致CAP 与 BASE 理论 《从分布式一致性谈到CAP理论、BASE理论》 一致性分类：强一致(立即一致)；弱一致(可在单位时间内实现一致，比如秒级)；最终一致(弱一致的一种，一定时间内最终一致) CAP：一致性、可用性、分区容错性(网络故障引起) BASE：Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性） BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。 分布式锁 《分布式锁的几种实现方式》 基于数据库的分布式锁：优点：操作简单、容易理解。缺点：存在单点问题、数据库性能够开销较大、不可重入； 基于缓存的分布式锁：优点：非阻塞、性能好。缺点：操作不好容易造成锁无法释放的情况。 Zookeeper 分布式锁：通过有序临时节点实现锁机制，自己对应的节点需要最小，则被认为是获得了锁。优点：集群可以透明解决单点问题，避免锁不被释放问题，同时锁可以重入。缺点：性能不如缓存方式，吞吐量会随着zk集群规模变大而下降。 《基于Zookeeper的分布式锁》 清楚的原理描述 + Java 代码示例。 《jedisLock—redis分布式锁实现》 基于 setnx(set if ont exists)，有则返回false，否则返回true。并支持过期时间。 《Memcached 和 Redis 分布式锁方案》 利用 memcached 的 add（有别于set）操作，当key存在时，返回false。 分布式一致性算法PAXOS 《分布式系列文章——Paxos算法原理与推导》 《Paxos–&gt;Fast Paxos–&gt;Zookeeper分析》 《【分布式】Zookeeper与Paxos》 Zab 《Zab：Zookeeper 中的分布式一致性协议介绍》 Raft 《Raft 为什么是更易理解的分布式一致性算法》 三种角色：Leader（领袖）、Follower（群众）、Candidate（候选人） 通过随机等待的方式发出投票，得票多的获胜。 Gossip 《Gossip算法》 两阶段提交、多阶段提交 《关于分布式事务、两阶段提交协议、三阶提交协议》 幂等 《分布式系统—幂等性设计》 幂等特性的作用：该资源具备幂等性，请求方无需担心重复调用会产生错误。 常见保证幂等的手段：MVCC（类似于乐观锁）、去重表(唯一索引)、悲观锁、一次性token、序列号方式。 分布式一致方案 《分布式系统事务一致性解决方案》 《保证分布式系统数据一致性的6种方案》 分布式 Leader 节点选举 《利用zookeeper实现分布式leader节点选举》 TCC(Try/Confirm/Cancel) 柔性事务 《传统事务与柔性事务》 基于BASE理论：基本可用、柔性状态、最终一致。 解决方案：记录日志+补偿（正向补充或者回滚）、消息重试(要求程序要幂等)；“无锁设计”、采用乐观锁机制。 分布式文件系统 说说分布式文件存储系统-基本架构 ？ 《各种分布式文件系统的比较》 ？ HDFS：大批量数据读写，用于高吞吐量的场景，不适合小文件。 FastDFS：轻量级、适合小文件。 唯一ID 生成全局唯一ID 《高并发分布式系统中生成全局唯一Id汇总》 Twitter 方案（Snowflake 算法）：41位时间戳+10位机器标识（比如IP，服务器名称等）+12位序列号(本地计数器) Flicker 方案：MySQL自增ID + “REPLACE INTO XXX:SELECT LAST_INSERT_ID();” UUID：缺点，无序，字符串过长，占用空间，影响检索性能。 MongoDB 方案：利用 ObjectId。缺点：不能自增。 《TDDL 在分布式下的SEQUENCE原理》 在数据库中创建 sequence 表，用于记录，当前已被占用的id最大值。 每台客户端主机取一个id区间（比如 1000~2000）缓存在本地，并更新 sequence 表中的id最大值记录。 客户端主机之间取不同的id区间，用完再取，使用乐观锁机制控制并发。 一致性Hash算法 《一致性哈希算法》 设计思想 &amp; 开发模式DDD(Domain-driven Design - 领域驱动设计) 《浅谈我对DDD领域驱动设计的理解》 概念：DDD 主要对传统软件开发流程(分析-设计-编码)中各阶段的割裂问题而提出，避免由于一开始分析不明或在软件开发过程中的信息流转不一致而造成软件无法交付（和需求方设想不一致）的问题。DDD 强调一切以领域（Domain）为中心，强调领域专家（Domain Expert）的作用，强调先定义好领域模型之后在进行开发，并且领域模型可以指导开发（所谓的驱动）。 过程：理解领域、拆分领域、细化领域，模型的准确性取决于模型的理解深度。 设计：DDD 中提出了建模工具，比如聚合、实体、值对象、工厂、仓储、领域服务、领域事件来帮助领域建模。 《领域驱动设计的基础知识总结》 领域（Doamin）本质上就是问题域，比如一个电商系统，一个论坛系统等。 界限上下文（Bounded Context）：阐述子域之间的关系，可以简单理解成一个子系统或组件模块。 领域模型（Domain Model）：DDD的核心是建立（用通用描述语言、工具—领域通用语言）正确的领域模型；反应业务需求的本质，包括实体和过程；其贯穿软件分析、设计、开发 的整个过程；常用表达领域模型的方式：图、代码或文字； 领域通用语言：领域专家、开发设计人员都能立即的语言或工具。 经典分层架构：用户界面/展示层、应用层、领域层、基础设施层，是四层架构模式。 使用的模式： 关联尽量少，尽量单项，尽量降低整体复杂度。 实体（Entity）：领域中的唯一标示，一个实体的属性尽量少，少则清晰。 值对象（Value Object）：没有唯一标识，且属性值不可变，小二简单的对象，比如Date。 领域服务（Domain Service）： 协调多个领域对象，只有方法没有状态(不存数据)；可以分为应用层服务，领域层服务、基础层服务。 聚合及聚合根（Aggregate，Aggregate Root）：聚合定义了一组具有内聚关系的相关对象的集合；聚合根是对聚合引用的唯一元素；当修改一个聚合时，必须在事务级别；大部分领域模型中，有70%的聚合通常只有一个实体，30%只有2~3个实体；如果一个聚合只有一个实体，那么这个实体就是聚合根；如果有多个实体，那么我们可以思考聚合内哪个对象有独立存在的意义并且可以和外部直接进行交互； 工厂（Factory）：类似于设计模式中的工厂模式。 仓储（Repository）：持久化到DB，管理对象，且只对聚合设计仓储。 《领域驱动设计(DDD)实现之路》 聚合：比如一辆汽车（Car）包含了引擎（Engine）、车轮（Wheel）和油箱（Tank）等组件，缺一不可。 《领域驱动设计系列（2）浅析VO、DTO、DO、PO的概念、区别和用处》 命令查询职责分离(CQRS)CQRS — Command Query Responsibility Seperation 《领域驱动设计系列 (六)：CQRS》 核心思想：读写分离（查询和更新在不同的方法中），不同的流程只是不同的设计方式，CQ代码分离，分布式环境中会有明显体现（有冗余数据的情况下），目的是为了高性能。 《DDD CQRS架构和传统架构的优缺点比较》 最终一致的设计理念；依赖于高可用消息中间件。 《CQRS架构简介》 一个实现 CQRS 的抽象案例。 《深度长文：我对CQRS/EventSourcing架构的思考》 CQRS 模式分析 + 12306 抢票案例 贫血，充血模型 《贫血，充血模型的解释以及一些经验》 失血模型：老子和儿子分别定义，相互不知道，二者实体定义中完全没有业务逻辑，通过外部Service进行关联。 贫血模型：老子知道儿子，儿子也知道老子；部分业务逻辑放到实体中；优点：各层单项依赖，结构清楚，易于维护；缺点：不符合OO思想，相比于充血模式，Service层较为厚重； 充血模型：和贫血模型类似，区别在于如何划分业务逻辑。优点：Service层比较薄，只充当Facade的角色，不和DAO打交道、复合OO思想；缺点：非单项依赖，DO和DAO之间双向依赖、和Service层的逻辑划分容易造成混乱。 肿胀模式：是一种极端情况，取消Service层、全部业务逻辑放在DO中；优点：符合OO思想、简化了分层；缺点：暴露信息过多、很多非DO逻辑也会强行并入DO。这种模式应该避免。 作者主张使用贫血模式。 Actor 模式TODO 响应式编程ReactorTODO RxJavaTODO Vert.xTODO DODAF2.0 《DODAF2.0方法论》 《DODAF2.0之能力视角如何落地》 Serverless无需过多关系服务器的服务架构理念。 《什么是Serverless无服务器架构？》 Serverless 不代表出去服务器，而是去除对服务器运行状态的关心。 Serverless 代表一思维方式的转变，从“构建一套服务在一台服务器上，对对个事件进行响应转变为构建一个为服务器，来响应一个事件”。 Serverless 不代表某个具体的框架。 《如何理解Serverless？》 依赖于 Baas （(Mobile) Backend as a Service） 和 Faas （Functions as a service） Service Mesh 《什么是Service Mesh？》 《初识 Service Mesh》 《什么是Service Mesh？》 项目管理架构评审 《架构设计之如何评审架构设计说明书》 《人人都是架构师：非功能性需求》 重构 《架构之重构的12条军规》 代码规范 《阿里巴巴Java开发手册》 代码 Review制度还是制度! 另外，每个公司需要根据自己的需求和目标制定自己的 check list 《为什么你做不好 Code Review？》 代码 review 做的好，在于制度建设。 《从零开始Code Review》 《Code Review Checklist》 《Java Code Review Checklist》 《如何用 gitlab 做 code review》 RUP 《运用RUP 4+1视图方法进行软件架构设计》 看板管理 《说说看板在项目中的应用》 SCRUMSCRUM - 争球 3个角色:Product Owner(PO) 产品负责人;Scrum Master（SM），推动Scrum执行;Team 开发团队。 3个工件：Product Backlog 产品TODOLIST，含优先级;Sprint Backlog 功能开发 TODO LIST；燃尽图； 五个价值观：专注、勇气、公开、承诺、尊重。 《敏捷项目管理流程-Scrum框架最全总结！》 《敏捷其实很简单3—敏捷方法之scrum》 敏捷开发TODO 极限编程（XP）XP - eXtreme Programming 《主流敏捷开发方法：极限编程XP》 是一种指导开发人员的方法论。 4大价值： 沟通：鼓励口头沟通，提高效率。 简单：够用就好。 反馈：及时反馈、通知相关人。 勇气：提倡拥抱变化，敢于重构。 5个原则：快速反馈、简单性假设、逐步修改、提倡更改（小步快跑）、优质工作（保证质量的前提下保证小步快跑）。 5个工作：阶段性冲刺；冲刺计划会议；每日站立会议；冲刺后review；回顾会议。 结对编程边写码，边review。能够增强代码质量、减少bug。 《结对编程》 PDCA 循环质量管理P——PLAN 策划，D——DO 实施，C——CHECK 检查，A——ACT 改进 《PDCA》 FMEA管理模式TODO 通用业务术语TODO 技术趋势TODO 政策、法规TODO 法律严格遵守刑法253法条我国刑法第253条之一规定： 国家机关或者金融、电信、交通、教育、医疗等单位的工作人员，违反国家规定，将本单位在履行职责或者提供服务过程中获得的公民个人信息，出售或者非法提供给他人，情节严重的，处3年以下有期徒刑或者拘役，并处或者单处罚金。 窃取或者以其他方法非法获取上述信息，情节严重的，依照前款的规定处罚。 单位犯前两款罪的，对单位判处罚金，并对其直接负责的主管人员和其他直接责任人员，依照各该款的规定处罚。 最高人民法院、最高人民检察院关于执行《中华人民共和国刑法》确定罪名的补充规定（四）规定：触犯刑法第253条之一第1款之规定，构成“出售、非法提供公民个人信息罪”；触犯刑法第253条之一第2款之规定，构成“非法获取公民个人信息罪” 《非法获取公民个人信息罪》 架构师素质 《架构师画像》 业务理解和抽象能力 NB的代码能力 全面：1. 在面对业务问题上，架构师脑海里是否会浮现出多种技术方案；2. 在做系统设计时是否考虑到了足够多的方方面面；3. 在做系统设计时是否考虑到了足够多的方方面面； 全局：是否考虑到了对上下游的系统的影响。 权衡：权衡投入产出比；优先级和节奏控制； 《关于架构优化和设计，架构师必须知道的事情》 要去考虑的细节：模块化、轻耦合、无共享架构；减少各个组件之前的依赖、注意服务之间依赖所有造成的链式失败及影响等。 基础设施、配置、测试、开发、运维综合考虑。 考虑人、团队、和组织的影响。 《如何才能真正的提高自己，成为一名出色的架构师？》 《架构师的必备素质和成长途径》 素质：业务理解、技术广度、技术深度、丰富经验、沟通能力、动手能力、美学素养。 成长路径：2年积累知识、4年积累技能和组内影响力、7年积累部门内影响力、7年以上积累跨部门影响力。 《架构设计师—你在哪层楼？》 第一层的架构师看到的只是产品本身 第二层的架构师不仅看到自己的产品，还看到了整体的方案 第三层的架构师看到的是商业价值 团队管理TODO 招聘资讯行业资讯 36kr Techweb 公众号列表TODO 博客团队博客 阿里中间件博客 美团点评技术团队博客 个人博客 阮一峰的网络日志 酷壳 - COOLSHELL-陈皓 hellojava-阿里毕玄 Cm’s Blog 程序猿DD-翟永超-《Spring Cloud微服务实战》作者 综合门户、社区国内： CSDN 老牌技术社区、不必解释。 51cto.com ITeye 偏 Java 方向 博客园 ChinaUnix 偏 Linux 方向 开源中国社区 深度开源 伯乐在线 涵盖 IT职场、Web前端、后端、移动端、数据库等方面内容，偏技术端。 ITPUB 腾讯云— 云+社区 阿里云— 云栖社区 IBM DeveloperWorks 开发者头条 LinkedKeeper 国外： DZone Reddit 问答、讨论类社区 segmentfault 问答+专栏 知乎 stackoverflow 行业数据分析 艾瑞网 QUEST MOBILE 国家数据 TalkingData 专项网站 测试: 领测国际 测试窝 TesterHome 运维: * [运维派](http://www.yunweipai.com/) * [Abcdocker](https://www.abcdocker.com/) Java: ImportNew 专注于 Java 技术分享 HowToDoInJava 英文博客 安全 红黑联盟 FreeBuf 大数据 中国大数据 其他专题网站： DockerInfo 专注于 Docker 应用及咨询、教程的网站。 Linux公社 Linux 主题社区 其他类 程序员技能图谱 推荐参考书在线电子书 《深入理解Spring Cloud与微服务构建》 《阿里技术参考图册-研发篇》 《阿里技术参考图册-算法篇》 《2018美团点评技术年货（合辑）》70M InfoQ《架构师》月刊 《架构师之路》 纸质书开发方面 《阿里巴巴Java开发手册》京东 淘宝 架构方面 《软件架构师的12项修炼：技术技能篇》京东 淘宝 《架构之美》京东 淘宝 《分布式服务架构》京东 淘宝 《聊聊架构》 京东 淘宝 《云原生应用架构实践》京东 淘宝 《亿级流量网站架构核心技术》京东 淘宝 《淘宝技术这十年》京东 淘宝 《企业IT架构转型之道-中台战略思想与架构实战》 京东 淘宝 《高可用架构（第1卷）》京东 淘宝 技术管理方面 《CTO说》京东 淘宝 《技术管理之巅》京东 淘宝 《网易一千零一夜：互联网产品项目管理实战》京东 淘宝 基础理论 《数学之美》京东 淘宝 《编程珠玑》京东 淘宝 工具方面TODO 大数据方面技术资源开源资源 github Apache 软件基金会 手册、文档、教程国内： W3Cschool Runoob.com HTML 、 CSS、XML、Java、Python、PHP、设计模式等入门手册。 Love2.io 很多很多中文在线电子书，是一个全新的开源技术文档分享平台。 gitbook.cn 付费电子书。 ApacheCN AI、大数据方面系列中文文档。 国外： Quick Code 免费在线技术教程。 gitbook.com 有部分中文电子书。 Cheatography Cheat Sheets 大全，单页文档网站。 Tutorialspoint 知名教程网站，提供Java、Python、JS、SQL、大数据等高质量入门教程。 在线课堂 学徒无忧 极客时间 segmentfault 斯达克学院 牛客网 极客学院 51CTO学院 会议、活动 QCon ArchSummit GITC全球互联网技术大会 活动发布平台: 活动行 常用APP 极客时间 得到 找工作 Boss直聘 拉勾网 猎聘 100Offer 工具 极客搜索 技术文章搜索引擎。 代码托管 Coding 码云 文件服务 七牛 又拍云 综合云服务商 阿里云 腾讯云 百度云 新浪云 金山云 亚马逊云(AWS) 谷歌云 微软云 VPS Linode]]></content>
      <categories>
        <category>服务器端</category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
</search>
