<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2019%2F06%2F10%2Fassets%2Fconcurrent-Java-a548e92e%2F</url>
    <content type="text"><![CDATA[{"requires":true,"lockfileVersion":1,"dependencies":{"@sindresorhus/is":{"version":"0.14.0","resolved":"https://registry.npmjs.org/@sindresorhus/is/-/is-0.14.0.tgz","integrity":"sha512-9NET910DNaIPngYnLLPeg+Ogzqsi9uM4mSboU5y6p8S5DzMTVEsJZrawi+BoDNUVBa2DhJqQYUFvMDfgU062LQ=="},"@szmarczak/http-timer":{"version":"1.1.2","resolved":"https://registry.npmjs.org/@szmarczak/http-timer/-/http-timer-1.1.2.tgz","integrity":"sha512-XIB2XbzHTN6ieIjfIMV9hlVcfPU26s2vafYWQcZHWXHOxiaRZYEDKEwdl129Zyg50+foYV2jCgtrqSA6qNuNSA==","requires":{"defer-to-connect":"^1.0.1"}},"agent-base":{"version":"4.2.1","resolved":"https://registry.npmjs.org/agent-base/-/agent-base-4.2.1.tgz","integrity":"sha512-JVwXMr9nHYTUXsBFKUqhJwvlcYU/blreOEUkhNR2eXZIvwd+c+o5V4MgDPKWnMS/56awN3TRzIP+KoPn+roQtg==","requires":{"es6-promisify":"^5.0.0"}},"ansi-align":{"version":"3.0.0","resolved":"https://registry.npmjs.org/ansi-align/-/ansi-align-3.0.0.tgz","integrity":"sha512-ZpClVKqXN3RGBmKibdfWzqCY4lnjEuoNzU5T0oEFpfd/z5qJHVarukridD4juLO2FXMiwUQxr9WqQtaYa8XRYw==","requires":{"string-width":"^3.0.0"},"dependencies":{"ansi-regex":{"version":"4.1.0","resolved":"https://registry.npmjs.org/ansi-regex/-/ansi-regex-4.1.0.tgz","integrity":"sha512-1apePfXM1UOSqw0o9IiFAovVz9M5S1Dg+4TrDwfMewQ6p/rmMueb7tWZjQ1rx4Loy1ArBggoqGpfqqdI4rondg=="},"string-width":{"version":"3.1.0","resolved":"https://registry.npmjs.org/string-width/-/string-width-3.1.0.tgz","integrity":"sha512-vafcv6KjVZKSgz06oM/H6GDBrAtz8vdhQakGjFIvNrHA6y3HCF1CInLy+QLq8dTJPQ1b+KDUqDFctkdRW44e1w==","requires":{"emoji-regex":"^7.0.1","is-fullwidth-code-point":"^2.0.0","strip-ansi":"^5.1.0"}},"strip-ansi":{"version":"5.2.0","resolved":"https://registry.npmjs.org/strip-ansi/-/strip-ansi-5.2.0.tgz","integrity":"sha512-DuRs1gKbBqsMKIZlrffwlug8MHkcnpjs5VPmL1PAh+mA30U0DTotfDZ0d2UUsXpPmPmMMJ6W773MaA3J+lbiWA==","requires":{"ansi-regex":"^4.1.0"}}}},"ansi-escapes":{"version":"1.4.0","resolved":"https://registry.npmjs.org/ansi-escapes/-/ansi-escapes-1.4.0.tgz","integrity":"sha1-06ioOzGapneTZisT52HHkRQiMG4="},"ansi-regex":{"version":"2.1.1","resolved":"https://registry.npmjs.org/ansi-regex/-/ansi-regex-2.1.1.tgz","integrity":"sha1-w7M6te42DYbg5ijwRorn7yfWVN8="},"ansi-styles":{"version":"2.2.1","resolved":"https://registry.npmjs.org/ansi-styles/-/ansi-styles-2.2.1.tgz","integrity":"sha1-tDLdM1i2NM914eRmQ2gkBTPB3b4="},"anymatch":{"version":"2.0.0","resolved":"https://registry.npmjs.org/anymatch/-/anymatch-2.0.0.tgz","integrity":"sha512-5teOsQWABXHHBFP9y3skS5P3d/WfWXpv3FUpy+LorMrNYaT9pI4oLMQX7jzQ2KklNpGpWHzdCXTDT2Y3XGlZBw==","requires":{"micromatch":"^3.1.4","normalize-path":"^2.1.1"},"dependencies":{"normalize-path":{"version":"2.1.1","resolved":"https://registry.npmjs.org/normalize-path/-/normalize-path-2.1.1.tgz","integrity":"sha1-GrKLVW4Zg2Oowab35vogE3/mrtk=","requires":{"remove-trailing-separator":"^1.0.1"}}}},"argparse":{"version":"1.0.10","resolved":"https://registry.npmjs.org/argparse/-/argparse-1.0.10.tgz","integrity":"sha512-o5Roy6tNG4SL/FOkCAN6RzjiakZS25RLYFrcMttJqbdd8BWrnA+fGz57iN5Pb06pvBGvl5gQ0B48dJlslXvoTg==","requires":{"sprintf-js":"~1.0.2"}},"arr-diff":{"version":"4.0.0","resolved":"https://registry.npmjs.org/arr-diff/-/arr-diff-4.0.0.tgz","integrity":"sha1-1kYQdP6/7HHn4VI1dhoyml3HxSA="},"arr-flatten":{"version":"1.1.0","resolved":"https://registry.npmjs.org/arr-flatten/-/arr-flatten-1.1.0.tgz","integrity":"sha512-L3hKV5R/p5o81R7O02IGnwpDmkp6E982XhtbuwSe3O4qOtMMMtodicASA1Cny2U+aCXcNpml+m4dPsvsJ3jatg=="},"arr-union":{"version":"3.1.0","resolved":"https://registry.npmjs.org/arr-union/-/arr-union-3.1.0.tgz","integrity":"sha1-45sJrqne+Gao8gbiiK9jkZuuOcQ="},"array-unique":{"version":"0.3.2","resolved":"https://registry.npmjs.org/array-unique/-/array-unique-0.3.2.tgz","integrity":"sha1-qJS3XUvE9s1nnvMkSp/Y9Gri1Cg="},"assign-symbols":{"version":"1.0.0","resolved":"https://registry.npmjs.org/assign-symbols/-/assign-symbols-1.0.0.tgz","integrity":"sha1-WWZ/QfrdTyDMvCu5a41Pf3jsA2c="},"async-each":{"version":"1.0.3","resolved":"https://registry.npmjs.org/async-each/-/async-each-1.0.3.tgz","integrity":"sha512-z/WhQ5FPySLdvREByI2vZiTWwCnF0moMJ1hK9YQwDTHKh6I7/uSckMetoRGb5UBZPC1z0jlw+n/XCgjeH7y1AQ=="},"atob":{"version":"2.1.2","resolved":"https://registry.npmjs.org/atob/-/atob-2.1.2.tgz","integrity":"sha512-Wm6ukoaOGJi/73p/cl2GvLjTI5JM1k/O14isD73YML8StrH/7/lRFgmg8nICZgD3bZZvjwCGxtMOD3wWNAu8cg=="},"babel-polyfill":{"version":"6.23.0","resolved":"https://registry.npmjs.org/babel-polyfill/-/babel-polyfill-6.23.0.tgz","integrity":"sha1-g2TKYt+Or7gwSZ9pkXdGbDsDSZ0=","requires":{"babel-runtime":"^6.22.0","core-js":"^2.4.0","regenerator-runtime":"^0.10.0"}},"babel-runtime":{"version":"6.26.0","resolved":"https://registry.npmjs.org/babel-runtime/-/babel-runtime-6.26.0.tgz","integrity":"sha1-llxwWGaOgrVde/4E/yM3vItWR/4=","requires":{"core-js":"^2.4.0","regenerator-runtime":"^0.11.0"},"dependencies":{"regenerator-runtime":{"version":"0.11.1","resolved":"https://registry.npmjs.org/regenerator-runtime/-/regenerator-runtime-0.11.1.tgz","integrity":"sha512-MguG95oij0fC3QV3URf4V2SDYGJhJnJGqvIIgdECeODCT98wSWDAJ94SSuVpYQUoTcGUIL6L4yNB7j1DFFHSBg=="}}},"balanced-match":{"version":"1.0.0","resolved":"https://registry.npmjs.org/balanced-match/-/balanced-match-1.0.0.tgz","integrity":"sha1-ibTRmasr7kneFk6gK4nORi1xt2c=","optional":true},"base":{"version":"0.11.2","resolved":"https://registry.npmjs.org/base/-/base-0.11.2.tgz","integrity":"sha512-5T6P4xPgpp0YDFvSWwEZ4NoE3aM4QBQXDzmVbraCkFj8zHM+mba8SyqB5DbZWyR7mYHo6Y7BdQo3MoA4m0TeQg==","requires":{"cache-base":"^1.0.1","class-utils":"^0.3.5","component-emitter":"^1.2.1","define-property":"^1.0.0","isobject":"^3.0.1","mixin-deep":"^1.2.0","pascalcase":"^0.1.1"},"dependencies":{"define-property":{"version":"1.0.0","resolved":"https://registry.npmjs.org/define-property/-/define-property-1.0.0.tgz","integrity":"sha1-dp66rz9KY6rTr56NMEybvnm/sOY=","requires":{"is-descriptor":"^1.0.0"}},"is-accessor-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-accessor-descriptor/-/is-accessor-descriptor-1.0.0.tgz","integrity":"sha512-m5hnHTkcVsPfqx3AKlyttIPb7J+XykHvJP2B9bZDjlhLIoEq4XoK64Vg7boZlVWYK6LUY94dYPEE7Lh0ZkZKcQ==","requires":{"kind-of":"^6.0.0"}},"is-data-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-data-descriptor/-/is-data-descriptor-1.0.0.tgz","integrity":"sha512-jbRXy1FmtAoCjQkVmIVYwuuqDFUbaOeDjmed1tOGPrsMhtJA4rD9tkgA0F1qJ3gRFRXcHYVkdeaP50Q5rE/jLQ==","requires":{"kind-of":"^6.0.0"}},"is-descriptor":{"version":"1.0.2","resolved":"https://registry.npmjs.org/is-descriptor/-/is-descriptor-1.0.2.tgz","integrity":"sha512-2eis5WqQGV7peooDyLmNEPUrps9+SXX5c9pL3xEB+4e9HnGuDa7mB7kHxHw4CbqS9k1T2hOH3miL8n8WtiYVtg==","requires":{"is-accessor-descriptor":"^1.0.0","is-data-descriptor":"^1.0.0","kind-of":"^6.0.2"}}}},"binary-extensions":{"version":"1.13.1","resolved":"https://registry.npmjs.org/binary-extensions/-/binary-extensions-1.13.1.tgz","integrity":"sha512-Un7MIEDdUC5gNpcGDV97op1Ywk748MpHcFTHoYs6qnj1Z3j7I53VG3nwZhKzoBZmbdRNnb6WRdFlwl7tSDuZGw=="},"bluebird":{"version":"3.5.5","resolved":"https://registry.npmjs.org/bluebird/-/bluebird-3.5.5.tgz","integrity":"sha512-5am6HnnfN+urzt4yfg7IgTbotDjIT/u8AJpEt0sIU9FtXfVeezXAPKswrG+xKUCOYAINpSdgZVDU6QFh+cuH3w=="},"boxen":{"version":"3.2.0","resolved":"https://registry.npmjs.org/boxen/-/boxen-3.2.0.tgz","integrity":"sha512-cU4J/+NodM3IHdSL2yN8bqYqnmlBTidDR4RC7nJs61ZmtGz8VZzM3HLQX0zY5mrSmPtR3xWwsq2jOUQqFZN8+A==","requires":{"ansi-align":"^3.0.0","camelcase":"^5.3.1","chalk":"^2.4.2","cli-boxes":"^2.2.0","string-width":"^3.0.0","term-size":"^1.2.0","type-fest":"^0.3.0","widest-line":"^2.0.0"},"dependencies":{"ansi-regex":{"version":"4.1.0","resolved":"https://registry.npmjs.org/ansi-regex/-/ansi-regex-4.1.0.tgz","integrity":"sha512-1apePfXM1UOSqw0o9IiFAovVz9M5S1Dg+4TrDwfMewQ6p/rmMueb7tWZjQ1rx4Loy1ArBggoqGpfqqdI4rondg=="},"ansi-styles":{"version":"3.2.1","resolved":"https://registry.npmjs.org/ansi-styles/-/ansi-styles-3.2.1.tgz","integrity":"sha512-VT0ZI6kZRdTh8YyJw3SMbYm/u+NqfsAxEpWO0Pf9sq8/e94WxxOpPKx9FR1FlyCtOVDNOQ+8ntlqFxiRc+r5qA==","requires":{"color-convert":"^1.9.0"}},"chalk":{"version":"2.4.2","resolved":"https://registry.npmjs.org/chalk/-/chalk-2.4.2.tgz","integrity":"sha512-Mti+f9lpJNcwF4tWV8/OrTTtF1gZi+f8FqlyAdouralcFWFQWF2+NgCHShjkCb+IFBLq9buZwE1xckQU4peSuQ==","requires":{"ansi-styles":"^3.2.1","escape-string-regexp":"^1.0.5","supports-color":"^5.3.0"}},"string-width":{"version":"3.1.0","resolved":"https://registry.npmjs.org/string-width/-/string-width-3.1.0.tgz","integrity":"sha512-vafcv6KjVZKSgz06oM/H6GDBrAtz8vdhQakGjFIvNrHA6y3HCF1CInLy+QLq8dTJPQ1b+KDUqDFctkdRW44e1w==","requires":{"emoji-regex":"^7.0.1","is-fullwidth-code-point":"^2.0.0","strip-ansi":"^5.1.0"}},"strip-ansi":{"version":"5.2.0","resolved":"https://registry.npmjs.org/strip-ansi/-/strip-ansi-5.2.0.tgz","integrity":"sha512-DuRs1gKbBqsMKIZlrffwlug8MHkcnpjs5VPmL1PAh+mA30U0DTotfDZ0d2UUsXpPmPmMMJ6W773MaA3J+lbiWA==","requires":{"ansi-regex":"^4.1.0"}},"supports-color":{"version":"5.5.0","resolved":"https://registry.npmjs.org/supports-color/-/supports-color-5.5.0.tgz","integrity":"sha512-QjVjwdXIt408MIiAqCX4oUKsgU2EqAGzs2Ppkm4aQYbjm+ZEWEcW4SfFNTr4uMNZma0ey4f5lgLrkB0aX0QMow==","requires":{"has-flag":"^3.0.0"}}}},"brace-expansion":{"version":"1.1.11","resolved":"https://registry.npmjs.org/brace-expansion/-/brace-expansion-1.1.11.tgz","integrity":"sha512-iCuPHDFgrHX7H2vEI/5xpz07zSHB00TpugqhmYtVmMO6518mCuRMoOYFldEBl0g187ufozdaHgWKcYFb61qGiA==","optional":true,"requires":{"balanced-match":"^1.0.0","concat-map":"0.0.1"}},"braces":{"version":"2.3.2","resolved":"https://registry.npmjs.org/braces/-/braces-2.3.2.tgz","integrity":"sha512-aNdbnj9P8PjdXU4ybaWLK2IF3jc/EoDYbC7AazW6to3TRsfXxscC9UXOB5iDiEQrkyIbWp2SLQda4+QAa7nc3w==","requires":{"arr-flatten":"^1.1.0","array-unique":"^0.3.2","extend-shallow":"^2.0.1","fill-range":"^4.0.0","isobject":"^3.0.1","repeat-element":"^1.1.2","snapdragon":"^0.8.1","snapdragon-node":"^2.0.1","split-string":"^3.0.2","to-regex":"^3.0.1"},"dependencies":{"extend-shallow":{"version":"2.0.1","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz","integrity":"sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=","requires":{"is-extendable":"^0.1.0"}}}},"cache-base":{"version":"1.0.1","resolved":"https://registry.npmjs.org/cache-base/-/cache-base-1.0.1.tgz","integrity":"sha512-AKcdTnFSWATd5/GCPRxr2ChwIJ85CeyrEyjRHlKxQ56d4XJMGym0uAiKn0xbLOGOl3+yRpOTi484dVCEc5AUzQ==","requires":{"collection-visit":"^1.0.0","component-emitter":"^1.2.1","get-value":"^2.0.6","has-value":"^1.0.0","isobject":"^3.0.1","set-value":"^2.0.0","to-object-path":"^0.3.0","union-value":"^1.0.0","unset-value":"^1.0.0"}},"cacheable-request":{"version":"6.0.0","resolved":"https://registry.npmjs.org/cacheable-request/-/cacheable-request-6.0.0.tgz","integrity":"sha512-2N7AmszH/WPPpl5Z3XMw1HAP+8d+xugnKQAeKvxFZ/04dbT/CAznqwbl+7eSr3HkwdepNwtb2yx3CAMQWvG01Q==","requires":{"clone-response":"^1.0.2","get-stream":"^4.0.0","http-cache-semantics":"^4.0.0","keyv":"^3.0.0","lowercase-keys":"^1.0.1","normalize-url":"^3.1.0","responselike":"^1.0.2"}},"camelcase":{"version":"5.3.1","resolved":"https://registry.npmjs.org/camelcase/-/camelcase-5.3.1.tgz","integrity":"sha512-L28STB170nwWS63UjtlEOE3dldQApaJXZkOI1uMFfzf3rRuPegHaHesyee+YxQ+W6SvRDQV6UrdOdRiR153wJg=="},"chalk":{"version":"1.1.3","resolved":"https://registry.npmjs.org/chalk/-/chalk-1.1.3.tgz","integrity":"sha1-qBFcVeSnAv5NFQq9OHKCKn4J/Jg=","requires":{"ansi-styles":"^2.2.1","escape-string-regexp":"^1.0.2","has-ansi":"^2.0.0","strip-ansi":"^3.0.0","supports-color":"^2.0.0"}},"chardet":{"version":"0.4.2","resolved":"https://registry.npmjs.org/chardet/-/chardet-0.4.2.tgz","integrity":"sha1-tUc7M9yXxCTl2Y3IfVXU2KKci/I="},"chokidar":{"version":"2.1.6","resolved":"https://registry.npmjs.org/chokidar/-/chokidar-2.1.6.tgz","integrity":"sha512-V2jUo67OKkc6ySiRpJrjlpJKl9kDuG+Xb8VgsGzb+aEouhgS1D0weyPU4lEzdAcsCAvrih2J2BqyXqHWvVLw5g==","requires":{"anymatch":"^2.0.0","async-each":"^1.0.1","braces":"^2.3.2","fsevents":"^1.2.7","glob-parent":"^3.1.0","inherits":"^2.0.3","is-binary-path":"^1.0.0","is-glob":"^4.0.0","normalize-path":"^3.0.0","path-is-absolute":"^1.0.0","readdirp":"^2.2.1","upath":"^1.1.1"}},"ci-info":{"version":"2.0.0","resolved":"https://registry.npmjs.org/ci-info/-/ci-info-2.0.0.tgz","integrity":"sha512-5tK7EtrZ0N+OLFMthtqOj4fI2Jeb88C4CAZPu25LDVUgXJ0A3Js4PMGqrn0JU1W0Mh1/Z8wZzYPxqUrXeBboCQ=="},"cint":{"version":"8.2.1","resolved":"https://registry.npmjs.org/cint/-/cint-8.2.1.tgz","integrity":"sha1-cDhrG0jidz0NYxZqVa/5TvRFahI="},"class-utils":{"version":"0.3.6","resolved":"https://registry.npmjs.org/class-utils/-/class-utils-0.3.6.tgz","integrity":"sha512-qOhPa/Fj7s6TY8H8esGu5QNpMMQxz79h+urzrNYN6mn+9BnxlDGf5QZ+XeCDsxSjPqsSR56XOZOJmpeurnLMeg==","requires":{"arr-union":"^3.1.0","define-property":"^0.2.5","isobject":"^3.0.0","static-extend":"^0.1.1"},"dependencies":{"define-property":{"version":"0.2.5","resolved":"https://registry.npmjs.org/define-property/-/define-property-0.2.5.tgz","integrity":"sha1-w1se+RjsPJkPmlvFe+BKrOxcgRY=","requires":{"is-descriptor":"^0.1.0"}}}},"cli-boxes":{"version":"2.2.0","resolved":"https://registry.npmjs.org/cli-boxes/-/cli-boxes-2.2.0.tgz","integrity":"sha512-gpaBrMAizVEANOpfZp/EEUixTXDyGt7DFzdK5hU+UbWt/J0lB0w20ncZj59Z9a93xHb9u12zF5BS6i9RKbtg4w=="},"cli-cursor":{"version":"2.1.0","resolved":"https://registry.npmjs.org/cli-cursor/-/cli-cursor-2.1.0.tgz","integrity":"sha1-s12sN2R5+sw+lHR9QdDQ9SOP/LU=","requires":{"restore-cursor":"^2.0.0"}},"cli-table":{"version":"0.3.1","resolved":"https://registry.npmjs.org/cli-table/-/cli-table-0.3.1.tgz","integrity":"sha1-9TsFJmqLGguTSz0IIebi3FkUriM=","requires":{"colors":"1.0.3"},"dependencies":{"colors":{"version":"1.0.3","resolved":"https://registry.npmjs.org/colors/-/colors-1.0.3.tgz","integrity":"sha1-BDP0TYCWgP3rYO0mDxsMJi6CpAs="}}},"cli-width":{"version":"2.2.0","resolved":"https://registry.npmjs.org/cli-width/-/cli-width-2.2.0.tgz","integrity":"sha1-/xnt6Kml5XkyQUewwR8PvLq+1jk="},"clone-response":{"version":"1.0.2","resolved":"https://registry.npmjs.org/clone-response/-/clone-response-1.0.2.tgz","integrity":"sha1-0dyXOSAxTfZ/vrlCI7TuNQI56Ws=","requires":{"mimic-response":"^1.0.0"}},"collection-visit":{"version":"1.0.0","resolved":"https://registry.npmjs.org/collection-visit/-/collection-visit-1.0.0.tgz","integrity":"sha1-S8A3PBZLwykbTTaMgpzxqApZ3KA=","requires":{"map-visit":"^1.0.0","object-visit":"^1.0.0"}},"color-convert":{"version":"1.9.3","resolved":"https://registry.npmjs.org/color-convert/-/color-convert-1.9.3.tgz","integrity":"sha512-QfAUtd+vFdAtFQcC8CCyYt1fYWxSqAiK2cSD6zDB8N3cpsEBAvRxp9zOGg6G/SHHJYAT88/az/IuDGALsNVbGg==","requires":{"color-name":"1.1.3"}},"color-name":{"version":"1.1.3","resolved":"https://registry.npmjs.org/color-name/-/color-name-1.1.3.tgz","integrity":"sha1-p9BVi9icQveV3UIyj3QIMcpTvCU="},"colors":{"version":"1.3.3","resolved":"https://registry.npmjs.org/colors/-/colors-1.3.3.tgz","integrity":"sha512-mmGt/1pZqYRjMxB1axhTo16/snVZ5krrKkcmMeVKxzECMMXoCgnvTPp10QgHfcbQZw8Dq2jMNG6je4JlWU0gWg=="},"commander":{"version":"2.20.0","resolved":"https://registry.npmjs.org/commander/-/commander-2.20.0.tgz","integrity":"sha512-7j2y+40w61zy6YC2iRNpUe/NwhNyoXrYpHMrSunaMG64nRnaf96zO/KMQR4OyN/UnE5KLyEBnKHd4aG3rskjpQ=="},"component-emitter":{"version":"1.3.0","resolved":"https://registry.npmjs.org/component-emitter/-/component-emitter-1.3.0.tgz","integrity":"sha512-Rd3se6QB+sO1TwqZjscQrurpEPIfO0/yYnSin6Q/rD3mOutHvUrCAhJub3r90uNb+SESBuE0QYoB90YdfatsRg=="},"concat-map":{"version":"0.0.1","resolved":"https://registry.npmjs.org/concat-map/-/concat-map-0.0.1.tgz","integrity":"sha1-2Klr13/Wjfd5OnMDajug1UBdR3s=","optional":true},"config-chain":{"version":"1.1.12","resolved":"https://registry.npmjs.org/config-chain/-/config-chain-1.1.12.tgz","integrity":"sha512-a1eOIcu8+7lUInge4Rpf/n4Krkf3Dd9lqhljRzII1/Zno/kRtUWnznPO3jOKBmTEktkt3fkxisUcivoj0ebzoA==","requires":{"ini":"^1.3.4","proto-list":"~1.2.1"}},"configstore":{"version":"4.0.0","resolved":"https://registry.npmjs.org/configstore/-/configstore-4.0.0.tgz","integrity":"sha512-CmquAXFBocrzaSM8mtGPMM/HiWmyIpr4CcJl/rgY2uCObZ/S7cKU0silxslqJejl+t/T9HS8E0PUNQD81JGUEQ==","requires":{"dot-prop":"^4.1.0","graceful-fs":"^4.1.2","make-dir":"^1.0.0","unique-string":"^1.0.0","write-file-atomic":"^2.0.0","xdg-basedir":"^3.0.0"}},"copy-descriptor":{"version":"0.1.1","resolved":"https://registry.npmjs.org/copy-descriptor/-/copy-descriptor-0.1.1.tgz","integrity":"sha1-Z29us8OZl8LuGsOpJP1hJHSPV40="},"core-js":{"version":"2.6.9","resolved":"https://registry.npmjs.org/core-js/-/core-js-2.6.9.tgz","integrity":"sha512-HOpZf6eXmnl7la+cUdMnLvUxKNqLUzJvgIziQ0DiF3JwSImNphIqdGqzj6hIKyX04MmV0poclQ7+wjWvxQyR2A=="},"core-util-is":{"version":"1.0.2","resolved":"https://registry.npmjs.org/core-util-is/-/core-util-is-1.0.2.tgz","integrity":"sha1-tf1UIgqivFq1eqtxQMlAdUUDwac="},"cross-spawn":{"version":"5.1.0","resolved":"https://registry.npmjs.org/cross-spawn/-/cross-spawn-5.1.0.tgz","integrity":"sha1-6L0O/uWPz/b4+UUQoKVUu/ojVEk=","requires":{"lru-cache":"^4.0.1","shebang-command":"^1.2.0","which":"^1.2.9"}},"crypto-random-string":{"version":"1.0.0","resolved":"https://registry.npmjs.org/crypto-random-string/-/crypto-random-string-1.0.0.tgz","integrity":"sha1-ojD2T1aDEOFJgAmUB5DsmVRbyn4="},"debug":{"version":"2.6.9","resolved":"https://registry.npmjs.org/debug/-/debug-2.6.9.tgz","integrity":"sha512-bC7ElrdJaJnPbAP+1EotYvqZsb3ecl5wi6Bfi6BJTUcNowp6cvspg0jXznRTKDjm/E7AdgFBVeAPVMNcKGsHMA==","requires":{"ms":"2.0.0"}},"decode-uri-component":{"version":"0.2.0","resolved":"https://registry.npmjs.org/decode-uri-component/-/decode-uri-component-0.2.0.tgz","integrity":"sha1-6zkTMzRYd1y4TNGh+uBiEGu4dUU="},"decompress-response":{"version":"3.3.0","resolved":"https://registry.npmjs.org/decompress-response/-/decompress-response-3.3.0.tgz","integrity":"sha1-gKTdMjdIOEv6JICDYirt7Jgq3/M=","requires":{"mimic-response":"^1.0.0"}},"deep-extend":{"version":"0.6.0","resolved":"https://registry.npmjs.org/deep-extend/-/deep-extend-0.6.0.tgz","integrity":"sha512-LOHxIOaPYdHlJRtCQfDIVZtfw/ufM8+rVj649RIHzcm/vGwQRXFt6OPqIFWsm2XEMrNIEtWR64sY1LEKD2vAOA=="},"defer-to-connect":{"version":"1.0.2","resolved":"https://registry.npmjs.org/defer-to-connect/-/defer-to-connect-1.0.2.tgz","integrity":"sha512-k09hcQcTDY+cwgiwa6PYKLm3jlagNzQ+RSvhjzESOGOx+MNOuXkxTfEvPrO1IOQ81tArCFYQgi631clB70RpQw=="},"define-property":{"version":"2.0.2","resolved":"https://registry.npmjs.org/define-property/-/define-property-2.0.2.tgz","integrity":"sha512-jwK2UV4cnPpbcG7+VRARKTZPUWowwXA8bzH5NP6ud0oeAxyYPuGZUAC7hMugpCdz4BeSZl2Dl9k66CHJ/46ZYQ==","requires":{"is-descriptor":"^1.0.2","isobject":"^3.0.1"},"dependencies":{"is-accessor-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-accessor-descriptor/-/is-accessor-descriptor-1.0.0.tgz","integrity":"sha512-m5hnHTkcVsPfqx3AKlyttIPb7J+XykHvJP2B9bZDjlhLIoEq4XoK64Vg7boZlVWYK6LUY94dYPEE7Lh0ZkZKcQ==","requires":{"kind-of":"^6.0.0"}},"is-data-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-data-descriptor/-/is-data-descriptor-1.0.0.tgz","integrity":"sha512-jbRXy1FmtAoCjQkVmIVYwuuqDFUbaOeDjmed1tOGPrsMhtJA4rD9tkgA0F1qJ3gRFRXcHYVkdeaP50Q5rE/jLQ==","requires":{"kind-of":"^6.0.0"}},"is-descriptor":{"version":"1.0.2","resolved":"https://registry.npmjs.org/is-descriptor/-/is-descriptor-1.0.2.tgz","integrity":"sha512-2eis5WqQGV7peooDyLmNEPUrps9+SXX5c9pL3xEB+4e9HnGuDa7mB7kHxHw4CbqS9k1T2hOH3miL8n8WtiYVtg==","requires":{"is-accessor-descriptor":"^1.0.0","is-data-descriptor":"^1.0.0","kind-of":"^6.0.2"}}}},"dot-prop":{"version":"4.2.0","resolved":"https://registry.npmjs.org/dot-prop/-/dot-prop-4.2.0.tgz","integrity":"sha512-tUMXrxlExSW6U2EXiiKGSBVdYgtV8qlHL+C10TsW4PURY/ic+eaysnSkwB4kA/mBlCyy/IKDJ+Lc3wbWeaXtuQ==","requires":{"is-obj":"^1.0.0"}},"duplexer3":{"version":"0.1.4","resolved":"https://registry.npmjs.org/duplexer3/-/duplexer3-0.1.4.tgz","integrity":"sha1-7gHdHKwO08vH/b6jfcCo8c4ALOI="},"emoji-regex":{"version":"7.0.3","resolved":"https://registry.npmjs.org/emoji-regex/-/emoji-regex-7.0.3.tgz","integrity":"sha512-CwBLREIQ7LvYFB0WyRvwhq5N5qPhc6PMjD6bYggFlI5YyDgl+0vxq5VHbMOFqLg7hfWzmu8T5Z1QofhmTIhItA=="},"encoding":{"version":"0.1.12","resolved":"https://registry.npmjs.org/encoding/-/encoding-0.1.12.tgz","integrity":"sha1-U4tm8+5izRq1HsMjgp0flIDHS+s=","requires":{"iconv-lite":"~0.4.13"}},"end-of-stream":{"version":"1.4.1","resolved":"https://registry.npmjs.org/end-of-stream/-/end-of-stream-1.4.1.tgz","integrity":"sha512-1MkrZNvWTKCaigbn+W15elq2BB/L22nqrSY5DKlo3X6+vclJm8Bb5djXJBmEX6fS3+zCh/F4VBK5Z2KxJt4s2Q==","requires":{"once":"^1.4.0"}},"es6-promise":{"version":"4.2.6","resolved":"https://registry.npmjs.org/es6-promise/-/es6-promise-4.2.6.tgz","integrity":"sha512-aRVgGdnmW2OiySVPUC9e6m+plolMAJKjZnQlCwNSuK5yQ0JN61DZSO1X1Ufd1foqWRAlig0rhduTCHe7sVtK5Q=="},"es6-promisify":{"version":"5.0.0","resolved":"https://registry.npmjs.org/es6-promisify/-/es6-promisify-5.0.0.tgz","integrity":"sha1-UQnWLz5W6pZ8S2NQWu8IKRyKUgM=","requires":{"es6-promise":"^4.0.3"}},"escape-string-regexp":{"version":"1.0.5","resolved":"https://registry.npmjs.org/escape-string-regexp/-/escape-string-regexp-1.0.5.tgz","integrity":"sha1-G2HAViGQqN/2rjuyzwIAyhMLhtQ="},"esprima":{"version":"4.0.1","resolved":"https://registry.npmjs.org/esprima/-/esprima-4.0.1.tgz","integrity":"sha512-eGuFFw7Upda+g4p+QHvnW0RyTX/SVeJBDM/gCtMARO0cLuT2HcEKnTPvhjV6aGeqrCB/sbNop0Kszm0jsaWU4A=="},"execa":{"version":"0.7.0","resolved":"https://registry.npmjs.org/execa/-/execa-0.7.0.tgz","integrity":"sha1-lEvs00zEHuMqY6n68nrVpl/Fl3c=","requires":{"cross-spawn":"^5.0.1","get-stream":"^3.0.0","is-stream":"^1.1.0","npm-run-path":"^2.0.0","p-finally":"^1.0.0","signal-exit":"^3.0.0","strip-eof":"^1.0.0"},"dependencies":{"get-stream":{"version":"3.0.0","resolved":"https://registry.npmjs.org/get-stream/-/get-stream-3.0.0.tgz","integrity":"sha1-jpQ9E1jcN1VQVOy+LtsFqhdO3hQ="}}},"expand-brackets":{"version":"2.1.4","resolved":"https://registry.npmjs.org/expand-brackets/-/expand-brackets-2.1.4.tgz","integrity":"sha1-t3c14xXOMPa27/D4OwQVGiJEliI=","requires":{"debug":"^2.3.3","define-property":"^0.2.5","extend-shallow":"^2.0.1","posix-character-classes":"^0.1.0","regex-not":"^1.0.0","snapdragon":"^0.8.1","to-regex":"^3.0.1"},"dependencies":{"define-property":{"version":"0.2.5","resolved":"https://registry.npmjs.org/define-property/-/define-property-0.2.5.tgz","integrity":"sha1-w1se+RjsPJkPmlvFe+BKrOxcgRY=","requires":{"is-descriptor":"^0.1.0"}},"extend-shallow":{"version":"2.0.1","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz","integrity":"sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=","requires":{"is-extendable":"^0.1.0"}}}},"extend-shallow":{"version":"3.0.2","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-3.0.2.tgz","integrity":"sha1-Jqcarwc7OfshJxcnRhMcJwQCjbg=","requires":{"assign-symbols":"^1.0.0","is-extendable":"^1.0.1"},"dependencies":{"is-extendable":{"version":"1.0.1","resolved":"https://registry.npmjs.org/is-extendable/-/is-extendable-1.0.1.tgz","integrity":"sha512-arnXMxT1hhoKo9k1LZdmlNyJdDDfy2v0fXjFlmok4+i8ul/6WlbVge9bhM74OpNPQPMGUToDtz+KXa1PneJxOA==","requires":{"is-plain-object":"^2.0.4"}}}},"external-editor":{"version":"2.2.0","resolved":"https://registry.npmjs.org/external-editor/-/external-editor-2.2.0.tgz","integrity":"sha512-bSn6gvGxKt+b7+6TKEv1ZycHleA7aHhRHyAqJyp5pbUFuYYNIzpZnQDk7AsYckyWdEnTeAnay0aCy2aV6iTk9A==","requires":{"chardet":"^0.4.0","iconv-lite":"^0.4.17","tmp":"^0.0.33"}},"extglob":{"version":"2.0.4","resolved":"https://registry.npmjs.org/extglob/-/extglob-2.0.4.tgz","integrity":"sha512-Nmb6QXkELsuBr24CJSkilo6UHHgbekK5UiZgfE6UHD3Eb27YC6oD+bhcT+tJ6cl8dmsgdQxnWlcry8ksBIBLpw==","requires":{"array-unique":"^0.3.2","define-property":"^1.0.0","expand-brackets":"^2.1.4","extend-shallow":"^2.0.1","fragment-cache":"^0.2.1","regex-not":"^1.0.0","snapdragon":"^0.8.1","to-regex":"^3.0.1"},"dependencies":{"define-property":{"version":"1.0.0","resolved":"https://registry.npmjs.org/define-property/-/define-property-1.0.0.tgz","integrity":"sha1-dp66rz9KY6rTr56NMEybvnm/sOY=","requires":{"is-descriptor":"^1.0.0"}},"extend-shallow":{"version":"2.0.1","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz","integrity":"sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=","requires":{"is-extendable":"^0.1.0"}},"is-accessor-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-accessor-descriptor/-/is-accessor-descriptor-1.0.0.tgz","integrity":"sha512-m5hnHTkcVsPfqx3AKlyttIPb7J+XykHvJP2B9bZDjlhLIoEq4XoK64Vg7boZlVWYK6LUY94dYPEE7Lh0ZkZKcQ==","requires":{"kind-of":"^6.0.0"}},"is-data-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-data-descriptor/-/is-data-descriptor-1.0.0.tgz","integrity":"sha512-jbRXy1FmtAoCjQkVmIVYwuuqDFUbaOeDjmed1tOGPrsMhtJA4rD9tkgA0F1qJ3gRFRXcHYVkdeaP50Q5rE/jLQ==","requires":{"kind-of":"^6.0.0"}},"is-descriptor":{"version":"1.0.2","resolved":"https://registry.npmjs.org/is-descriptor/-/is-descriptor-1.0.2.tgz","integrity":"sha512-2eis5WqQGV7peooDyLmNEPUrps9+SXX5c9pL3xEB+4e9HnGuDa7mB7kHxHw4CbqS9k1T2hOH3miL8n8WtiYVtg==","requires":{"is-accessor-descriptor":"^1.0.0","is-data-descriptor":"^1.0.0","kind-of":"^6.0.2"}}}},"fast-diff":{"version":"1.2.0","resolved":"https://registry.npmjs.org/fast-diff/-/fast-diff-1.2.0.tgz","integrity":"sha512-xJuoT5+L99XlZ8twedaRf6Ax2TgQVxvgZOYoPKqZufmJib0tL2tegPBOZb1pVNgIhlqDlA0eO0c3wBvQcmzx4w=="},"figures":{"version":"2.0.0","resolved":"https://registry.npmjs.org/figures/-/figures-2.0.0.tgz","integrity":"sha1-OrGi0qYsi/tDGgyUy3l6L84nyWI=","requires":{"escape-string-regexp":"^1.0.5"}},"fill-range":{"version":"4.0.0","resolved":"https://registry.npmjs.org/fill-range/-/fill-range-4.0.0.tgz","integrity":"sha1-1USBHUKPmOsGpj3EAtJAPDKMOPc=","requires":{"extend-shallow":"^2.0.1","is-number":"^3.0.0","repeat-string":"^1.6.1","to-regex-range":"^2.1.0"},"dependencies":{"extend-shallow":{"version":"2.0.1","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz","integrity":"sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=","requires":{"is-extendable":"^0.1.0"}}}},"find-up":{"version":"4.0.0","resolved":"https://registry.npmjs.org/find-up/-/find-up-4.0.0.tgz","integrity":"sha512-zoH7ZWPkRdgwYCDVoQTzqjG8JSPANhtvLhh4KVUHyKnaUJJrNeFmWIkTcNuJmR3GLMEmGYEf2S2bjgx26JTF+Q==","requires":{"locate-path":"^5.0.0"}},"for-in":{"version":"1.0.2","resolved":"https://registry.npmjs.org/for-in/-/for-in-1.0.2.tgz","integrity":"sha1-gQaNKVqBQuwKxybG4iAMMPttXoA="},"fragment-cache":{"version":"0.2.1","resolved":"https://registry.npmjs.org/fragment-cache/-/fragment-cache-0.2.1.tgz","integrity":"sha1-QpD60n8T6Jvn8zeZxrxaCr//DRk=","requires":{"map-cache":"^0.2.2"}},"fsevents":{"version":"1.2.9","resolved":"https://registry.npmjs.org/fsevents/-/fsevents-1.2.9.tgz","integrity":"sha512-oeyj2H3EjjonWcFjD5NvZNE9Rqe4UW+nQBU2HNeKw0koVLEFIhtyETyAakeAM3de7Z/SW5kcA+fZUait9EApnw==","optional":true,"requires":{"nan":"^2.12.1","node-pre-gyp":"^0.12.0"},"dependencies":{"abbrev":{"version":"1.1.1","bundled":true,"optional":true},"ansi-regex":{"version":"2.1.1","bundled":true,"optional":true},"aproba":{"version":"1.2.0","bundled":true,"optional":true},"are-we-there-yet":{"version":"1.1.5","bundled":true,"optional":true,"requires":{"delegates":"^1.0.0","readable-stream":"^2.0.6"}},"balanced-match":{"version":"1.0.0","bundled":true,"optional":true},"brace-expansion":{"version":"1.1.11","bundled":true,"optional":true,"requires":{"balanced-match":"^1.0.0","concat-map":"0.0.1"}},"chownr":{"version":"1.1.1","bundled":true,"optional":true},"code-point-at":{"version":"1.1.0","bundled":true,"optional":true},"concat-map":{"version":"0.0.1","bundled":true,"optional":true},"console-control-strings":{"version":"1.1.0","bundled":true,"optional":true},"core-util-is":{"version":"1.0.2","bundled":true,"optional":true},"debug":{"version":"4.1.1","bundled":true,"optional":true,"requires":{"ms":"^2.1.1"}},"deep-extend":{"version":"0.6.0","bundled":true,"optional":true},"delegates":{"version":"1.0.0","bundled":true,"optional":true},"detect-libc":{"version":"1.0.3","bundled":true,"optional":true},"fs-minipass":{"version":"1.2.5","bundled":true,"optional":true,"requires":{"minipass":"^2.2.1"}},"fs.realpath":{"version":"1.0.0","bundled":true,"optional":true},"gauge":{"version":"2.7.4","bundled":true,"optional":true,"requires":{"aproba":"^1.0.3","console-control-strings":"^1.0.0","has-unicode":"^2.0.0","object-assign":"^4.1.0","signal-exit":"^3.0.0","string-width":"^1.0.1","strip-ansi":"^3.0.1","wide-align":"^1.1.0"}},"glob":{"version":"7.1.3","bundled":true,"optional":true,"requires":{"fs.realpath":"^1.0.0","inflight":"^1.0.4","inherits":"2","minimatch":"^3.0.4","once":"^1.3.0","path-is-absolute":"^1.0.0"}},"has-unicode":{"version":"2.0.1","bundled":true,"optional":true},"iconv-lite":{"version":"0.4.24","bundled":true,"optional":true,"requires":{"safer-buffer":">= 2.1.2 < 3"}},"ignore-walk":{"version":"3.0.1","bundled":true,"optional":true,"requires":{"minimatch":"^3.0.4"}},"inflight":{"version":"1.0.6","bundled":true,"optional":true,"requires":{"once":"^1.3.0","wrappy":"1"}},"inherits":{"version":"2.0.3","bundled":true,"optional":true},"ini":{"version":"1.3.5","bundled":true,"optional":true},"is-fullwidth-code-point":{"version":"1.0.0","bundled":true,"optional":true,"requires":{"number-is-nan":"^1.0.0"}},"isarray":{"version":"1.0.0","bundled":true,"optional":true},"minimatch":{"version":"3.0.4","bundled":true,"optional":true,"requires":{"brace-expansion":"^1.1.7"}},"minimist":{"version":"0.0.8","bundled":true,"optional":true},"minipass":{"version":"2.3.5","bundled":true,"optional":true,"requires":{"safe-buffer":"^5.1.2","yallist":"^3.0.0"}},"minizlib":{"version":"1.2.1","bundled":true,"optional":true,"requires":{"minipass":"^2.2.1"}},"mkdirp":{"version":"0.5.1","bundled":true,"optional":true,"requires":{"minimist":"0.0.8"}},"ms":{"version":"2.1.1","bundled":true,"optional":true},"needle":{"version":"2.3.0","bundled":true,"optional":true,"requires":{"debug":"^4.1.0","iconv-lite":"^0.4.4","sax":"^1.2.4"}},"node-pre-gyp":{"version":"0.12.0","bundled":true,"optional":true,"requires":{"detect-libc":"^1.0.2","mkdirp":"^0.5.1","needle":"^2.2.1","nopt":"^4.0.1","npm-packlist":"^1.1.6","npmlog":"^4.0.2","rc":"^1.2.7","rimraf":"^2.6.1","semver":"^5.3.0","tar":"^4"}},"nopt":{"version":"4.0.1","bundled":true,"optional":true,"requires":{"abbrev":"1","osenv":"^0.1.4"}},"npm-bundled":{"version":"1.0.6","bundled":true,"optional":true},"npm-packlist":{"version":"1.4.1","bundled":true,"optional":true,"requires":{"ignore-walk":"^3.0.1","npm-bundled":"^1.0.1"}},"npmlog":{"version":"4.1.2","bundled":true,"optional":true,"requires":{"are-we-there-yet":"~1.1.2","console-control-strings":"~1.1.0","gauge":"~2.7.3","set-blocking":"~2.0.0"}},"number-is-nan":{"version":"1.0.1","bundled":true,"optional":true},"object-assign":{"version":"4.1.1","bundled":true,"optional":true},"once":{"version":"1.4.0","bundled":true,"optional":true,"requires":{"wrappy":"1"}},"os-homedir":{"version":"1.0.2","bundled":true,"optional":true},"os-tmpdir":{"version":"1.0.2","bundled":true,"optional":true},"osenv":{"version":"0.1.5","bundled":true,"optional":true,"requires":{"os-homedir":"^1.0.0","os-tmpdir":"^1.0.0"}},"path-is-absolute":{"version":"1.0.1","bundled":true,"optional":true},"process-nextick-args":{"version":"2.0.0","bundled":true,"optional":true},"rc":{"version":"1.2.8","bundled":true,"optional":true,"requires":{"deep-extend":"^0.6.0","ini":"~1.3.0","minimist":"^1.2.0","strip-json-comments":"~2.0.1"},"dependencies":{"minimist":{"version":"1.2.0","bundled":true,"optional":true}}},"readable-stream":{"version":"2.3.6","bundled":true,"optional":true,"requires":{"core-util-is":"~1.0.0","inherits":"~2.0.3","isarray":"~1.0.0","process-nextick-args":"~2.0.0","safe-buffer":"~5.1.1","string_decoder":"~1.1.1","util-deprecate":"~1.0.1"}},"rimraf":{"version":"2.6.3","bundled":true,"optional":true,"requires":{"glob":"^7.1.3"}},"safe-buffer":{"version":"5.1.2","bundled":true,"optional":true},"safer-buffer":{"version":"2.1.2","bundled":true,"optional":true},"sax":{"version":"1.2.4","bundled":true,"optional":true},"semver":{"version":"5.7.0","bundled":true,"optional":true},"set-blocking":{"version":"2.0.0","bundled":true,"optional":true},"signal-exit":{"version":"3.0.2","bundled":true,"optional":true},"string-width":{"version":"1.0.2","bundled":true,"optional":true,"requires":{"code-point-at":"^1.0.0","is-fullwidth-code-point":"^1.0.0","strip-ansi":"^3.0.0"}},"string_decoder":{"version":"1.1.1","bundled":true,"optional":true,"requires":{"safe-buffer":"~5.1.0"}},"strip-ansi":{"version":"3.0.1","bundled":true,"optional":true,"requires":{"ansi-regex":"^2.0.0"}},"strip-json-comments":{"version":"2.0.1","bundled":true,"optional":true},"tar":{"version":"4.4.8","bundled":true,"optional":true,"requires":{"chownr":"^1.1.1","fs-minipass":"^1.2.5","minipass":"^2.3.4","minizlib":"^1.1.1","mkdirp":"^0.5.0","safe-buffer":"^5.1.2","yallist":"^3.0.2"}},"util-deprecate":{"version":"1.0.2","bundled":true,"optional":true},"wide-align":{"version":"1.1.3","bundled":true,"optional":true,"requires":{"string-width":"^1.0.2 || 2"}},"wrappy":{"version":"1.0.2","bundled":true,"optional":true},"yallist":{"version":"3.0.3","bundled":true,"optional":true}}},"get-proxy":{"version":"2.1.0","resolved":"https://registry.npmjs.org/get-proxy/-/get-proxy-2.1.0.tgz","integrity":"sha512-zmZIaQTWnNQb4R4fJUEp/FC51eZsc6EkErspy3xtIYStaq8EB/hDIWipxsal+E8rz0qD7f2sL/NA9Xee4RInJw==","requires":{"npm-conf":"^1.1.0"}},"get-stdin":{"version":"7.0.0","resolved":"https://registry.npmjs.org/get-stdin/-/get-stdin-7.0.0.tgz","integrity":"sha512-zRKcywvrXlXsA0v0i9Io4KDRaAw7+a1ZpjRwl9Wox8PFlVCCHra7E9c4kqXCoCM9nR5tBkaTTZRBoCm60bFqTQ=="},"get-stream":{"version":"4.1.0","resolved":"https://registry.npmjs.org/get-stream/-/get-stream-4.1.0.tgz","integrity":"sha512-GMat4EJ5161kIy2HevLlr4luNjBgvmj413KaQA7jt4V8B4RDsfpHk7WQ9GVqfYyyx8OS/L66Kox+rJRNklLK7w==","requires":{"pump":"^3.0.0"}},"get-value":{"version":"2.0.6","resolved":"https://registry.npmjs.org/get-value/-/get-value-2.0.6.tgz","integrity":"sha1-3BXKHGcjh8p2vTesCjlbogQqLCg="},"glob":{"version":"6.0.4","resolved":"https://registry.npmjs.org/glob/-/glob-6.0.4.tgz","integrity":"sha1-DwiGD2oVUSey+t1PnOJLGqtuTSI=","optional":true,"requires":{"inflight":"^1.0.4","inherits":"2","minimatch":"2 || 3","once":"^1.3.0","path-is-absolute":"^1.0.0"}},"glob-parent":{"version":"3.1.0","resolved":"https://registry.npmjs.org/glob-parent/-/glob-parent-3.1.0.tgz","integrity":"sha1-nmr2KZ2NO9K9QEMIMr0RPfkGxa4=","requires":{"is-glob":"^3.1.0","path-dirname":"^1.0.0"},"dependencies":{"is-glob":{"version":"3.1.0","resolved":"https://registry.npmjs.org/is-glob/-/is-glob-3.1.0.tgz","integrity":"sha1-e6WuJCF4BKxwcHuWkiVnSGzD6Eo=","requires":{"is-extglob":"^2.1.0"}}}},"global-dirs":{"version":"0.1.1","resolved":"https://registry.npmjs.org/global-dirs/-/global-dirs-0.1.1.tgz","integrity":"sha1-sxnA3UYH81PzvpzKTHL8FIxJ9EU=","requires":{"ini":"^1.3.4"}},"got":{"version":"9.6.0","resolved":"https://registry.npmjs.org/got/-/got-9.6.0.tgz","integrity":"sha512-R7eWptXuGYxwijs0eV+v3o6+XH1IqVK8dJOEecQfTmkncw9AV4dcw/Dhxi8MdlqPthxxpZyizMzyg8RTmEsG+Q==","requires":{"@sindresorhus/is":"^0.14.0","@szmarczak/http-timer":"^1.1.2","cacheable-request":"^6.0.0","decompress-response":"^3.3.0","duplexer3":"^0.1.4","get-stream":"^4.1.0","lowercase-keys":"^1.0.1","mimic-response":"^1.0.1","p-cancelable":"^1.0.0","to-readable-stream":"^1.0.0","url-parse-lax":"^3.0.0"}},"graceful-fs":{"version":"4.1.15","resolved":"https://registry.npmjs.org/graceful-fs/-/graceful-fs-4.1.15.tgz","integrity":"sha512-6uHUhOPEBgQ24HM+r6b/QwWfZq+yiFcipKFrOFiBEnWdy5sdzYoi+pJeQaPI5qOLRFqWmAXUPQNsielzdLoecA=="},"has-ansi":{"version":"2.0.0","resolved":"https://registry.npmjs.org/has-ansi/-/has-ansi-2.0.0.tgz","integrity":"sha1-NPUEnOHs3ysGSa8+8k5F7TVBbZE=","requires":{"ansi-regex":"^2.0.0"}},"has-flag":{"version":"3.0.0","resolved":"https://registry.npmjs.org/has-flag/-/has-flag-3.0.0.tgz","integrity":"sha1-tdRU3CGZriJWmfNGfloH87lVuv0="},"has-value":{"version":"1.0.0","resolved":"https://registry.npmjs.org/has-value/-/has-value-1.0.0.tgz","integrity":"sha1-GLKB2lhbHFxR3vJMkw7SmgvmsXc=","requires":{"get-value":"^2.0.6","has-values":"^1.0.0","isobject":"^3.0.0"}},"has-values":{"version":"1.0.0","resolved":"https://registry.npmjs.org/has-values/-/has-values-1.0.0.tgz","integrity":"sha1-lbC2P+whRmGab+V/51Yo1aOe/k8=","requires":{"is-number":"^3.0.0","kind-of":"^4.0.0"},"dependencies":{"kind-of":{"version":"4.0.0","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-4.0.0.tgz","integrity":"sha1-IIE989cSkosgc3hpGkUGb65y3Vc=","requires":{"is-buffer":"^1.1.5"}}}},"has-yarn":{"version":"2.1.0","resolved":"https://registry.npmjs.org/has-yarn/-/has-yarn-2.1.0.tgz","integrity":"sha512-UqBRqi4ju7T+TqGNdqAO0PaSVGsDGJUBQvk9eUWNGRY1CFGDzYhLWoM7JQEemnlvVcv/YEmc2wNW8BC24EnUsw=="},"hexo-bunyan":{"version":"1.0.0","resolved":"https://registry.npmjs.org/hexo-bunyan/-/hexo-bunyan-1.0.0.tgz","integrity":"sha512-RymT8Ck+K77mLt9BEYNb4uyfC7RIQnU5N3laXowMrS28jj2h89VHJCOnhV00mmta4fHRqNa07kP1Hrn17nvMkQ==","requires":{"moment":"^2.10.6","mv":"~2","safe-json-stringify":"~1"}},"hexo-fs":{"version":"1.0.2","resolved":"https://registry.npmjs.org/hexo-fs/-/hexo-fs-1.0.2.tgz","integrity":"sha512-cbDnYuk6IndW/Fr2RcfZsZXE5wlG6tFoeBgZsHY230sSYalvX4JBPOUrE8As7Agysl+NGMthtr/Drtuliy5foQ==","requires":{"bluebird":"^3.5.1","chokidar":"^2.0.4","escape-string-regexp":"^1.0.5","graceful-fs":"^4.1.11"}},"hexo-helper-live2d":{"version":"3.1.1","resolved":"https://registry.npmjs.org/hexo-helper-live2d/-/hexo-helper-live2d-3.1.1.tgz","integrity":"sha512-YqWAyCjB91kLBYJnmYJZtecKvfw+ph0cIZfycjZeJ4rYvF4d4Z+C5TVVW2SPstOEbNvwK0PpimUhi6r3hT5z7g==","requires":{"colors":"^1.3.3","hexo-fs":"^1.0.2","hexo-log":"^0.2.0","live2d-widget":"^3.1.3","lodash":"^4.17.11","npm-check-updates":"^3.1.3","opencollective":"^1.0.3","opencollective-postinstall":"^2.0.2","path":"^0.12.7"}},"hexo-log":{"version":"0.2.0","resolved":"https://registry.npmjs.org/hexo-log/-/hexo-log-0.2.0.tgz","integrity":"sha512-fzoc+GQexxPPILTjoOQILnA3ZG2MFgqMBVel4xvJ11pXptw9+f97ynTgDAExXafyp9Nz2ChXRuqlCYgPtZSlxQ==","requires":{"chalk":"^1.1.1","hexo-bunyan":"^1.0.0"}},"http-cache-semantics":{"version":"4.0.3","resolved":"https://registry.npmjs.org/http-cache-semantics/-/http-cache-semantics-4.0.3.tgz","integrity":"sha512-TcIMG3qeVLgDr1TEd2XvHaTnMPwYQUQMIBLy+5pLSDKYFc7UIqj39w8EGzZkaxoLv/l2K8HaI0t5AVA+YYgUew=="},"https-proxy-agent":{"version":"2.2.1","resolved":"https://registry.npmjs.org/https-proxy-agent/-/https-proxy-agent-2.2.1.tgz","integrity":"sha512-HPCTS1LW51bcyMYbxUIOO4HEOlQ1/1qRaFWcyxvwaqUS9TY88aoEuHUY33kuAh1YhVVaDQhLZsnPd+XNARWZlQ==","requires":{"agent-base":"^4.1.0","debug":"^3.1.0"},"dependencies":{"debug":{"version":"3.2.6","resolved":"https://registry.npmjs.org/debug/-/debug-3.2.6.tgz","integrity":"sha512-mel+jf7nrtEl5Pn1Qx46zARXKDpBbvzezse7p7LqINmdoIk8PYP5SySaxEmYv6TZ0JyEKA1hsCId6DIhgITtWQ==","requires":{"ms":"^2.1.1"}},"ms":{"version":"2.1.1","resolved":"https://registry.npmjs.org/ms/-/ms-2.1.1.tgz","integrity":"sha512-tgp+dl5cGk28utYktBsrFqA7HKgrhgPsg6Z/EfhWI4gl1Hwq8B/GmY/0oXZ6nF8hDVesS/FpnYaD/kOWhYQvyg=="}}},"iconv-lite":{"version":"0.4.24","resolved":"https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.4.24.tgz","integrity":"sha512-v3MXnZAcvnywkTUEZomIActle7RXXeedOR31wwl7VlyoXO4Qi9arvSenNQWne1TcRwhCL1HwLI21bEqdpj8/rA==","requires":{"safer-buffer":">= 2.1.2 < 3"}},"import-lazy":{"version":"2.1.0","resolved":"https://registry.npmjs.org/import-lazy/-/import-lazy-2.1.0.tgz","integrity":"sha1-BWmOPUXIjo1+nZLLBYTnfwlvPkM="},"imurmurhash":{"version":"0.1.4","resolved":"https://registry.npmjs.org/imurmurhash/-/imurmurhash-0.1.4.tgz","integrity":"sha1-khi5srkoojixPcT7a21XbyMUU+o="},"inflight":{"version":"1.0.6","resolved":"https://registry.npmjs.org/inflight/-/inflight-1.0.6.tgz","integrity":"sha1-Sb1jMdfQLQwJvJEKEHW6gWW1bfk=","optional":true,"requires":{"once":"^1.3.0","wrappy":"1"}},"inherits":{"version":"2.0.3","resolved":"https://registry.npmjs.org/inherits/-/inherits-2.0.3.tgz","integrity":"sha1-Yzwsg+PaQqUC9SRmAiSA9CCCYd4="},"ini":{"version":"1.3.5","resolved":"https://registry.npmjs.org/ini/-/ini-1.3.5.tgz","integrity":"sha512-RZY5huIKCMRWDUqZlEi72f/lmXKMvuszcMBduliQ3nnWbx9X/ZBQO7DijMEYS9EhHBb2qacRUMtC7svLwe0lcw=="},"inquirer":{"version":"3.0.6","resolved":"https://registry.npmjs.org/inquirer/-/inquirer-3.0.6.tgz","integrity":"sha1-4EqqnQW3o8ubD0B9BDdfBEcZA0c=","requires":{"ansi-escapes":"^1.1.0","chalk":"^1.0.0","cli-cursor":"^2.1.0","cli-width":"^2.0.0","external-editor":"^2.0.1","figures":"^2.0.0","lodash":"^4.3.0","mute-stream":"0.0.7","run-async":"^2.2.0","rx":"^4.1.0","string-width":"^2.0.0","strip-ansi":"^3.0.0","through":"^2.3.6"}},"is-accessor-descriptor":{"version":"0.1.6","resolved":"https://registry.npmjs.org/is-accessor-descriptor/-/is-accessor-descriptor-0.1.6.tgz","integrity":"sha1-qeEss66Nh2cn7u84Q/igiXtcmNY=","requires":{"kind-of":"^3.0.2"},"dependencies":{"kind-of":{"version":"3.2.2","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz","integrity":"sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=","requires":{"is-buffer":"^1.1.5"}}}},"is-binary-path":{"version":"1.0.1","resolved":"https://registry.npmjs.org/is-binary-path/-/is-binary-path-1.0.1.tgz","integrity":"sha1-dfFmQrSA8YenEcgUFh/TpKdlWJg=","requires":{"binary-extensions":"^1.0.0"}},"is-buffer":{"version":"1.1.6","resolved":"https://registry.npmjs.org/is-buffer/-/is-buffer-1.1.6.tgz","integrity":"sha512-NcdALwpXkTm5Zvvbk7owOUSvVvBKDgKP5/ewfXEznmQFfs4ZRmanOeKBTjRVjka3QFoN6XJ+9F3USqfHqTaU5w=="},"is-ci":{"version":"2.0.0","resolved":"https://registry.npmjs.org/is-ci/-/is-ci-2.0.0.tgz","integrity":"sha512-YfJT7rkpQB0updsdHLGWrvhBJfcfzNNawYDNIyQXJz0IViGf75O8EBPKSdvw2rF+LGCsX4FZ8tcr3b19LcZq4w==","requires":{"ci-info":"^2.0.0"}},"is-data-descriptor":{"version":"0.1.4","resolved":"https://registry.npmjs.org/is-data-descriptor/-/is-data-descriptor-0.1.4.tgz","integrity":"sha1-C17mSDiOLIYCgueT8YVv7D8wG1Y=","requires":{"kind-of":"^3.0.2"},"dependencies":{"kind-of":{"version":"3.2.2","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz","integrity":"sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=","requires":{"is-buffer":"^1.1.5"}}}},"is-descriptor":{"version":"0.1.6","resolved":"https://registry.npmjs.org/is-descriptor/-/is-descriptor-0.1.6.tgz","integrity":"sha512-avDYr0SB3DwO9zsMov0gKCESFYqCnE4hq/4z3TdUlukEy5t9C0YRq7HLrsN52NAcqXKaepeCD0n+B0arnVG3Hg==","requires":{"is-accessor-descriptor":"^0.1.6","is-data-descriptor":"^0.1.4","kind-of":"^5.0.0"},"dependencies":{"kind-of":{"version":"5.1.0","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-5.1.0.tgz","integrity":"sha512-NGEErnH6F2vUuXDh+OlbcKW7/wOcfdRHaZ7VWtqCztfHri/++YKmP51OdWeGPuqCOba6kk2OTe5d02VmTB80Pw=="}}},"is-extendable":{"version":"0.1.1","resolved":"https://registry.npmjs.org/is-extendable/-/is-extendable-0.1.1.tgz","integrity":"sha1-YrEQ4omkcUGOPsNqYX1HLjAd/Ik="},"is-extglob":{"version":"2.1.1","resolved":"https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz","integrity":"sha1-qIwCU1eR8C7TfHahueqXc8gz+MI="},"is-fullwidth-code-point":{"version":"2.0.0","resolved":"https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-2.0.0.tgz","integrity":"sha1-o7MKXE8ZkYMWeqq5O+764937ZU8="},"is-glob":{"version":"4.0.1","resolved":"https://registry.npmjs.org/is-glob/-/is-glob-4.0.1.tgz","integrity":"sha512-5G0tKtBTFImOqDnLB2hG6Bp2qcKEFduo4tZu9MT/H6NQv/ghhy30o55ufafxJ/LdH79LLs2Kfrn85TLKyA7BUg==","requires":{"is-extglob":"^2.1.1"}},"is-installed-globally":{"version":"0.1.0","resolved":"https://registry.npmjs.org/is-installed-globally/-/is-installed-globally-0.1.0.tgz","integrity":"sha1-Df2Y9akRFxbdU13aZJL2e/PSWoA=","requires":{"global-dirs":"^0.1.0","is-path-inside":"^1.0.0"}},"is-npm":{"version":"3.0.0","resolved":"https://registry.npmjs.org/is-npm/-/is-npm-3.0.0.tgz","integrity":"sha512-wsigDr1Kkschp2opC4G3yA6r9EgVA6NjRpWzIi9axXqeIaAATPRJc4uLujXe3Nd9uO8KoDyA4MD6aZSeXTADhA=="},"is-number":{"version":"3.0.0","resolved":"https://registry.npmjs.org/is-number/-/is-number-3.0.0.tgz","integrity":"sha1-JP1iAaR4LPUFYcgQJ2r8fRLXEZU=","requires":{"kind-of":"^3.0.2"},"dependencies":{"kind-of":{"version":"3.2.2","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz","integrity":"sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=","requires":{"is-buffer":"^1.1.5"}}}},"is-obj":{"version":"1.0.1","resolved":"https://registry.npmjs.org/is-obj/-/is-obj-1.0.1.tgz","integrity":"sha1-PkcprB9f3gJc19g6iW2rn09n2w8="},"is-path-inside":{"version":"1.0.1","resolved":"https://registry.npmjs.org/is-path-inside/-/is-path-inside-1.0.1.tgz","integrity":"sha1-jvW33lBDej/cprToZe96pVy0gDY=","requires":{"path-is-inside":"^1.0.1"}},"is-plain-object":{"version":"2.0.4","resolved":"https://registry.npmjs.org/is-plain-object/-/is-plain-object-2.0.4.tgz","integrity":"sha512-h5PpgXkWitc38BBMYawTYMWJHFZJVnBquFE57xFpjB8pJFiF6gZ+bU+WyI/yqXiFR5mdLsgYNaPe8uao6Uv9Og==","requires":{"isobject":"^3.0.1"}},"is-promise":{"version":"2.1.0","resolved":"https://registry.npmjs.org/is-promise/-/is-promise-2.1.0.tgz","integrity":"sha1-eaKp7OfwlugPNtKy87wWwf9L8/o="},"is-stream":{"version":"1.1.0","resolved":"https://registry.npmjs.org/is-stream/-/is-stream-1.1.0.tgz","integrity":"sha1-EtSj3U5o4Lec6428hBc66A2RykQ="},"is-windows":{"version":"1.0.2","resolved":"https://registry.npmjs.org/is-windows/-/is-windows-1.0.2.tgz","integrity":"sha512-eXK1UInq2bPmjyX6e3VHIzMLobc4J94i4AWn+Hpq3OU5KkrRC96OAcR3PRJ/pGu6m8TRnBHP9dkXQVsT/COVIA=="},"is-yarn-global":{"version":"0.3.0","resolved":"https://registry.npmjs.org/is-yarn-global/-/is-yarn-global-0.3.0.tgz","integrity":"sha512-VjSeb/lHmkoyd8ryPVIKvOCn4D1koMqY+vqyjjUfc3xyKtP4dYOxM44sZrnqQSzSds3xyOrUTLTC9LVCVgLngw=="},"isarray":{"version":"1.0.0","resolved":"https://registry.npmjs.org/isarray/-/isarray-1.0.0.tgz","integrity":"sha1-u5NdSFgsuhaMBoNJV6VKPgcSTxE="},"isexe":{"version":"2.0.0","resolved":"https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz","integrity":"sha1-6PvzdNxVb/iUehDcsFctYz8s+hA="},"isobject":{"version":"3.0.1","resolved":"https://registry.npmjs.org/isobject/-/isobject-3.0.1.tgz","integrity":"sha1-TkMekrEalzFjaqH5yNHMvP2reN8="},"jju":{"version":"1.4.0","resolved":"https://registry.npmjs.org/jju/-/jju-1.4.0.tgz","integrity":"sha1-o6vicYryQaKykE+EpiWXDzia4yo="},"js-yaml":{"version":"3.13.1","resolved":"https://registry.npmjs.org/js-yaml/-/js-yaml-3.13.1.tgz","integrity":"sha512-YfbcO7jXDdyj0DGxYVSlSeQNHbD7XPWvrVWeVUujrQEoZzWJIRrCPoyk6kL6IAjAG2IolMK4T0hNUe0HOUs5Jw==","requires":{"argparse":"^1.0.7","esprima":"^4.0.0"}},"json-buffer":{"version":"3.0.0","resolved":"https://registry.npmjs.org/json-buffer/-/json-buffer-3.0.0.tgz","integrity":"sha1-Wx85evx11ne96Lz8Dkfh+aPZqJg="},"json-parse-helpfulerror":{"version":"1.0.3","resolved":"https://registry.npmjs.org/json-parse-helpfulerror/-/json-parse-helpfulerror-1.0.3.tgz","integrity":"sha1-E/FM4C7tTpgSl7ZOueO5MuLdE9w=","requires":{"jju":"^1.1.0"}},"json5":{"version":"1.0.1","resolved":"https://registry.npmjs.org/json5/-/json5-1.0.1.tgz","integrity":"sha512-aKS4WQjPenRxiQsC93MNfjx+nbF4PAdYzmd/1JIj8HYzqfbu86beTuNgXDzPknWk0n0uARlyewZo4s++ES36Ow==","requires":{"minimist":"^1.2.0"},"dependencies":{"minimist":{"version":"1.2.0","resolved":"https://registry.npmjs.org/minimist/-/minimist-1.2.0.tgz","integrity":"sha1-o1AIsg9BOD7sH7kU9M1d95omQoQ="}}},"keyv":{"version":"3.1.0","resolved":"https://registry.npmjs.org/keyv/-/keyv-3.1.0.tgz","integrity":"sha512-9ykJ/46SN/9KPM/sichzQ7OvXyGDYKGTaDlKMGCAlg2UK8KRy4jb0d8sFc+0Tt0YYnThq8X2RZgCg74RPxgcVA==","requires":{"json-buffer":"3.0.0"}},"kind-of":{"version":"6.0.2","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-6.0.2.tgz","integrity":"sha512-s5kLOcnH0XqDO+FvuaLX8DDjZ18CGFk7VygH40QoKPUQhW4e2rvM0rwUq0t8IQDOwYSeLK01U90OjzBTme2QqA=="},"kleur":{"version":"3.0.3","resolved":"https://registry.npmjs.org/kleur/-/kleur-3.0.3.tgz","integrity":"sha512-eTIzlVOSUR+JxdDFepEYcBMtZ9Qqdef+rnzWdRZuMbOywu5tO2w2N7rqjoANZ5k9vywhL6Br1VRjUIgTQx4E8w=="},"latest-version":{"version":"5.1.0","resolved":"https://registry.npmjs.org/latest-version/-/latest-version-5.1.0.tgz","integrity":"sha512-weT+r0kTkRQdCdYCNtkMwWXQTMEswKrFBkm4ckQOMVhhqhIMI1UT2hMj+1iigIhgSZm5gTmrRXBNoGUgaTY1xA==","requires":{"package-json":"^6.3.0"}},"live2d-widget":{"version":"3.1.4","resolved":"https://registry.npmjs.org/live2d-widget/-/live2d-widget-3.1.4.tgz","integrity":"sha512-KseUqwiGZLb1SArr+lDaXl8AjXSFI/x/Z+BbDIyPHwElGAktCvhFYw/SDkrI4LL5bnVuGMLKMg6crMTa23KGDQ==","requires":{"opencollective":"^1.0.3","opencollective-postinstall":"^2.0.1"}},"locate-path":{"version":"5.0.0","resolved":"https://registry.npmjs.org/locate-path/-/locate-path-5.0.0.tgz","integrity":"sha512-t7hw9pI+WvuwNJXwk5zVHpyhIqzg2qTlklJOf0mVxGSbe3Fp2VieZcduNYjaLDoy6p9uGpQEGWG87WpMKlNq8g==","requires":{"p-locate":"^4.1.0"}},"lodash":{"version":"4.17.11","resolved":"https://registry.npmjs.org/lodash/-/lodash-4.17.11.tgz","integrity":"sha512-cQKh8igo5QUhZ7lg38DYWAxMvjSAKG0A8wGSVimP07SIUEK2UO+arSRKbRZWtelMtN5V0Hkwh5ryOto/SshYIg=="},"lowercase-keys":{"version":"1.0.1","resolved":"https://registry.npmjs.org/lowercase-keys/-/lowercase-keys-1.0.1.tgz","integrity":"sha512-G2Lj61tXDnVFFOi8VZds+SoQjtQC3dgokKdDG2mTm1tx4m50NUHBOZSBwQQHyy0V12A0JTG4icfZQH+xPyh8VA=="},"lru-cache":{"version":"4.1.5","resolved":"https://registry.npmjs.org/lru-cache/-/lru-cache-4.1.5.tgz","integrity":"sha512-sWZlbEP2OsHNkXrMl5GYk/jKk70MBng6UU4YI/qGDYbgf6YbP4EvmqISbXCoJiRKs+1bSpFHVgQxvJ17F2li5g==","requires":{"pseudomap":"^1.0.2","yallist":"^2.1.2"}},"make-dir":{"version":"1.3.0","resolved":"https://registry.npmjs.org/make-dir/-/make-dir-1.3.0.tgz","integrity":"sha512-2w31R7SJtieJJnQtGc7RVL2StM2vGYVfqUOvUDxH6bC6aJTxPxTF0GnIgCyu7tjockiUWAYQRbxa7vKn34s5sQ==","requires":{"pify":"^3.0.0"}},"map-cache":{"version":"0.2.2","resolved":"https://registry.npmjs.org/map-cache/-/map-cache-0.2.2.tgz","integrity":"sha1-wyq9C9ZSXZsFFkW7TyasXcmKDb8="},"map-visit":{"version":"1.0.0","resolved":"https://registry.npmjs.org/map-visit/-/map-visit-1.0.0.tgz","integrity":"sha1-7Nyo8TFE5mDxtb1B8S80edmN+48=","requires":{"object-visit":"^1.0.0"}},"micromatch":{"version":"3.1.10","resolved":"https://registry.npmjs.org/micromatch/-/micromatch-3.1.10.tgz","integrity":"sha512-MWikgl9n9M3w+bpsY3He8L+w9eF9338xRl8IAO5viDizwSzziFEyUzo2xrrloB64ADbTf8uA8vRqqttDTOmccg==","requires":{"arr-diff":"^4.0.0","array-unique":"^0.3.2","braces":"^2.3.1","define-property":"^2.0.2","extend-shallow":"^3.0.2","extglob":"^2.0.4","fragment-cache":"^0.2.1","kind-of":"^6.0.2","nanomatch":"^1.2.9","object.pick":"^1.3.0","regex-not":"^1.0.0","snapdragon":"^0.8.1","to-regex":"^3.0.2"}},"mimic-fn":{"version":"1.2.0","resolved":"https://registry.npmjs.org/mimic-fn/-/mimic-fn-1.2.0.tgz","integrity":"sha512-jf84uxzwiuiIVKiOLpfYk7N46TSy8ubTonmneY9vrpHNAnp0QBt2BxWV9dO3/j+BoVAb+a5G6YDPW3M5HOdMWQ=="},"mimic-response":{"version":"1.0.1","resolved":"https://registry.npmjs.org/mimic-response/-/mimic-response-1.0.1.tgz","integrity":"sha512-j5EctnkH7amfV/q5Hgmoal1g2QHFJRraOtmx0JpIqkxhBhI/lJSl1nMpQ45hVarwNETOoWEimndZ4QK0RHxuxQ=="},"minimatch":{"version":"3.0.4","resolved":"https://registry.npmjs.org/minimatch/-/minimatch-3.0.4.tgz","integrity":"sha512-yJHVQEhyqPLUTgt9B83PXu6W3rx4MvvHvSUvToogpwoGDOUQ+yDrR0HRot+yOCdCO7u4hX3pWft6kWBBcqh0UA==","optional":true,"requires":{"brace-expansion":"^1.1.7"}},"minimist":{"version":"0.0.8","resolved":"https://registry.npmjs.org/minimist/-/minimist-0.0.8.tgz","integrity":"sha1-hX/Kv8M5fSYluCKCYuhqp6ARsF0=","optional":true},"mixin-deep":{"version":"1.3.1","resolved":"https://registry.npmjs.org/mixin-deep/-/mixin-deep-1.3.1.tgz","integrity":"sha512-8ZItLHeEgaqEvd5lYBXfm4EZSFCX29Jb9K+lAHhDKzReKBQKj3R+7NOF6tjqYi9t4oI8VUfaWITJQm86wnXGNQ==","requires":{"for-in":"^1.0.2","is-extendable":"^1.0.1"},"dependencies":{"is-extendable":{"version":"1.0.1","resolved":"https://registry.npmjs.org/is-extendable/-/is-extendable-1.0.1.tgz","integrity":"sha512-arnXMxT1hhoKo9k1LZdmlNyJdDDfy2v0fXjFlmok4+i8ul/6WlbVge9bhM74OpNPQPMGUToDtz+KXa1PneJxOA==","requires":{"is-plain-object":"^2.0.4"}}}},"mkdirp":{"version":"0.5.1","resolved":"https://registry.npmjs.org/mkdirp/-/mkdirp-0.5.1.tgz","integrity":"sha1-MAV0OOrGz3+MR2fzhkjWaX11yQM=","optional":true,"requires":{"minimist":"0.0.8"}},"moment":{"version":"2.24.0","resolved":"https://registry.npmjs.org/moment/-/moment-2.24.0.tgz","integrity":"sha512-bV7f+6l2QigeBBZSM/6yTNq4P2fNpSWj/0e7jQcy87A8e7o2nAfP/34/2ky5Vw4B9S446EtIhodAzkFCcR4dQg==","optional":true},"ms":{"version":"2.0.0","resolved":"https://registry.npmjs.org/ms/-/ms-2.0.0.tgz","integrity":"sha1-VgiurfwAvmwpAd9fmGF4jeDVl8g="},"mute-stream":{"version":"0.0.7","resolved":"https://registry.npmjs.org/mute-stream/-/mute-stream-0.0.7.tgz","integrity":"sha1-MHXOk7whuPq0PhvE2n6BFe0ee6s="},"mv":{"version":"2.1.1","resolved":"https://registry.npmjs.org/mv/-/mv-2.1.1.tgz","integrity":"sha1-rmzg1vbV4KT32JN5jQPB6pVZtqI=","optional":true,"requires":{"mkdirp":"~0.5.1","ncp":"~2.0.0","rimraf":"~2.4.0"}},"nan":{"version":"2.14.0","resolved":"https://registry.npmjs.org/nan/-/nan-2.14.0.tgz","integrity":"sha512-INOFj37C7k3AfaNTtX8RhsTw7qRy7eLET14cROi9+5HAVbbHuIWUHEauBv5qT4Av2tWasiTY1Jw6puUNqRJXQg==","optional":true},"nanomatch":{"version":"1.2.13","resolved":"https://registry.npmjs.org/nanomatch/-/nanomatch-1.2.13.tgz","integrity":"sha512-fpoe2T0RbHwBTBUOftAfBPaDEi06ufaUai0mE6Yn1kacc3SnTErfb/h+X94VXzI64rKFHYImXSvdwGGCmwOqCA==","requires":{"arr-diff":"^4.0.0","array-unique":"^0.3.2","define-property":"^2.0.2","extend-shallow":"^3.0.2","fragment-cache":"^0.2.1","is-windows":"^1.0.2","kind-of":"^6.0.2","object.pick":"^1.3.0","regex-not":"^1.0.0","snapdragon":"^0.8.1","to-regex":"^3.0.1"}},"ncp":{"version":"2.0.0","resolved":"https://registry.npmjs.org/ncp/-/ncp-2.0.0.tgz","integrity":"sha1-GVoh1sRuNh0vsSgbo4uR6d9727M=","optional":true},"nested-error-stacks":{"version":"2.0.1","resolved":"https://registry.npmjs.org/nested-error-stacks/-/nested-error-stacks-2.0.1.tgz","integrity":"sha512-SrQrok4CATudVzBS7coSz26QRSmlK9TzzoFbeKfcPBUFPjcQM9Rqvr/DlJkOrwI/0KcgvMub1n1g5Jt9EgRn4A=="},"node-alias":{"version":"1.0.4","resolved":"https://registry.npmjs.org/node-alias/-/node-alias-1.0.4.tgz","integrity":"sha1-HxuRa1a56iQcATX5fO1pQPVW8pI=","requires":{"chalk":"^1.1.1","lodash":"^4.2.0"}},"node-fetch":{"version":"1.6.3","resolved":"https://registry.npmjs.org/node-fetch/-/node-fetch-1.6.3.tgz","integrity":"sha1-3CNO3WSJmC1Y6PDbT2lQKavNjAQ=","requires":{"encoding":"^0.1.11","is-stream":"^1.0.1"}},"normalize-path":{"version":"3.0.0","resolved":"https://registry.npmjs.org/normalize-path/-/normalize-path-3.0.0.tgz","integrity":"sha512-6eZs5Ls3WtCisHWp9S2GUy8dqkpGi4BVSz3GaqiE6ezub0512ESztXUwUB6C6IKbQkY2Pnb/mD4WYojCRwcwLA=="},"normalize-url":{"version":"3.3.0","resolved":"https://registry.npmjs.org/normalize-url/-/normalize-url-3.3.0.tgz","integrity":"sha512-U+JJi7duF1o+u2pynbp2zXDW2/PADgC30f0GsHZtRh+HOcXHnw137TrNlyxxRvWW5fjKd3bcLHPxofWuCjaeZg=="},"npm-check-updates":{"version":"3.1.10","resolved":"https://registry.npmjs.org/npm-check-updates/-/npm-check-updates-3.1.10.tgz","integrity":"sha512-qs4aYg3IbsHbzVyG/4bapgkuflkHj2yAR36ES7fSvDKs3NBotkAWpiaQ9JUbXgqSZwLuvusxBaZgpYlM3fMZSw==","requires":{"bluebird":"^3.5.5","chalk":"^2.4.2","cint":"^8.2.1","cli-table":"^0.3.1","commander":"^2.20.0","fast-diff":"^1.2.0","find-up":"4.0.0","get-proxy":"^2.1.0","get-stdin":"^7.0.0","https-proxy-agent":"^2.2.1","json-parse-helpfulerror":"^1.0.3","lodash":"^4.17.11","node-alias":"^1.0.4","npm-conf":"^1.1.3","package-json":"^6.3.0","progress":"^2.0.3","prompts":"^2.1.0","rc-config-loader":"^2.0.3","registry-url":"^5.1.0","requireg":"^0.2.2","semver":"^6.1.1","semver-utils":"^1.1.4","spawn-please":"^0.3.0","update-notifier":"^3.0.0"},"dependencies":{"ansi-styles":{"version":"3.2.1","resolved":"https://registry.npmjs.org/ansi-styles/-/ansi-styles-3.2.1.tgz","integrity":"sha512-VT0ZI6kZRdTh8YyJw3SMbYm/u+NqfsAxEpWO0Pf9sq8/e94WxxOpPKx9FR1FlyCtOVDNOQ+8ntlqFxiRc+r5qA==","requires":{"color-convert":"^1.9.0"}},"chalk":{"version":"2.4.2","resolved":"https://registry.npmjs.org/chalk/-/chalk-2.4.2.tgz","integrity":"sha512-Mti+f9lpJNcwF4tWV8/OrTTtF1gZi+f8FqlyAdouralcFWFQWF2+NgCHShjkCb+IFBLq9buZwE1xckQU4peSuQ==","requires":{"ansi-styles":"^3.2.1","escape-string-regexp":"^1.0.5","supports-color":"^5.3.0"}},"supports-color":{"version":"5.5.0","resolved":"https://registry.npmjs.org/supports-color/-/supports-color-5.5.0.tgz","integrity":"sha512-QjVjwdXIt408MIiAqCX4oUKsgU2EqAGzs2Ppkm4aQYbjm+ZEWEcW4SfFNTr4uMNZma0ey4f5lgLrkB0aX0QMow==","requires":{"has-flag":"^3.0.0"}}}},"npm-conf":{"version":"1.1.3","resolved":"https://registry.npmjs.org/npm-conf/-/npm-conf-1.1.3.tgz","integrity":"sha512-Yic4bZHJOt9RCFbRP3GgpqhScOY4HH3V2P8yBj6CeYq118Qr+BLXqT2JvpJ00mryLESpgOxf5XlFv4ZjXxLScw==","requires":{"config-chain":"^1.1.11","pify":"^3.0.0"}},"npm-run-path":{"version":"2.0.2","resolved":"https://registry.npmjs.org/npm-run-path/-/npm-run-path-2.0.2.tgz","integrity":"sha1-NakjLfo11wZ7TLLd8jV7GHFTbF8=","requires":{"path-key":"^2.0.0"}},"object-assign":{"version":"4.1.1","resolved":"https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz","integrity":"sha1-IQmtx5ZYh8/AXLvUQsrIv7s2CGM="},"object-copy":{"version":"0.1.0","resolved":"https://registry.npmjs.org/object-copy/-/object-copy-0.1.0.tgz","integrity":"sha1-fn2Fi3gb18mRpBupde04EnVOmYw=","requires":{"copy-descriptor":"^0.1.0","define-property":"^0.2.5","kind-of":"^3.0.3"},"dependencies":{"define-property":{"version":"0.2.5","resolved":"https://registry.npmjs.org/define-property/-/define-property-0.2.5.tgz","integrity":"sha1-w1se+RjsPJkPmlvFe+BKrOxcgRY=","requires":{"is-descriptor":"^0.1.0"}},"kind-of":{"version":"3.2.2","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz","integrity":"sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=","requires":{"is-buffer":"^1.1.5"}}}},"object-keys":{"version":"1.1.1","resolved":"https://registry.npmjs.org/object-keys/-/object-keys-1.1.1.tgz","integrity":"sha512-NuAESUOUMrlIXOfHKzD6bpPu3tYt3xvjNdRIQ+FeT0lNb4K8WR70CaDxhuNguS2XG+GjkyMwOzsN5ZktImfhLA=="},"object-visit":{"version":"1.0.1","resolved":"https://registry.npmjs.org/object-visit/-/object-visit-1.0.1.tgz","integrity":"sha1-95xEk68MU3e1n+OdOV5BBC3QRbs=","requires":{"isobject":"^3.0.0"}},"object.pick":{"version":"1.3.0","resolved":"https://registry.npmjs.org/object.pick/-/object.pick-1.3.0.tgz","integrity":"sha1-h6EKxMFpS9Lhy/U1kaZhQftd10c=","requires":{"isobject":"^3.0.1"}},"once":{"version":"1.4.0","resolved":"https://registry.npmjs.org/once/-/once-1.4.0.tgz","integrity":"sha1-WDsap3WWHUsROsF9nFC6753Xa9E=","requires":{"wrappy":"1"}},"onetime":{"version":"2.0.1","resolved":"https://registry.npmjs.org/onetime/-/onetime-2.0.1.tgz","integrity":"sha1-BnQoIw/WdEOyeUsiu6UotoZ5YtQ=","requires":{"mimic-fn":"^1.0.0"}},"opencollective":{"version":"1.0.3","resolved":"https://registry.npmjs.org/opencollective/-/opencollective-1.0.3.tgz","integrity":"sha1-ruY3K8KBRFg2kMPKja7PwSDdDvE=","requires":{"babel-polyfill":"6.23.0","chalk":"1.1.3","inquirer":"3.0.6","minimist":"1.2.0","node-fetch":"1.6.3","opn":"4.0.2"},"dependencies":{"minimist":{"version":"1.2.0","resolved":"https://registry.npmjs.org/minimist/-/minimist-1.2.0.tgz","integrity":"sha1-o1AIsg9BOD7sH7kU9M1d95omQoQ="}}},"opencollective-postinstall":{"version":"2.0.2","resolved":"https://registry.npmjs.org/opencollective-postinstall/-/opencollective-postinstall-2.0.2.tgz","integrity":"sha512-pVOEP16TrAO2/fjej1IdOyupJY8KDUM1CvsaScRbw6oddvpQoOfGk4ywha0HKKVAD6RkW4x6Q+tNBwhf3Bgpuw=="},"opn":{"version":"4.0.2","resolved":"https://registry.npmjs.org/opn/-/opn-4.0.2.tgz","integrity":"sha1-erwi5kTf9jsKltWrfyeQwPAavJU=","requires":{"object-assign":"^4.0.1","pinkie-promise":"^2.0.0"}},"os-tmpdir":{"version":"1.0.2","resolved":"https://registry.npmjs.org/os-tmpdir/-/os-tmpdir-1.0.2.tgz","integrity":"sha1-u+Z0BseaqFxc/sdm/lc0VV36EnQ="},"p-cancelable":{"version":"1.1.0","resolved":"https://registry.npmjs.org/p-cancelable/-/p-cancelable-1.1.0.tgz","integrity":"sha512-s73XxOZ4zpt1edZYZzvhqFa6uvQc1vwUa0K0BdtIZgQMAJj9IbebH+JkgKZc9h+B05PKHLOTl4ajG1BmNrVZlw=="},"p-finally":{"version":"1.0.0","resolved":"https://registry.npmjs.org/p-finally/-/p-finally-1.0.0.tgz","integrity":"sha1-P7z7FbiZpEEjs0ttzBi3JDNqLK4="},"p-limit":{"version":"2.2.0","resolved":"https://registry.npmjs.org/p-limit/-/p-limit-2.2.0.tgz","integrity":"sha512-pZbTJpoUsCzV48Mc9Nh51VbwO0X9cuPFE8gYwx9BTCt9SF8/b7Zljd2fVgOxhIF/HDTKgpVzs+GPhyKfjLLFRQ==","requires":{"p-try":"^2.0.0"}},"p-locate":{"version":"4.1.0","resolved":"https://registry.npmjs.org/p-locate/-/p-locate-4.1.0.tgz","integrity":"sha512-R79ZZ/0wAxKGu3oYMlz8jy/kbhsNrS7SKZ7PxEHBgJ5+F2mtFW2fK2cOtBh1cHYkQsbzFV7I+EoRKe6Yt0oK7A==","requires":{"p-limit":"^2.2.0"}},"p-try":{"version":"2.2.0","resolved":"https://registry.npmjs.org/p-try/-/p-try-2.2.0.tgz","integrity":"sha512-R4nPAVTAU0B9D35/Gk3uJf/7XYbQcyohSKdvAxIRSNghFl4e71hVoGnBNQz9cWaXxO2I10KTC+3jMdvvoKw6dQ=="},"package-json":{"version":"6.3.0","resolved":"https://registry.npmjs.org/package-json/-/package-json-6.3.0.tgz","integrity":"sha512-XO7WS3EEXd48vmW633Y97Mh9xuENFiOevI9G+ExfTG/k6xuY9cBd3fxkAoDMSEsNZXasaVJIJ1rD/n7GMf18bA==","requires":{"got":"^9.6.0","registry-auth-token":"^3.4.0","registry-url":"^5.0.0","semver":"^5.6.0"},"dependencies":{"semver":{"version":"5.7.0","resolved":"https://registry.npmjs.org/semver/-/semver-5.7.0.tgz","integrity":"sha512-Ya52jSX2u7QKghxeoFGpLwCtGlt7j0oY9DYb5apt9nPlJ42ID+ulTXESnt/qAQcoSERyZ5sl3LDIOw0nAn/5DA=="}}},"pascalcase":{"version":"0.1.1","resolved":"https://registry.npmjs.org/pascalcase/-/pascalcase-0.1.1.tgz","integrity":"sha1-s2PlXoAGym/iF4TS2yK9FdeRfxQ="},"path":{"version":"0.12.7","resolved":"https://registry.npmjs.org/path/-/path-0.12.7.tgz","integrity":"sha1-1NwqUGxM4hl+tIHr/NWzbAFAsQ8=","requires":{"process":"^0.11.1","util":"^0.10.3"}},"path-dirname":{"version":"1.0.2","resolved":"https://registry.npmjs.org/path-dirname/-/path-dirname-1.0.2.tgz","integrity":"sha1-zDPSTVJeCZpTiMAzbG4yuRYGCeA="},"path-exists":{"version":"3.0.0","resolved":"https://registry.npmjs.org/path-exists/-/path-exists-3.0.0.tgz","integrity":"sha1-zg6+ql94yxiSXqfYENe1mwEP1RU="},"path-is-absolute":{"version":"1.0.1","resolved":"https://registry.npmjs.org/path-is-absolute/-/path-is-absolute-1.0.1.tgz","integrity":"sha1-F0uSaHNVNP+8es5r9TpanhtcX18="},"path-is-inside":{"version":"1.0.2","resolved":"https://registry.npmjs.org/path-is-inside/-/path-is-inside-1.0.2.tgz","integrity":"sha1-NlQX3t5EQw0cEa9hAn+s8HS9/FM="},"path-key":{"version":"2.0.1","resolved":"https://registry.npmjs.org/path-key/-/path-key-2.0.1.tgz","integrity":"sha1-QRyttXTFoUDTpLGRDUDYDMn0C0A="},"path-parse":{"version":"1.0.6","resolved":"https://registry.npmjs.org/path-parse/-/path-parse-1.0.6.tgz","integrity":"sha512-GSmOT2EbHrINBf9SR7CDELwlJ8AENk3Qn7OikK4nFYAu3Ote2+JYNVvkpAEQm3/TLNEJFD/xZJjzyxg3KBWOzw=="},"pify":{"version":"3.0.0","resolved":"https://registry.npmjs.org/pify/-/pify-3.0.0.tgz","integrity":"sha1-5aSs0sEB/fPZpNB/DbxNtJ3SgXY="},"pinkie":{"version":"2.0.4","resolved":"https://registry.npmjs.org/pinkie/-/pinkie-2.0.4.tgz","integrity":"sha1-clVrgM+g1IqXToDnckjoDtT3+HA="},"pinkie-promise":{"version":"2.0.1","resolved":"https://registry.npmjs.org/pinkie-promise/-/pinkie-promise-2.0.1.tgz","integrity":"sha1-ITXW36ejWMBprJsXh3YogihFD/o=","requires":{"pinkie":"^2.0.0"}},"posix-character-classes":{"version":"0.1.1","resolved":"https://registry.npmjs.org/posix-character-classes/-/posix-character-classes-0.1.1.tgz","integrity":"sha1-AerA/jta9xoqbAL+q7jB/vfgDqs="},"prepend-http":{"version":"2.0.0","resolved":"https://registry.npmjs.org/prepend-http/-/prepend-http-2.0.0.tgz","integrity":"sha1-6SQ0v6XqjBn0HN/UAddBo8gZ2Jc="},"process":{"version":"0.11.10","resolved":"https://registry.npmjs.org/process/-/process-0.11.10.tgz","integrity":"sha1-czIwDoQBYb2j5podHZGn1LwW8YI="},"process-nextick-args":{"version":"2.0.0","resolved":"https://registry.npmjs.org/process-nextick-args/-/process-nextick-args-2.0.0.tgz","integrity":"sha512-MtEC1TqN0EU5nephaJ4rAtThHtC86dNN9qCuEhtshvpVBkAW5ZO7BASN9REnF9eoXGcRub+pFuKEpOHE+HbEMw=="},"progress":{"version":"2.0.3","resolved":"https://registry.npmjs.org/progress/-/progress-2.0.3.tgz","integrity":"sha512-7PiHtLll5LdnKIMw100I+8xJXR5gW2QwWYkT6iJva0bXitZKa/XMrSbdmg3r2Xnaidz9Qumd0VPaMrZlF9V9sA=="},"prompts":{"version":"2.1.0","resolved":"https://registry.npmjs.org/prompts/-/prompts-2.1.0.tgz","integrity":"sha512-+x5TozgqYdOwWsQFZizE/Tra3fKvAoy037kOyU6cgz84n8f6zxngLOV4O32kTwt9FcLCxAqw0P/c8rOr9y+Gfg==","requires":{"kleur":"^3.0.2","sisteransi":"^1.0.0"}},"proto-list":{"version":"1.2.4","resolved":"https://registry.npmjs.org/proto-list/-/proto-list-1.2.4.tgz","integrity":"sha1-IS1b/hMYMGpCD2QCuOJv85ZHqEk="},"pseudomap":{"version":"1.0.2","resolved":"https://registry.npmjs.org/pseudomap/-/pseudomap-1.0.2.tgz","integrity":"sha1-8FKijacOYYkX7wqKw0wa5aaChrM="},"pump":{"version":"3.0.0","resolved":"https://registry.npmjs.org/pump/-/pump-3.0.0.tgz","integrity":"sha512-LwZy+p3SFs1Pytd/jYct4wpv49HiYCqd9Rlc5ZVdk0V+8Yzv6jR5Blk3TRmPL1ft69TxP0IMZGJ+WPFU2BFhww==","requires":{"end-of-stream":"^1.1.0","once":"^1.3.1"}},"rc":{"version":"1.2.8","resolved":"https://registry.npmjs.org/rc/-/rc-1.2.8.tgz","integrity":"sha512-y3bGgqKj3QBdxLbLkomlohkvsA8gdAiUQlSBJnBhfn+BPxg4bc62d8TcBW15wavDfgexCgccckhcZvywyQYPOw==","requires":{"deep-extend":"^0.6.0","ini":"~1.3.0","minimist":"^1.2.0","strip-json-comments":"~2.0.1"},"dependencies":{"minimist":{"version":"1.2.0","resolved":"https://registry.npmjs.org/minimist/-/minimist-1.2.0.tgz","integrity":"sha1-o1AIsg9BOD7sH7kU9M1d95omQoQ="}}},"rc-config-loader":{"version":"2.0.3","resolved":"https://registry.npmjs.org/rc-config-loader/-/rc-config-loader-2.0.3.tgz","integrity":"sha512-CN9BmvV9Kcl6c4WEZ8w13JFazLYtKnqxxKGKuiCS8yZpUtJFa/nd7PdjVnRBJJ89U3OZfR6DmqOqMvmZ5ZFNMw==","requires":{"debug":"^3.1.0","js-yaml":"^3.12.0","json5":"^1.0.1","object-assign":"^4.1.0","object-keys":"^1.0.12","path-exists":"^3.0.0","require-from-string":"^2.0.2"},"dependencies":{"debug":{"version":"3.2.6","resolved":"https://registry.npmjs.org/debug/-/debug-3.2.6.tgz","integrity":"sha512-mel+jf7nrtEl5Pn1Qx46zARXKDpBbvzezse7p7LqINmdoIk8PYP5SySaxEmYv6TZ0JyEKA1hsCId6DIhgITtWQ==","requires":{"ms":"^2.1.1"}},"ms":{"version":"2.1.1","resolved":"https://registry.npmjs.org/ms/-/ms-2.1.1.tgz","integrity":"sha512-tgp+dl5cGk28utYktBsrFqA7HKgrhgPsg6Z/EfhWI4gl1Hwq8B/GmY/0oXZ6nF8hDVesS/FpnYaD/kOWhYQvyg=="}}},"readable-stream":{"version":"2.3.6","resolved":"https://registry.npmjs.org/readable-stream/-/readable-stream-2.3.6.tgz","integrity":"sha512-tQtKA9WIAhBF3+VLAseyMqZeBjW0AHJoxOtYqSUZNJxauErmLbVm2FW1y+J/YA9dUrAC39ITejlZWhVIwawkKw==","requires":{"core-util-is":"~1.0.0","inherits":"~2.0.3","isarray":"~1.0.0","process-nextick-args":"~2.0.0","safe-buffer":"~5.1.1","string_decoder":"~1.1.1","util-deprecate":"~1.0.1"}},"readdirp":{"version":"2.2.1","resolved":"https://registry.npmjs.org/readdirp/-/readdirp-2.2.1.tgz","integrity":"sha512-1JU/8q+VgFZyxwrJ+SVIOsh+KywWGpds3NTqikiKpDMZWScmAYyKIgqkO+ARvNWJfXeXR1zxz7aHF4u4CyH6vQ==","requires":{"graceful-fs":"^4.1.11","micromatch":"^3.1.10","readable-stream":"^2.0.2"}},"regenerator-runtime":{"version":"0.10.5","resolved":"https://registry.npmjs.org/regenerator-runtime/-/regenerator-runtime-0.10.5.tgz","integrity":"sha1-M2w+/BIgrc7dosn6tntaeVWjNlg="},"regex-not":{"version":"1.0.2","resolved":"https://registry.npmjs.org/regex-not/-/regex-not-1.0.2.tgz","integrity":"sha512-J6SDjUgDxQj5NusnOtdFxDwN/+HWykR8GELwctJ7mdqhcyy1xEc4SRFHUXvxTp661YaVKAjfRLZ9cCqS6tn32A==","requires":{"extend-shallow":"^3.0.2","safe-regex":"^1.1.0"}},"registry-auth-token":{"version":"3.4.0","resolved":"https://registry.npmjs.org/registry-auth-token/-/registry-auth-token-3.4.0.tgz","integrity":"sha512-4LM6Fw8eBQdwMYcES4yTnn2TqIasbXuwDx3um+QRs7S55aMKCBKBxvPXl2RiUjHwuJLTyYfxSpmfSAjQpcuP+A==","requires":{"rc":"^1.1.6","safe-buffer":"^5.0.1"}},"registry-url":{"version":"5.1.0","resolved":"https://registry.npmjs.org/registry-url/-/registry-url-5.1.0.tgz","integrity":"sha512-8acYXXTI0AkQv6RAOjE3vOaIXZkT9wo4LOFbBKYQEEnnMNBpKqdUrI6S4NT0KPIo/WVvJ5tE/X5LF/TQUf0ekw==","requires":{"rc":"^1.2.8"}},"remove-trailing-separator":{"version":"1.1.0","resolved":"https://registry.npmjs.org/remove-trailing-separator/-/remove-trailing-separator-1.1.0.tgz","integrity":"sha1-wkvOKig62tW8P1jg1IJJuSN52O8="},"repeat-element":{"version":"1.1.3","resolved":"https://registry.npmjs.org/repeat-element/-/repeat-element-1.1.3.tgz","integrity":"sha512-ahGq0ZnV5m5XtZLMb+vP76kcAM5nkLqk0lpqAuojSKGgQtn4eRi4ZZGm2olo2zKFH+sMsWaqOCW1dqAnOru72g=="},"repeat-string":{"version":"1.6.1","resolved":"https://registry.npmjs.org/repeat-string/-/repeat-string-1.6.1.tgz","integrity":"sha1-jcrkcOHIirwtYA//Sndihtp15jc="},"require-from-string":{"version":"2.0.2","resolved":"https://registry.npmjs.org/require-from-string/-/require-from-string-2.0.2.tgz","integrity":"sha512-Xf0nWe6RseziFMu+Ap9biiUbmplq6S9/p+7w7YXP/JBHhrUDDUhwa+vANyubuqfZWTveU//DYVGsDG7RKL/vEw=="},"requireg":{"version":"0.2.2","resolved":"https://registry.npmjs.org/requireg/-/requireg-0.2.2.tgz","integrity":"sha512-nYzyjnFcPNGR3lx9lwPPPnuQxv6JWEZd2Ci0u9opN7N5zUEPIhY/GbL3vMGOr2UXwEg9WwSyV9X9Y/kLFgPsOg==","requires":{"nested-error-stacks":"~2.0.1","rc":"~1.2.7","resolve":"~1.7.1"}},"resolve":{"version":"1.7.1","resolved":"https://registry.npmjs.org/resolve/-/resolve-1.7.1.tgz","integrity":"sha512-c7rwLofp8g1U+h1KNyHL/jicrKg1Ek4q+Lr33AL65uZTinUZHe30D5HlyN5V9NW0JX1D5dXQ4jqW5l7Sy/kGfw==","requires":{"path-parse":"^1.0.5"}},"resolve-url":{"version":"0.2.1","resolved":"https://registry.npmjs.org/resolve-url/-/resolve-url-0.2.1.tgz","integrity":"sha1-LGN/53yJOv0qZj/iGqkIAGjiBSo="},"responselike":{"version":"1.0.2","resolved":"https://registry.npmjs.org/responselike/-/responselike-1.0.2.tgz","integrity":"sha1-kYcg7ztjHFZCvgaPFa3lpG9Loec=","requires":{"lowercase-keys":"^1.0.0"}},"restore-cursor":{"version":"2.0.0","resolved":"https://registry.npmjs.org/restore-cursor/-/restore-cursor-2.0.0.tgz","integrity":"sha1-n37ih/gv0ybU/RYpI9YhKe7g368=","requires":{"onetime":"^2.0.0","signal-exit":"^3.0.2"}},"ret":{"version":"0.1.15","resolved":"https://registry.npmjs.org/ret/-/ret-0.1.15.tgz","integrity":"sha512-TTlYpa+OL+vMMNG24xSlQGEJ3B/RzEfUlLct7b5G/ytav+wPrplCpVMFuwzXbkecJrb6IYo1iFb0S9v37754mg=="},"rimraf":{"version":"2.4.5","resolved":"https://registry.npmjs.org/rimraf/-/rimraf-2.4.5.tgz","integrity":"sha1-7nEM5dk6j9uFb7Xqj/Di11k0sto=","optional":true,"requires":{"glob":"^6.0.1"}},"run-async":{"version":"2.3.0","resolved":"https://registry.npmjs.org/run-async/-/run-async-2.3.0.tgz","integrity":"sha1-A3GrSuC91yDUFm19/aZP96RFpsA=","requires":{"is-promise":"^2.1.0"}},"rx":{"version":"4.1.0","resolved":"https://registry.npmjs.org/rx/-/rx-4.1.0.tgz","integrity":"sha1-pfE/957zt0D+MKqAP7CfmIBdR4I="},"safe-buffer":{"version":"5.1.2","resolved":"https://registry.npmjs.org/safe-buffer/-/safe-buffer-5.1.2.tgz","integrity":"sha512-Gd2UZBJDkXlY7GbJxfsE8/nvKkUEU1G38c1siN6QP6a9PT9MmHB8GnpscSmMJSoF8LOIrt8ud/wPtojys4G6+g=="},"safe-json-stringify":{"version":"1.2.0","resolved":"https://registry.npmjs.org/safe-json-stringify/-/safe-json-stringify-1.2.0.tgz","integrity":"sha512-gH8eh2nZudPQO6TytOvbxnuhYBOvDBBLW52tz5q6X58lJcd/tkmqFR+5Z9adS8aJtURSXWThWy/xJtJwixErvg==","optional":true},"safe-regex":{"version":"1.1.0","resolved":"https://registry.npmjs.org/safe-regex/-/safe-regex-1.1.0.tgz","integrity":"sha1-QKNmnzsHfR6UPURinhV91IAjvy4=","requires":{"ret":"~0.1.10"}},"safer-buffer":{"version":"2.1.2","resolved":"https://registry.npmjs.org/safer-buffer/-/safer-buffer-2.1.2.tgz","integrity":"sha512-YZo3K82SD7Riyi0E1EQPojLz7kpepnSQI9IyPbHHg1XXXevb5dJI7tpyN2ADxGcQbHG7vcyRHk0cbwqcQriUtg=="},"semver":{"version":"6.1.1","resolved":"https://registry.npmjs.org/semver/-/semver-6.1.1.tgz","integrity":"sha512-rWYq2e5iYW+fFe/oPPtYJxYgjBm8sC4rmoGdUOgBB7VnwKt6HrL793l2voH1UlsyYZpJ4g0wfjnTEO1s1NP2eQ=="},"semver-diff":{"version":"2.1.0","resolved":"https://registry.npmjs.org/semver-diff/-/semver-diff-2.1.0.tgz","integrity":"sha1-S7uEN8jTfksM8aaP1ybsbWRdbTY=","requires":{"semver":"^5.0.3"},"dependencies":{"semver":{"version":"5.7.0","resolved":"https://registry.npmjs.org/semver/-/semver-5.7.0.tgz","integrity":"sha512-Ya52jSX2u7QKghxeoFGpLwCtGlt7j0oY9DYb5apt9nPlJ42ID+ulTXESnt/qAQcoSERyZ5sl3LDIOw0nAn/5DA=="}}},"semver-utils":{"version":"1.1.4","resolved":"https://registry.npmjs.org/semver-utils/-/semver-utils-1.1.4.tgz","integrity":"sha512-EjnoLE5OGmDAVV/8YDoN5KiajNadjzIp9BAHOhYeQHt7j0UWxjmgsx4YD48wp4Ue1Qogq38F1GNUJNqF1kKKxA=="},"set-value":{"version":"2.0.0","resolved":"https://registry.npmjs.org/set-value/-/set-value-2.0.0.tgz","integrity":"sha512-hw0yxk9GT/Hr5yJEYnHNKYXkIA8mVJgd9ditYZCe16ZczcaELYYcfvaXesNACk2O8O0nTiPQcQhGUQj8JLzeeg==","requires":{"extend-shallow":"^2.0.1","is-extendable":"^0.1.1","is-plain-object":"^2.0.3","split-string":"^3.0.1"},"dependencies":{"extend-shallow":{"version":"2.0.1","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz","integrity":"sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=","requires":{"is-extendable":"^0.1.0"}}}},"shebang-command":{"version":"1.2.0","resolved":"https://registry.npmjs.org/shebang-command/-/shebang-command-1.2.0.tgz","integrity":"sha1-RKrGW2lbAzmJaMOfNj/uXer98eo=","requires":{"shebang-regex":"^1.0.0"}},"shebang-regex":{"version":"1.0.0","resolved":"https://registry.npmjs.org/shebang-regex/-/shebang-regex-1.0.0.tgz","integrity":"sha1-2kL0l0DAtC2yypcoVxyxkMmO/qM="},"signal-exit":{"version":"3.0.2","resolved":"https://registry.npmjs.org/signal-exit/-/signal-exit-3.0.2.tgz","integrity":"sha1-tf3AjxKH6hF4Yo5BXiUTK3NkbG0="},"sisteransi":{"version":"1.0.0","resolved":"https://registry.npmjs.org/sisteransi/-/sisteransi-1.0.0.tgz","integrity":"sha512-N+z4pHB4AmUv0SjveWRd6q1Nj5w62m5jodv+GD8lvmbY/83T/rpbJGZOnK5T149OldDj4Db07BSv9xY4K6NTPQ=="},"snapdragon":{"version":"0.8.2","resolved":"https://registry.npmjs.org/snapdragon/-/snapdragon-0.8.2.tgz","integrity":"sha512-FtyOnWN/wCHTVXOMwvSv26d+ko5vWlIDD6zoUJ7LW8vh+ZBC8QdljveRP+crNrtBwioEUWy/4dMtbBjA4ioNlg==","requires":{"base":"^0.11.1","debug":"^2.2.0","define-property":"^0.2.5","extend-shallow":"^2.0.1","map-cache":"^0.2.2","source-map":"^0.5.6","source-map-resolve":"^0.5.0","use":"^3.1.0"},"dependencies":{"define-property":{"version":"0.2.5","resolved":"https://registry.npmjs.org/define-property/-/define-property-0.2.5.tgz","integrity":"sha1-w1se+RjsPJkPmlvFe+BKrOxcgRY=","requires":{"is-descriptor":"^0.1.0"}},"extend-shallow":{"version":"2.0.1","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz","integrity":"sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=","requires":{"is-extendable":"^0.1.0"}}}},"snapdragon-node":{"version":"2.1.1","resolved":"https://registry.npmjs.org/snapdragon-node/-/snapdragon-node-2.1.1.tgz","integrity":"sha512-O27l4xaMYt/RSQ5TR3vpWCAB5Kb/czIcqUFOM/C4fYcLnbZUc1PkjTAMjof2pBWaSTwOUd6qUHcFGVGj7aIwnw==","requires":{"define-property":"^1.0.0","isobject":"^3.0.0","snapdragon-util":"^3.0.1"},"dependencies":{"define-property":{"version":"1.0.0","resolved":"https://registry.npmjs.org/define-property/-/define-property-1.0.0.tgz","integrity":"sha1-dp66rz9KY6rTr56NMEybvnm/sOY=","requires":{"is-descriptor":"^1.0.0"}},"is-accessor-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-accessor-descriptor/-/is-accessor-descriptor-1.0.0.tgz","integrity":"sha512-m5hnHTkcVsPfqx3AKlyttIPb7J+XykHvJP2B9bZDjlhLIoEq4XoK64Vg7boZlVWYK6LUY94dYPEE7Lh0ZkZKcQ==","requires":{"kind-of":"^6.0.0"}},"is-data-descriptor":{"version":"1.0.0","resolved":"https://registry.npmjs.org/is-data-descriptor/-/is-data-descriptor-1.0.0.tgz","integrity":"sha512-jbRXy1FmtAoCjQkVmIVYwuuqDFUbaOeDjmed1tOGPrsMhtJA4rD9tkgA0F1qJ3gRFRXcHYVkdeaP50Q5rE/jLQ==","requires":{"kind-of":"^6.0.0"}},"is-descriptor":{"version":"1.0.2","resolved":"https://registry.npmjs.org/is-descriptor/-/is-descriptor-1.0.2.tgz","integrity":"sha512-2eis5WqQGV7peooDyLmNEPUrps9+SXX5c9pL3xEB+4e9HnGuDa7mB7kHxHw4CbqS9k1T2hOH3miL8n8WtiYVtg==","requires":{"is-accessor-descriptor":"^1.0.0","is-data-descriptor":"^1.0.0","kind-of":"^6.0.2"}}}},"snapdragon-util":{"version":"3.0.1","resolved":"https://registry.npmjs.org/snapdragon-util/-/snapdragon-util-3.0.1.tgz","integrity":"sha512-mbKkMdQKsjX4BAL4bRYTj21edOf8cN7XHdYUJEe+Zn99hVEYcMvKPct1IqNe7+AZPirn8BCDOQBHQZknqmKlZQ==","requires":{"kind-of":"^3.2.0"},"dependencies":{"kind-of":{"version":"3.2.2","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz","integrity":"sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=","requires":{"is-buffer":"^1.1.5"}}}},"source-map":{"version":"0.5.7","resolved":"https://registry.npmjs.org/source-map/-/source-map-0.5.7.tgz","integrity":"sha1-igOdLRAh0i0eoUyA2OpGi6LvP8w="},"source-map-resolve":{"version":"0.5.2","resolved":"https://registry.npmjs.org/source-map-resolve/-/source-map-resolve-0.5.2.tgz","integrity":"sha512-MjqsvNwyz1s0k81Goz/9vRBe9SZdB09Bdw+/zYyO+3CuPk6fouTaxscHkgtE8jKvf01kVfl8riHzERQ/kefaSA==","requires":{"atob":"^2.1.1","decode-uri-component":"^0.2.0","resolve-url":"^0.2.1","source-map-url":"^0.4.0","urix":"^0.1.0"}},"source-map-url":{"version":"0.4.0","resolved":"https://registry.npmjs.org/source-map-url/-/source-map-url-0.4.0.tgz","integrity":"sha1-PpNdfd1zYxuXZZlW1VEo6HtQhKM="},"spawn-please":{"version":"0.3.0","resolved":"https://registry.npmjs.org/spawn-please/-/spawn-please-0.3.0.tgz","integrity":"sha1-2zOOxM/2Orxp8dDgjO6euL69nRE="},"split-string":{"version":"3.1.0","resolved":"https://registry.npmjs.org/split-string/-/split-string-3.1.0.tgz","integrity":"sha512-NzNVhJDYpwceVVii8/Hu6DKfD2G+NrQHlS/V/qgv763EYudVwEcMQNxd2lh+0VrUByXN/oJkl5grOhYWvQUYiw==","requires":{"extend-shallow":"^3.0.0"}},"sprintf-js":{"version":"1.0.3","resolved":"https://registry.npmjs.org/sprintf-js/-/sprintf-js-1.0.3.tgz","integrity":"sha1-BOaSb2YolTVPPdAVIDYzuFcpfiw="},"static-extend":{"version":"0.1.2","resolved":"https://registry.npmjs.org/static-extend/-/static-extend-0.1.2.tgz","integrity":"sha1-YICcOcv/VTNyJv1eC1IPNB8ftcY=","requires":{"define-property":"^0.2.5","object-copy":"^0.1.0"},"dependencies":{"define-property":{"version":"0.2.5","resolved":"https://registry.npmjs.org/define-property/-/define-property-0.2.5.tgz","integrity":"sha1-w1se+RjsPJkPmlvFe+BKrOxcgRY=","requires":{"is-descriptor":"^0.1.0"}}}},"string-width":{"version":"2.1.1","resolved":"https://registry.npmjs.org/string-width/-/string-width-2.1.1.tgz","integrity":"sha512-nOqH59deCq9SRHlxq1Aw85Jnt4w6KvLKqWVik6oA9ZklXLNIOlqg4F2yrT1MVaTjAqvVwdfeZ7w7aCvJD7ugkw==","requires":{"is-fullwidth-code-point":"^2.0.0","strip-ansi":"^4.0.0"},"dependencies":{"ansi-regex":{"version":"3.0.0","resolved":"https://registry.npmjs.org/ansi-regex/-/ansi-regex-3.0.0.tgz","integrity":"sha1-7QMXwyIGT3lGbAKWa922Bas32Zg="},"strip-ansi":{"version":"4.0.0","resolved":"https://registry.npmjs.org/strip-ansi/-/strip-ansi-4.0.0.tgz","integrity":"sha1-qEeQIusaw2iocTibY1JixQXuNo8=","requires":{"ansi-regex":"^3.0.0"}}}},"string_decoder":{"version":"1.1.1","resolved":"https://registry.npmjs.org/string_decoder/-/string_decoder-1.1.1.tgz","integrity":"sha512-n/ShnvDi6FHbbVfviro+WojiFzv+s8MPMHBczVePfUpDJLwoLT0ht1l4YwBCbi8pJAveEEdnkHyPyTP/mzRfwg==","requires":{"safe-buffer":"~5.1.0"}},"strip-ansi":{"version":"3.0.1","resolved":"https://registry.npmjs.org/strip-ansi/-/strip-ansi-3.0.1.tgz","integrity":"sha1-ajhfuIU9lS1f8F0Oiq+UJ43GPc8=","requires":{"ansi-regex":"^2.0.0"}},"strip-eof":{"version":"1.0.0","resolved":"https://registry.npmjs.org/strip-eof/-/strip-eof-1.0.0.tgz","integrity":"sha1-u0P/VZim6wXYm1n80SnJgzE2Br8="},"strip-json-comments":{"version":"2.0.1","resolved":"https://registry.npmjs.org/strip-json-comments/-/strip-json-comments-2.0.1.tgz","integrity":"sha1-PFMZQukIwml8DsNEhYwobHygpgo="},"supports-color":{"version":"2.0.0","resolved":"https://registry.npmjs.org/supports-color/-/supports-color-2.0.0.tgz","integrity":"sha1-U10EXOa2Nj+kARcIRimZXp3zJMc="},"term-size":{"version":"1.2.0","resolved":"https://registry.npmjs.org/term-size/-/term-size-1.2.0.tgz","integrity":"sha1-RYuDiH8oj8Vtb/+/rSYuJmOO+mk=","requires":{"execa":"^0.7.0"}},"through":{"version":"2.3.8","resolved":"https://registry.npmjs.org/through/-/through-2.3.8.tgz","integrity":"sha1-DdTJ/6q8NXlgsbckEV1+Doai4fU="},"tmp":{"version":"0.0.33","resolved":"https://registry.npmjs.org/tmp/-/tmp-0.0.33.tgz","integrity":"sha512-jRCJlojKnZ3addtTOjdIqoRuPEKBvNXcGYqzO6zWZX8KfKEpnGY5jfggJQ3EjKuu8D4bJRr0y+cYJFmYbImXGw==","requires":{"os-tmpdir":"~1.0.2"}},"to-object-path":{"version":"0.3.0","resolved":"https://registry.npmjs.org/to-object-path/-/to-object-path-0.3.0.tgz","integrity":"sha1-KXWIt7Dn4KwI4E5nL4XB9JmeF68=","requires":{"kind-of":"^3.0.2"},"dependencies":{"kind-of":{"version":"3.2.2","resolved":"https://registry.npmjs.org/kind-of/-/kind-of-3.2.2.tgz","integrity":"sha1-MeohpzS6ubuw8yRm2JOupR5KPGQ=","requires":{"is-buffer":"^1.1.5"}}}},"to-readable-stream":{"version":"1.0.0","resolved":"https://registry.npmjs.org/to-readable-stream/-/to-readable-stream-1.0.0.tgz","integrity":"sha512-Iq25XBt6zD5npPhlLVXGFN3/gyR2/qODcKNNyTMd4vbm39HUaOiAM4PMq0eMVC/Tkxz+Zjdsc55g9yyz+Yq00Q=="},"to-regex":{"version":"3.0.2","resolved":"https://registry.npmjs.org/to-regex/-/to-regex-3.0.2.tgz","integrity":"sha512-FWtleNAtZ/Ki2qtqej2CXTOayOH9bHDQF+Q48VpWyDXjbYxA4Yz8iDB31zXOBUlOHHKidDbqGVrTUvQMPmBGBw==","requires":{"define-property":"^2.0.2","extend-shallow":"^3.0.2","regex-not":"^1.0.2","safe-regex":"^1.1.0"}},"to-regex-range":{"version":"2.1.1","resolved":"https://registry.npmjs.org/to-regex-range/-/to-regex-range-2.1.1.tgz","integrity":"sha1-fIDBe53+vlmeJzZ+DU3VWQFB2zg=","requires":{"is-number":"^3.0.0","repeat-string":"^1.6.1"}},"type-fest":{"version":"0.3.1","resolved":"https://registry.npmjs.org/type-fest/-/type-fest-0.3.1.tgz","integrity":"sha512-cUGJnCdr4STbePCgqNFbpVNCepa+kAVohJs1sLhxzdH+gnEoOd8VhbYa7pD3zZYGiURWM2xzEII3fQcRizDkYQ=="},"union-value":{"version":"1.0.0","resolved":"https://registry.npmjs.org/union-value/-/union-value-1.0.0.tgz","integrity":"sha1-XHHDTLW61dzr4+oM0IIHulqhrqQ=","requires":{"arr-union":"^3.1.0","get-value":"^2.0.6","is-extendable":"^0.1.1","set-value":"^0.4.3"},"dependencies":{"extend-shallow":{"version":"2.0.1","resolved":"https://registry.npmjs.org/extend-shallow/-/extend-shallow-2.0.1.tgz","integrity":"sha1-Ua99YUrZqfYQ6huvu5idaxxWiQ8=","requires":{"is-extendable":"^0.1.0"}},"set-value":{"version":"0.4.3","resolved":"https://registry.npmjs.org/set-value/-/set-value-0.4.3.tgz","integrity":"sha1-fbCPnT0i3H945Trzw79GZuzfzPE=","requires":{"extend-shallow":"^2.0.1","is-extendable":"^0.1.1","is-plain-object":"^2.0.1","to-object-path":"^0.3.0"}}}},"unique-string":{"version":"1.0.0","resolved":"https://registry.npmjs.org/unique-string/-/unique-string-1.0.0.tgz","integrity":"sha1-nhBXzKhRq7kzmPizOuGHuZyuwRo=","requires":{"crypto-random-string":"^1.0.0"}},"unset-value":{"version":"1.0.0","resolved":"https://registry.npmjs.org/unset-value/-/unset-value-1.0.0.tgz","integrity":"sha1-g3aHP30jNRef+x5vw6jtDfyKtVk=","requires":{"has-value":"^0.3.1","isobject":"^3.0.0"},"dependencies":{"has-value":{"version":"0.3.1","resolved":"https://registry.npmjs.org/has-value/-/has-value-0.3.1.tgz","integrity":"sha1-ex9YutpiyoJ+wKIHgCVlSEWZXh8=","requires":{"get-value":"^2.0.3","has-values":"^0.1.4","isobject":"^2.0.0"},"dependencies":{"isobject":{"version":"2.1.0","resolved":"https://registry.npmjs.org/isobject/-/isobject-2.1.0.tgz","integrity":"sha1-8GVWEJaj8dou9GJy+BXIQNh+DIk=","requires":{"isarray":"1.0.0"}}}},"has-values":{"version":"0.1.4","resolved":"https://registry.npmjs.org/has-values/-/has-values-0.1.4.tgz","integrity":"sha1-bWHeldkd/Km5oCCJrThL/49it3E="}}},"upath":{"version":"1.1.2","resolved":"https://registry.npmjs.org/upath/-/upath-1.1.2.tgz","integrity":"sha512-kXpym8nmDmlCBr7nKdIx8P2jNBa+pBpIUFRnKJ4dr8htyYGJFokkr2ZvERRtUN+9SY+JqXouNgUPtv6JQva/2Q=="},"update-notifier":{"version":"3.0.0","resolved":"https://registry.npmjs.org/update-notifier/-/update-notifier-3.0.0.tgz","integrity":"sha512-6Xe3oF2bvuoj4YECUc52yxVs94yWrxwqHbzyveDktTS1WhnlTRpNcQMxUshcB7nRVGi1jEXiqL5cW1S5WSyzKg==","requires":{"boxen":"^3.0.0","chalk":"^2.0.1","configstore":"^4.0.0","has-yarn":"^2.1.0","import-lazy":"^2.1.0","is-ci":"^2.0.0","is-installed-globally":"^0.1.0","is-npm":"^3.0.0","is-yarn-global":"^0.3.0","latest-version":"^5.0.0","semver-diff":"^2.0.0","xdg-basedir":"^3.0.0"},"dependencies":{"ansi-styles":{"version":"3.2.1","resolved":"https://registry.npmjs.org/ansi-styles/-/ansi-styles-3.2.1.tgz","integrity":"sha512-VT0ZI6kZRdTh8YyJw3SMbYm/u+NqfsAxEpWO0Pf9sq8/e94WxxOpPKx9FR1FlyCtOVDNOQ+8ntlqFxiRc+r5qA==","requires":{"color-convert":"^1.9.0"}},"chalk":{"version":"2.4.2","resolved":"https://registry.npmjs.org/chalk/-/chalk-2.4.2.tgz","integrity":"sha512-Mti+f9lpJNcwF4tWV8/OrTTtF1gZi+f8FqlyAdouralcFWFQWF2+NgCHShjkCb+IFBLq9buZwE1xckQU4peSuQ==","requires":{"ansi-styles":"^3.2.1","escape-string-regexp":"^1.0.5","supports-color":"^5.3.0"}},"supports-color":{"version":"5.5.0","resolved":"https://registry.npmjs.org/supports-color/-/supports-color-5.5.0.tgz","integrity":"sha512-QjVjwdXIt408MIiAqCX4oUKsgU2EqAGzs2Ppkm4aQYbjm+ZEWEcW4SfFNTr4uMNZma0ey4f5lgLrkB0aX0QMow==","requires":{"has-flag":"^3.0.0"}}}},"urix":{"version":"0.1.0","resolved":"https://registry.npmjs.org/urix/-/urix-0.1.0.tgz","integrity":"sha1-2pN/emLiH+wf0Y1Js1wpNQZ6bHI="},"url-parse-lax":{"version":"3.0.0","resolved":"https://registry.npmjs.org/url-parse-lax/-/url-parse-lax-3.0.0.tgz","integrity":"sha1-FrXK/Afb42dsGxmZF3gj1lA6yww=","requires":{"prepend-http":"^2.0.0"}},"use":{"version":"3.1.1","resolved":"https://registry.npmjs.org/use/-/use-3.1.1.tgz","integrity":"sha512-cwESVXlO3url9YWlFW/TA9cshCEhtu7IKJ/p5soJ/gGpj7vbvFrAY/eIioQ6Dw23KjZhYgiIo8HOs1nQ2vr/oQ=="},"util":{"version":"0.10.4","resolved":"https://registry.npmjs.org/util/-/util-0.10.4.tgz","integrity":"sha512-0Pm9hTQ3se5ll1XihRic3FDIku70C+iHUdT/W926rSgHV5QgXsYbKZN8MSC3tJtSkhuROzvsQjAaFENRXr+19A==","requires":{"inherits":"2.0.3"}},"util-deprecate":{"version":"1.0.2","resolved":"https://registry.npmjs.org/util-deprecate/-/util-deprecate-1.0.2.tgz","integrity":"sha1-RQ1Nyfpw3nMnYvvS1KKJgUGaDM8="},"which":{"version":"1.3.1","resolved":"https://registry.npmjs.org/which/-/which-1.3.1.tgz","integrity":"sha512-HxJdYWq1MTIQbJ3nw0cqssHoTNU267KlrDuGZ1WYlxDStUtKUhOaJmh112/TZmHxxUfuJqPXSOm7tDyas0OSIQ==","requires":{"isexe":"^2.0.0"}},"widest-line":{"version":"2.0.1","resolved":"https://registry.npmjs.org/widest-line/-/widest-line-2.0.1.tgz","integrity":"sha512-Ba5m9/Fa4Xt9eb2ELXt77JxVDV8w7qQrH0zS/TWSJdLyAwQjWoOzpzj5lwVftDz6n/EOu3tNACS84v509qwnJA==","requires":{"string-width":"^2.1.1"}},"wrappy":{"version":"1.0.2","resolved":"https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz","integrity":"sha1-tSQ9jz7BqjXxNkYFvA0QNuMKtp8="},"write-file-atomic":{"version":"2.4.3","resolved":"https://registry.npmjs.org/write-file-atomic/-/write-file-atomic-2.4.3.tgz","integrity":"sha512-GaETH5wwsX+GcnzhPgKcKjJ6M2Cq3/iZp1WyY/X1CSqrW+jVNM9Y7D8EC2sM4ZG/V8wZlSniJnCKWPmBYAucRQ==","requires":{"graceful-fs":"^4.1.11","imurmurhash":"^0.1.4","signal-exit":"^3.0.2"}},"xdg-basedir":{"version":"3.0.0","resolved":"https://registry.npmjs.org/xdg-basedir/-/xdg-basedir-3.0.0.tgz","integrity":"sha1-SWsswQnsqNus/i3HK2A8F8WHCtQ="},"yallist":{"version":"2.1.2","resolved":"https://registry.npmjs.org/yallist/-/yallist-2.1.2.tgz","integrity":"sha1-HBH5IY8HYImkfdUS+TxmmaaoHVI="}}}]]></content>
  </entry>
  <entry>
    <title><![CDATA[-Java]]></title>
    <url>%2F2019%2F06%2F10%2F%E5%B9%B6%E5%8F%91-Java%2F</url>
    <content type="text"><![CDATA[&lt;Java &gt;-        javaIO,java java    Runnable Thread classRunnableThread executorExecutor  CallableRunnable,CallableFuture,get() sleep   run()CPU yield() yield,yield (daemon),  Java - Java cpu                  java]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[javaIO]]></title>
    <url>%2F2019%2F06%2F04%2FjavaIO%2F</url>
    <content type="text"><![CDATA[JAVA IO ]]></content>
  </entry>
  <entry>
    <title><![CDATA[aTemp]]></title>
    <url>%2F2019%2F06%2F03%2FaTemp%2F</url>
    <content type="text"><![CDATA[ 1.2.3.4.5.6.6/3  ]]></content>
      <categories>
        <category>temp</category>
      </categories>
      <tags>
        <tag>temp file</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[o21bigdata.md]]></title>
    <url>%2F2019%2F05%2F28%2Fo21bigdata-md%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[skills]]></title>
    <url>%2F2019%2F05%2F27%2Fskills%2F</url>
    <content type="text"><![CDATA[   -Network File SystemNFS   RPC ]]></content>
      <tags>
        <tag>skills</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[imagetest]]></title>
    <url>%2F2019%2F05%2F27%2Fimagetest%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2019%2F05%2F08%2Fos%2F</url>
    <content type="text"><![CDATA[2019 05 08  09:53:05 CST]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 ]]></title>
    <url>%2F2019%2F05%2F07%2F2018%E5%8D%81%E6%9C%88%2F</url>
    <content type="text"><![CDATA[ToDo  json9key  setting up JVM  2018 10 08   CPU//USB/BIOS// 2018 10 09  09:40:08 CST    2018 10 10  14:01:51 CST 2018 10 16  14:54:35 CSTgradle 2018 10 17  14:54:35 CST socketsocket4BDS UNIXIP InternetSocketSocket220 110  2018 10 18  10:27:01 CST 2018 10 19  10:45:09 CST kafka-stream   11/21kafka   Spark-streaming storm kafka stream  flink IDEA Tranlation plugin repointellgin       Java Map Java  java  Driver 2018 10 22  15:23:46 CST java  java  bean hadoop 2.7.7 yarnok(spark hadoop-client lib(hdfsyarn)) spark 2.3.2(hadoop 2.7) ok hbase ok kafka hadoop 3 hadoop 2 spark 2hadoop2.7.7. spark on yarn  stand alone  2018 10 23  11:31:33 CST sbt build.sbt  1resolvers += &quot;cdh&quot; at &quot;https://repository.cloudera.com/artifactory/libs-release-local/&quot; Resolvers  cdh cdh  hexo gitpage github  port in user / address in use solve: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//netstat -ln |grep ****(4000)// process IDlsof -i:****(4000)kill -9 ID``` cdh xml -&gt;  -&gt; action -&gt; Download Client configurationzip ------hadoop api hadoop-commonhadoop-hdfshadoop-client---2018 10 24  09:08:33 CSTjps SparkSubmit ps -ef |grep spark SparkSubmit why?[jps](https://stackoverflow.com/questions/29990153/submit-kill-spark-application-program-programmatically-from-another-applicatio) think pad e540 cpu [cpu](https://blog.csdn.net/sycflash/article/details/6643492) cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 8 Intel(R) Core(TM) i7-4702MQ CPU @ 2.20GHz12 memory: sudo dmidecode -t memory dmidecode 3.1Getting SMBIOS data from sysfs.SMBIOS 2.7 present. Handle 0x0005, DMI type 16, 23 bytesPhysical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: None Maximum Capacity: 16 GB Error Information Handle: Not Provided Number Of Devices: 2 Handle 0x0006, DMI type 17, 34 bytesMemory Device Array Handle: 0x0005 Error Information Handle: Not Provided Total Width: 64 bits Data Width: 64 bits Size: 8192 MB Form Factor: SODIMM Set: None Locator: ChannelA-DIMM0 Bank Locator: BANK 0 Type: DDR3 Type Detail: Synchronous Speed: 1600 MT/s Manufacturer: Hynix/Hyundai Serial Number: 08C66216 Asset Tag: None Part Number: HMT41GS6AFR8A-PB Rank: Unknown Configured Clock Speed: 1600 MT/s Handle 0x0007, DMI type 17, 34 bytesMemory Device Array Handle: 0x0005 Error Information Handle: Not Provided Total Width: Unknown Data Width: Unknown Size: No Module Installed Form Factor: DIMM Set: None Locator: ChannelB-DIMM0 Bank Locator: BANK 2 Type: Unknown Type Detail: None Speed: Unknown Manufacturer: Not Specified Serial Number: Not Specified Asset Tag: None Part Number: Not Specified Rank: Unknown Configured Clock Speed: Unknown 12345678910111213141516171819202122232425262728293031323334353637388g 1.35v cpu---hive1.2.2 HIVE_HOMEPATHHADOOP_HOMEhiveserver2kill ---top:htop------2018 10 25  09:46:38 CSTsqoop  hive  8g4g---ulimit  vim /etc/security/limits.conf // soft nofile 65535 hard nofile 65535 12345678[ulimit ](http://coolnull.com/2796.html)---hadoop   hdfs  dfs.namenode.name.dir 123456789/tmp hdfs namenode -format ,namenode namenode --- ---mysql  sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf [mysqld] skip-grant-tables service mysql restart mysql  use mysqlupdate mysql.user set authentication_string=password(admin123) where user=root and Host = localhost;update user set plugin=mysql_native_password where User=root; # THIS LINEflush privileges;exit; mysql, 12345[](https://blog.csdn.net/yelllowcong/article/details/79641313)[ERROR 1524 (HY000): Plugin &apos;auth_socket&apos; is not loaded](https://stackoverflow.com/questions/37879448/mysql-fails-on-mysql-error-1524-hy000-plugin-auth-socket-is-not-loaded)allow extenal in GRANT ALL PRIVILEGES ON . TO root@% IDENTIFIED BY Admin@321 WITH GRANT OPTION; 12345678910111213141516 bind_address[Grand ](https://blog.csdn.net/Plus_RE/article/details/79649427)------sqoop install &amp;&amp; test sqoop list-databases connect jdbc:mysql://localhost:3306 username root password admin123 $ sqoop import connect jdbc:mysql://localhost:3306/mysql username root password admin123 table user target-dir /sqoop/data delete-target-dir num-mappers 1 fields-terminated-by \t 12345---github  sublime text 3 liscens  BEGIN LICENSE sgbteamSingle User LicenseEA7E-11532598891CBB9 F1513E4F 1A3405C1 A865D53F115F202E 7B91AB2D 0D2A40ED 352B269B76E84F0B CD69BFC7 59F2DFEF E267328F215652A3 E88F9D8F 4C38E3BA 5B2DAAE4969624E7 DC9CD4D5 717FB40C 1B9738CF20B3C4F1 E917B5B3 87C38D9C ACCE7DD85F7EF854 86B9743C FADC04AA FB0DA5C0F913BE58 42FEA319 F954EFDD AE881E0B END LICENSE  1234567891011ctrl+c------2018 10 29  15:23:31 CSTshell nohup   /dev/null   --- --- 2018 10 30  16:31:13 CST github --- 2018 10 31  13:51:25 CST ? vim  ? samza ---]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F02%2Fscheduler%2F</url>
    <content type="text"><![CDATA[kafka 2019 03 20  15:17:45 CSTingjavaingscalaingjvmjvmjvmtodays todo list priority item starttime endtime ok remarks 1  08:30 09:00 ok ]]></content>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2019%2F03%2F26%2F%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[idea : psvm: public static void main sout: System.out.println fori: for]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2019%2F03%2F25%2F%E6%B5%81%2F</url>
    <content type="text"><![CDATA[stream  source/sinkpublisher-subscriber   pullpushpushpull   backpressure           reactive streams()  pullpush pullpush  http://www.reactive-streams.org/ reactive stream Java APIJava API PublisherSubscriberSubscriptionProcessor&lt;T,R&gt; publisher  subscriber subscription token     subscription      processor ProcessorPublisherSubscriber  Processor&lt;T,R&gt;TR   JDK 9 APIJDK 9Publisher RxJavaJava 123456789101112131415161718192021222324252627import java.util.concurrent.CompletableFuture;import java.util.concurrent.ExecutionException;import java.util.concurrent.SubmissionPublisher;import java.util.stream.LongStream;public class ReactiveStreamsTest &#123; public static void main(String args[]) &#123; CompletableFuture&lt;Void&gt; subTask = null; try (SubmissionPublisher&lt;Long&gt; pub = new SubmissionPublisher&lt;&gt;()) &#123; System.out.println(&quot;Subscriber Buffer SizeL&quot; + pub.getMaxBufferCapacity()); subTask = pub.consume(System.out::println); LongStream.range(1L, 2566L).forEach( l -&gt; pub.submit(l)); &#125; if (subTask != null)&#123; try&#123; subTask.get(); &#125;catch (InterruptedException | ExecutionException e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; akka-streamskafka kafka streamsspark Streamingflink]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 ]]></title>
    <url>%2F2019%2F03%2F11%2F2019May%2F</url>
    <content type="text"><![CDATA[2019 03 08  15:37:20 CST   ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2019%2F02%2F25%2Fpinyi%2F</url>
    <content type="text"><![CDATA[5TA06133NC01 25.7 5TA06213NC01 28.3 5TA06233NC01 34.0 5TA06318NC01 38.6 5TA06338NC01 45.1 5TA06414NC01 55.6 5TA06434NC01 64.1 5TA86153NC01 70.4 5UB06183NC01 29.5 5UB06442NC01 34.2 5UB06483NC01 41.7 5UB06153NC01 25.6 5UB06193NC01 34.7 5UB86163NC01 111.0 5UB06312NC01 24.2 5TG06321NC01 75.5 5TG06322NC01 113.4 5TG06311NC01 45.7 5TG06312NC01 82.0 5TG06331NC01 32.5 5TG06342NC01 58.0 5TG06362NC01 87.7 5TG06372NC01 113.4]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala]]></title>
    <url>%2F2019%2F02%2F25%2Fscala%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[scala1.2.JVMJava from  3.4.FuturePromise java-  ,java  Runnablerun ThreadExecutorjava.utils.concurrent(Executor),Thread Callable sleepThread.yield()  jion() ]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag></tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning Akka]]></title>
    <url>%2F2019%2F02%2F12%2FLearningakka%2F</url>
    <content type="text"><![CDATA[OSOS JVMJVMJVMjava.lang.Thread  jvmscala main  akka Actor Actor  akka actor  object jingtaitiaojian { def main(args: Array[String]): Unit = { var i, j = 0 (1 to 10000).foreach(_ =&gt; i = i + 1) (1 to 10000).foreach(_ =&gt; Future(j = j + 1)) Thread.sleep(1000) println(s&quot;i $i and j $j&quot;) } } AKKAakka docsecurity announcementsakka remoteremote actor  actorref github akka clusterakka doc akka-remoteakka-clusterakka-remoteakka-remoteakka-cluster   VS Martin Fowler    akka  -.-?      akkaakka gossip  akka  Akka-Httpspray,typesafescala servlet lib,akkaakka-http,spraysprayakka-http  AKKA-HTTP , , 1024s ,1024s  , ,,akka-in-action : Akka-http (static html)(css)jsstackoverflow stackoverflow 12345678910import akka.http.scaladsl.model.StatusCodesval staticResources = (get &amp; pathPrefix(&quot;admin&quot;))&#123; (pathEndOrSingleSlash &amp; redirectToTrailingSlashIfMissing(StatusCodes.TemporaryRedirect)) &#123; getFromResource(&quot;admin/index.html&quot;) &#125; ~ &#123; getFromResourceDirectory(&quot;admin&quot;) &#125; &#125; getFromResourceDirectory  Completes GET requests with the content of the given classpath resource directory. For details refer to getFromDirectory which works the same way but obtaining the file from the filesystem instead of the applications classpath. Note that its not required to wrap this directive with get as this directive will only respond to GET requests. GET getFromDirectory getGET sbt-native-packager plugins.sbt 1addSbtPlugin(&quot;com.typesafe.sbt&quot; % &quot;sbt-native-packager&quot; % &quot;1.2.0&quot;) build.sbt 1234567891011121314151617181920import com.typesafe.sbt.packager.MappingsHelper.&#123;contentOf, directory&#125;enablePlugins(JavaServerAppPackaging)mainClass in Compile := Some(&quot;com.qinglongmu.dataplatform.WebServer&quot;)mappings in Universal ++= &#123; // optional example illustrating how to copy additional directory directory(&quot;scripts&quot;) ++ // copy configuration files to config directory contentOf(&quot;src/main/resources&quot;).toMap.mapValues(&quot;config/&quot; + _)&#125;// add &apos;config&apos; directory first in the classpath of the start script,// an alternative is to set the config file locations via CLI parameters// when starting the applicationscriptClasspath := Seq(&quot;../config/&quot;) ++ scriptClasspath.valuelicenses := Seq((&quot;CC0&quot;, url(&quot;http://creativecommons.org/publicdomain/zero/1.0&quot;))) config]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag></tag>
        <tag>Akka</tag>
        <tag>actor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 ]]></title>
    <url>%2F2019%2F02%2F11%2F2019February%2F</url>
    <content type="text"><![CDATA[2019 02 11  15:59:38 CST win10kms  1wget --no-check-certificate https://github.com/teddysun/across/raw/master/kms.sh &amp;&amp; chmod +x kms.sh &amp;&amp; ./kms.sh kms]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 ]]></title>
    <url>%2F2019%2F01%2F29%2F2019January%2F</url>
    <content type="text"><![CDATA[2019 01 03  15:42:15 CST producer consumer , &gt;1000/s new  KafkaProducer,.ProducerRecord 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 def main(args: Array[String]): Unit = &#123; val startTime = LocalDateTime.now() logger.info(s&quot;start time:$&#123;startTime&#125;&quot;) val props = new util.HashMap[String, Object]() props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBrokerList) //leader okrecall back props.put(ProducerConfig.ACKS_CONFIG, &quot;1&quot;) props.put(ProducerConfig.RETRIES_CONFIG, &quot;2&quot;) props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, &quot;1000&quot;) props.put(ProducerConfig.CLIENT_ID_CONFIG, &quot;testradio&quot;) props.put(ProducerConfig.BATCH_SIZE_CONFIG, &quot;163480&quot;) props.put(ProducerConfig.LINGER_MS_CONFIG, &quot;100&quot;) props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, &quot;500000000&quot;) props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;) props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;) val producer = new KafkaProducer[Int, Int](props) val randomer = new Random(100) try &#123; (0 to 1000000).foreach(_ =&gt; productData2Kafka(producer, &quot;ini-t2&quot;, randomer.nextInt, randomer.nextInt)) &#125; catch &#123; case e: Exception =&gt; logger.error(e.getMessage) false &#125; finally &#123; if (producer != null) producer.close() &#125; val endTime = LocalDateTime.now() logger.info(s&quot;end time:$&#123;endTime&#125;&quot;) logger.info(s&quot;spend time:$&#123;startTime&#125; to $endTime&quot;) &#125;////////////////////////////////////////////////////////////////////////////////// def productData2Kafka(producer: KafkaProducer[Int, Int], topicName: String, key: Int, value: Int) = &#123; // Zookeeper connection properties //logger.info(s&quot;producer: $&#123;producer.toString&#125;&quot;) //val producer = new KafkaProducer[String, MeasurementItems](props) // Send messages val topic = topicName //val message = new ProducerRecord[String, MeasurementItems](topic, null, measurementItems) val message = new ProducerRecord[Int, Int](topic, key, value) //val message = new ProducerRecord[String, String](topic, null, measurementItems.UUID) //val message = new ProducerRecord[String, MeasurementItems](topic, null, measurementItems) //val future = producer.send(message) val callback = new KafkaProducerCallback producer.send(message, callback) //val res = future.get(5,SECONDS) //logger.info(&quot;res:&quot;+res.toString) //true &#125;/////////////////////////////////////////////////////////////////////////////////////////////class KafkaProducerCallback extends Callback &#123; override def onCompletion(metadata: RecordMetadata, exception: Exception): Unit = &#123; if (exception==null)&#123; logger.debug(s&quot;send message $&#123;metadata.offset()&#125; to $&#123;metadata.topic()&#125; success&quot;) &#125;else&#123; logger.error(&quot;121&quot;+exception.getMessage) throw exception &#125; &#125;&#125; 2019 01 04  10:33:48 CST2019 01 09  17:35:15 CST  2019 01 13  00:05:22 CST 2019 01 29  10:32:59 CST pyCharm  1SSUJFAQGMI-eyJsaWNlbnNlSWQiOiJTU1VKRkFRR01JIiwibGljZW5zZWVOYW1lIjoiWmhpd2VpIEhvbmciLCJhc3NpZ25lZU5hbWUiOiIiLCJhc3NpZ25lZUVtYWlsIjoiIiwibGljZW5zZVJlc3RyaWN0aW9uIjoiRm9yIGVkdWNhdGlvbmFsIHVzZSBvbmx5IiwiY2hlY2tDb25jdXJyZW50VXNlIjpmYWxzZSwicHJvZHVjdHMiOlt7ImNvZGUiOiJJSSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IkFDIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiRFBOIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiUFMiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJHTyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IkRNIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiQ0wiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSUzAiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSQyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IlJEIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiUEMiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJSTSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IldTIiwicGFpZFVwVG8iOiIyMDE5LTEwLTIxIn0seyJjb2RlIjoiREIiLCJwYWlkVXBUbyI6IjIwMTktMTAtMjEifSx7ImNvZGUiOiJEQyIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9LHsiY29kZSI6IlJTVSIsInBhaWRVcFRvIjoiMjAxOS0xMC0yMSJ9XSwiaGFzaCI6IjEwNjQ1NTE3LzAiLCJncmFjZVBlcmlvZERheXMiOjAsImF1dG9Qcm9sb25nYXRlZCI6ZmFsc2UsImlzQXV0b1Byb2xvbmdhdGVkIjpmYWxzZX0=-eNTyizE3kmBWEVd8daP6msWpn1/6mapFOi/fYBbc8LokedHKs0W1P+RNBR7eWPuD8efGE0EI00CydiPSOz+7qFHMaW69aW/2x5JTH3Nb6qIH9qVWCZDi1Sb5BDQxpen5OUVGks6rOtaNkOIAhQMbZyKTEQDd9rg0hUEY0BxhwDdR1zWlCWFL9h0smFWqncVvvt5wX09W4WnepJ+wYvUOgW0gPJTwV1NsCoa5hfgh5tVOKqfiuT3uD1QYYKh1Q6DYAKDMpkkObEt6BAwg7Gdg4MV7/f4R01RSRaZm7JJuoECeRSswzMLipDLMeAXTEAeHOumgZVsofvkhYAGQUuvNXA==-MIIEPjCCAiagAwIBAgIBBTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTE1MTEwMjA4MjE0OFoXDTE4MTEwMTA4MjE0OFowETEPMA0GA1UEAwwGcHJvZDN5MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxcQkq+zdxlR2mmRYBPzGbUNdMN6OaXiXzxIWtMEkrJMO/5oUfQJbLLuMSMK0QHFmaI37WShyxZcfRCidwXjot4zmNBKnlyHodDij/78TmVqFl8nOeD5+07B8VEaIu7c3E1N+e1doC6wht4I4+IEmtsPAdoaj5WCQVQbrI8KeT8M9VcBIWX7fD0fhexfg3ZRt0xqwMcXGNp3DdJHiO0rCdU+Itv7EmtnSVq9jBG1usMSFvMowR25mju2JcPFp1+I4ZI+FqgR8gyG8oiNDyNEoAbsR3lOpI7grUYSvkB/xVy/VoklPCK2h0f0GJxFjnye8NT1PAywoyl7RmiAVRE/EKwIDAQABo4GZMIGWMAkGA1UdEwQCMAAwHQYDVR0OBBYEFGEpG9oZGcfLMGNBkY7SgHiMGgTcMEgGA1UdIwRBMD+AFKOetkhnQhI2Qb1t4Lm0oFKLl/GzoRykGjAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBggkA0myxg7KDeeEwEwYDVR0lBAwwCgYIKwYBBQUHAwEwCwYDVR0PBAQDAgWgMA0GCSqGSIb3DQEBCwUAA4ICAQC9WZuYgQedSuOc5TOUSrRigMw4/+wuC5EtZBfvdl4HT/8vzMW/oUlIP4YCvA0XKyBaCJ2iX+ZCDKoPfiYXiaSiH+HxAPV6J79vvouxKrWg2XV6ShFtPLP+0gPdGq3x9R3+kJbmAm8w+FOdlWqAfJrLvpzMGNeDU14YGXiZ9bVzmIQbwrBA+c/F4tlK/DV07dsNExihqFoibnqDiVNTGombaU2dDup2gwKdL81ua8EIcGNExHe82kjF4zwfadHk3bQVvbfdAwxcDy4xBjs3L4raPLU3yenSzr/OEur1+jfOxnQSmEcMXKXgrAQ9U55gwjcOFKrgOxEdek/Sk1VfOjvS+nuM4eyEruFMfaZHzoQiuw4IqgGc45ohFH0UUyjYcuFxxDSU9lMCv8qdHKm+wnPRb0l9l5vXsCBDuhAGYD6ss+Ga+aDY6f/qXZuUCEUOH3QUNbbCUlviSz6+GiRnt1kA9N2Qachl+2yBfaqUqr8h7Z2gsx5LcIf5kYNsqJ0GavXTVyWh7PYiKX4bs354ZQLUwwa/cG++2+wNWP+HtBhVxMRNTdVhSm38AknZlD+PTAsWGu9GyLmhti2EnVwGybSD2Dxmhxk3IPCkhKAK+pl0eWYGZWG3tJ9mZ7SowcXLWDFAk0lRJnKGFMTggrWjV8GYpw5bq23VmIqqDLgkNzuoog==]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 ]]></title>
    <url>%2F2019%2F01%2F14%2F2018%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[: 1. 2.(,,) 3.(,,) 4.tensorflow(python)java 5. 6.aws-ec2,aws-emr(,) 7.Django 8.,,kafka, 9.,: 1.(), ,,.]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[affinity readme]]></title>
    <url>%2F2019%2F01%2F11%2Faffinity%2F</url>
    <content type="text"><![CDATA[:  ###CircleCI    Http Layer        HOCON Avro   Keyspaces  Akka     Scala 2.11 JavaScript(affinity.js)  API kafka//materializer of state (log)    Cluster Architecture Akkaapi Zookeeper Kafka Akka Httpwebsocket RocksDbMemDbSimpleMap Avro Affinity  /avro univeresekafkaspark shell(kafkacase classesRDD)javascript,schemaschemaWeb:Akka HttpAPIweb-socket/avrodelta AffinityStreamHttp Stream/ HttpHTTP akka ! Tell flows? Ask flows  ?? Ask flows (Ask with retries)which() HttpStream GatewayAkka,Keyspaceactoractor - referenceKeyspace, StreamGatewayHttpGateway Keyspaceapache kafka - Affinity Universekafka -  Keyspace ActorPartition Actors - KeyspaceAkkaCoordinatorActors() R =N =/ - MemStore Keyspaces Actor KeyspaceKeyspace Keyspace -   R =N =/ - MemStore Keyspaces Actor KeyspaceKeyspace Keyspace -   Akka PatternsAkka ! Tell Akka ? Ask Akkaakka.patterns.ask io.amient.affinity.core.ack ?? Typed AskAskReply [T]T` Scatter [T]actoractorT ??3  TReply [T]ask () ! Future[T] pipeTo Http Layerakka apimemstoreJavaapiutiliities avro scala case class &lt; - &gt;schemaavroScala corescalajs-avroScala examples / ..Scala avroScalakafka / avro-formatter-kafka kafka avroScalakafka / avro-serde-kafka kafka produer kafkakafka / storage-kafka kafka / test-util-kafkaEmbeddedZooKeeperEmbeddedKafkaEmbeddedCfRegistry RocksStbMemStoreJavarocksdb sparkserdeCompactRDD avrows-clientWebJava]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>affinity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 ]]></title>
    <url>%2F2019%2F01%2F03%2F2018november%2F</url>
    <content type="text"><![CDATA[2018 11 01  10:07:33 CSTkafka stream  2018 11 02  09:52:44 CST ?  2018 11 05  10:51:19 CST  vim   /  n: N: vim  2018 11 06  10:42:27 CSThive json  hive  hive service hwi   -&gt; war -&gt; jar  hadoop hive cli set -v 2018 11 07  10:37:09 CST json sql   history  sqldistinctmrhash  scala   //TODO //java.lang.RuntimeException: Column type is neither timestamp nor date! incremental append check-column id merge-key id last-value 0 split-by id -m 8 hive query stuck at 99% If your query is getting stuck at 99% check out following options - Data skewness, if you have skewed data it might possible 1 reducer is doing all the work Duplicates keys on both side - If you have many duplicate join keys on both side your output might explode and query might get stuck One of your table is small try to use map join or if possible SMB join which is a huge performance gain over reduce side join Go to resource manager log and see amount of data job is accessing and writing. 2018 11 09  15:14:38 CST 2018 11 14  14:06:34 CST hash hash/ wiki:collision hash vs md5 MD5MD5 Message-Digest Algorithm12816hash value murmurhash3scala  LinearSeqLike.scala hashCode() murmurhash3,hash murmurhash Austin Appleby2008murmurhashMurMurHash33264128 MurMurHadoop hash SHA-224SHA-256SHA-384SHA-512SHA-2 SHA-1 SHA-2SHA-1  2018 11 16  11:52:40 CST? azkban job lets go &gt;&gt;&gt; 2018 11 17  17:43:59 CST todo hive json  hash  2018 11 21  10:29:31 CST org.apache.commons.langRandomStringUtilsrandomAlphanumeric(int length)length 12345String randomString=RandomStringUtils.randomAlphanumeric(10);println(RandomStringUtils.randomAlphabetic(10))println(RandomStringUtils.randomAlphanumeric(10))println(RandomStringUtils.randomAscii(10))   123456789def randomLonLat(): List[Double] =&#123; val lonRange = List(22.5114637164,22.5219235145) val latRange = List(113.9546695974,113.9576442599) val randomLon = Math.random() * (lonRange.last - lonRange.head)+ lonRange.head val randomLat = Math.random() * (latRange.last - latRange.head)+ latRange.head randomLon :: randomLat :: Nil &#125; java8 1LocalDateTime.now().toString date format 2018 11 22  10:17:23 CSTIBM: Java 8  Streams API  java8 stream Iterator,scala ** apt-get\   2018 11 27  10:36:52 CSTget akka-http static html\css\js,akka-http.md 12345678val staticResources = (get &amp; pathPrefix(&quot;elm&quot;))&#123; (pathEndOrSingleSlash &amp; redirectToTrailingSlashIfMissing(StatusCodes.TemporaryRedirect)) &#123; getFromResource(&quot;elm/index.html&quot;) &#125; ~ &#123; getFromResourceDirectory(&quot;elm&quot;) &#125; &#125; gitlab  2018 11 30  10:40:08 CST?Lineage ?   1   mvn install java8  scala 123456val (a,b) = (1,2) # can&apos;t be compiledval List(a,b) = List(1,2) # correctval (a,b) = Tuple2(1,2) # correct]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018 ]]></title>
    <url>%2F2019%2F01%2F03%2F2018December%2F</url>
    <content type="text"><![CDATA[2018 12 04  16:18:05 CSTubuntu18 2018 12 05  10:47:36 CST 2018 12 06  10:47:36 CSTapt-get scalarepl console  : apt-get  To fix the problem completely removed scala and install it with dpkg (not with apt): sudo apt-get remove scala-library scala sudo wget www.scala-lang.org/files/archive/scala-2.11.12.deb sudo dpkg -i scala-2.11.12.deb   repl. consolejava.lang.NumberFormatException: For input string: 0x100  export TERM=xterm-color  at 2019 01 03  09:56:26 CST: ,,ideaDebugger Object  md,, 2018 12 10  15:33:16 CSTidea, text 123456789101112131415161718192021222324252627282918/12/10 16:12:41 ERROR tool.ExportTool: Encountered IOException running export job:ENOENT: No such file or directory at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmodImpl(Native Method) at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmod(NativeIO.java:230) at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:652) at org.apache.hadoop.fs.FilterFileSystem.setPermission(FilterFileSystem.java:490) at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:599) at org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:94) at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:98) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:191) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1297) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1294) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1294) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1315) at org.apache.sqoop.mapreduce.ExportJobBase.doSubmitJob(ExportJobBase.java:324) at org.apache.sqoop.mapreduce.ExportJobBase.runJob(ExportJobBase.java:301) at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:442) at org.apache.sqoop.manager.SqlManager.exportTable(SqlManager.java:931) at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:80) at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252) mapreduce 2018 12 11  10:11:03 CSTjava8 666 12345678910111213141516171819202122232425Legacy:for (int i = 0; i &lt; 10; i++) &#123; System.out.println(i);&#125;Fancy:IntStream.range(0, 10).forEach( nbr -&gt; System.out.println(nbr));Why? ...because the execution of the following snippet takes 1 second and not 10 seconds:IntStream.range(0, 10).parallel().forEach( nbr -&gt; &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException ex) &#123;&#125; System.out.println(nbr); &#125;); tabtab,sed s/\t/,/g bdp_order_filterbznstype_12month.csv &gt; bdp_order_filterbznstype_comma_12months.csv wc -l bdp_order_comma_12months.csv linux 2018 12 12  14:02:12 CSTscala : 1234567891011def insertSort(xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List() case x :: xs1 =&gt; insert(x, insertSort(xs1)) &#125; def insert(x: Int, xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List(x) case y :: ys =&gt; if (x &lt;= y) x :: xs else y :: insert(x, ys) &#125; 1234567891011121314def bubblesort[A &lt;% Ordered[A]](list: List[A]): List[A] = &#123; def sort(as: List[A], bs: List[A]): List[A] = if (as.isEmpty) bs else bubble(as, Nil, bs) def bubble(as: List[A], zs: List[A], bs: List[A]): List[A] = as match &#123; case h1 :: h2 :: t =&gt; if (h1 &gt; h2) bubble(h1 :: t, h2 :: zs, bs) else bubble(h2 :: t, h1 :: zs, bs) case h1 :: Nil =&gt; sort(zs, h1 :: bs) &#125; sort(list, Nil)&#125; 2018 12 13  16:00:39 CSThive export -&gt; mysql  ubuntu7z666sudo apt-get install p7zip-full 7z x ****.7z c3p0\dbcp ali druid  //TODO sql  volatile: CC++C#Javavolatilevolatile/Cvolatilevolatile 2018 12 17  10:23:19 CSTdbcp+dbutilshive  kafka-connect -&gt; hdfs hive -&gt; kafka jdbc, scalaimplicit scala spray-jsoncase class  implict json , 1import spray.json._ 2018 12 18  09:26:13 CSTmaven: mvn cleanmvn install mvn clean install -e -U-e  -U Try: Try Success[A] Throwable  Failure[A] Option[A] Option[A]  Try[A]   A  Throwable   hive ThriftServer,jdbc,hiveserver2 12345678910111213141. hive server2 netstat -anp |grep 100002. conf/hive-site.xml  &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;192.168.206.128&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; hive export orc Can Sqoop export ORC? HOW TO SQOOP EXPORT A HIVE ORC TABLE TO A ORACLE DATABASE? Hope you must have some idea about Hive ORC tables. Lets create a ORC table here and feed some sample data in Hive. Similarly lets create a table structure in ORACLE in the meantime. We are going to use Sqoop-HCatalog Integration here. Just type sqoop export help in Bash and see what are all the sqoop parameter commands there for the Sqoop Export related to HCatalog. I got the details as below. HCatalog arguments: hcatalog-database HCatalog database name hcatalog-home Override $HCAT_HOME hcatalog-table HCatalog table name hive-home Override $HIVE_HOME hive-partition-key Sets the partition key to use when importing to hive hive-partition-value Sets the partition value to use when importing to hive map-column-hive Override mapping for specific column to hive types. 2018 12 19  14:34:26 CST ,mysql,full```1234567891011121314151617181920212223242526272829303132333435363738394041,,mysqldata  /var/lib/mysqlssd,[How to change MySQL data directory?](https://stackoverflow.com/questions/1795176/how-to-change-mysql-data-directory),mysqld /etc/mysql/mysql.conf.d/mysqld.cnf---kafka connectfile -&gt; kafka topicmysql -&gt; kafka topic----## 2018 12 20  15:35:29 CST[java &lt;-&gt; scala ](https://stackoverflow.com/questions/16583265/how-can-i-convert-scala-map-to-java-map-with-scala-float-to-java-float-k-v-conve),,,null,---[Join on foreign key in Kafka stream](https://stackoverflow.com/questions/53260817/join-on-foreign-key-in-kafka-stream)//TODO: kafka ProducerRecord usage: public class ProducerRecord&lt;K, V&gt; { private final String topic; private final Integer partition; private final Headers headers; private final K key; private final V value; private final Long timestamp;123456timestamp headers partitons---------------------------(PartialFuction[A,B]) -- {case 1 =&gt; 1case 2 =&gt; 2} 12scala () exp:(1,2) class:Tuple2 scala&gt; a: (Int, Int) = (0,10) scala&gt; res0: Class[_ &lt;: (Int, Int)] = class scala.Tuple2$mcII$sp 1234567## 2018 12 21  10:14:25 CST[Counting Number of messages stored in a kafka topic](https://stackoverflow.com/questions/41792703/counting-number-of-messages-stored-in-a-kafka-topic/41799011) bin/kafka-run-class.sh kafka.tools.GetOffsetShell broker-list localhost:9092,localhost:9093,localhost:9094 topic test-topic time -1 offsets 1 | awk -F : {sum += $3} END {print sum} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162---alt &lt;- :chrome -------------------[pdf \](https://www.ilovepdf.com/)---jmap(JVM Memory Map)heap dump-XX:+HeapDumpOnOutOfMemoryErrorOOMdump---ubuntu runlevel---## 2018 12 24  11:21:08 CSTsbt run ,sbtjvmOOM(GC error)build.sbt  :fork:=true,jvmbuild.sbtjavaoptions,sbt 0.13,,,sbt 1.x---//Todo:sbt dist javaoptions(jvm)kafka producer metadata---## 2018 12 26  17:30:22 CSTmain app ideabuild.run,sbt dist:java.lang.StackOverflowErroridea run main ,idea jvm run main ,jdk,jdkjvm,,max heap :4G,stack.sbt jvm,,,stack .: xxh@xxh-YangTianM4900c-00:/codeold/footinfoloaderbj$ whereis sbtsbt: /usr/local/sbt /home/xxh/bin/sbt /usr/local/sbt/bin/sbt /usr/local/sbt/bin/sbt.batxxh@xxh-YangTianM4900c-00:/codeold/footinfoloaderbj$ cd /usr/local/sbt/xxh@xxh-YangTianM4900c-00:/usr/local/sbt$ kkkkxxh@xxh-YangTianM4900c-00:/usr/local/sbt$ ;;bash:  `;; xxh@xxh-YangTianM4900c-00:/usr/local/sbt$ lbin/ conf/ lib/ sbt* xxh@xxh-YangTianM4900c-00:/usr/local/sbt$ cd conf/xxh@xxh-YangTianM4900c-00:/usr/local/sbt/conf$ ll 16drwxrwxr-x 2 xxh xxh 4096 12 4 17:40 ./drwxrwxr-x 5 xxh xxh 4096 12 4 17:37 ../-rw-rw-r 1 xxh xxh 255 12 4 17:40 sbtconfig.txt-rw-rw-r 1 xxh xxh 939 7 27 2017 sbtoptsxxh@xxh-YangTianM4900c-00:/usr/local/sbt/conf$ cat sbtopts The SBT Configuration file.Disable ANSI color codes# #-no-colors Starts sbt even if the current directory contains no sbt project.# -sbt-create Path to global settings/plugins directory (default: ~/.sbt)# #-sbt-dir /etc/sbt Path to shared boot directory (default: ~/.sbt/boot in 0.11 series)# #-sbt-boot ~/.sbt/boot Path to local Ivy repository (default: ~/.ivy2)# #-ivy ~/.ivy2 set memory options# #-mem Use local caches for projects, no sharing.# #-no-share Put SBT in offline mode.# #-offline Sets the SBT version to use.#-sbt-version 0.11.3 Scala version (default: latest release)# #-scala-home #-scala-version java version (default: java from PATH, currently $(java -version |&amp; grep version))# #-java-home xxh@xxh-YangTianM4900c-00:/usr/local/sbt/conf$ vim sbtopts =&gt; The SBT Configuration file.Disable ANSI color codes# #-no-colors Starts sbt even if the current directory contains no sbt project.# -sbt-create Path to global settings/plugins directory (default: ~/.sbt)# #-sbt-dir /etc/sbt Path to shared boot directory (default: ~/.sbt/boot in 0.11 series)# #-sbt-boot ~/.sbt/boot Path to local Ivy repository (default: ~/.ivy2)# #-ivy ~/.ivy2 set memory options# #-mem Use local caches for projects, no sharing.# #-no-share Put SBT in offline mode.# #-offline Sets the SBT version to use.#-sbt-version 0.11.3 Scala version (default: latest release)# #-scala-home #-scala-version java version (default: java from PATH, currently $(java -version |&amp; grep version))# #-java-home # ###################-J-Xmx6G-J-Xss2M ################### sbt dist ,ok --- &quot;|&quot;,,String.split : &gt; 1split* ^ | \ 2 | 2 &quot;|&quot; --- # : - ,,, - akka,,oom,, akka,(akka),Future,Promise,java - java8,lambda,scalamap, - sed  - jvm - sbt0.13,sbt1.0 # TODO Jvm kafka ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F13%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[CAP - CAP 1 1.1  1.2 1.2.1 1.2.2 1.2.3 1.2.4 1.3 1.3.1 1.3.1.1 1.3.1.2 1.3.2 1.3.2.1 1.3.2.2 1.3.3 2 2.1      2.1 3 3.1 3.2 3.3 3.4 3.4.2 4 4 (remote procedure call RPC) (remote method invocation RMI) (message-oriented middleware MOM) (stream) 4.1 ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive ]]></title>
    <url>%2F2018%2F12%2F07%2Fhive%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[hive kafka hive row format(file format) hiveserde(serializer-deserializer) hive  hive conf tmp,hive-default.xml -&gt; hive-site.xml 12345678910hive-site.xml and hive-default.xml.templatehive-default.xml.template contains the default values for various configuration variables that come prepackaged in a Hive distribution. In order to override any of the values, create hive-site.xml instead and set the value in that file as shown above.hive-default.xml.template is located in the conf directory in your installation root, and hive-site.xml should also be created in the same directory.Please note that the template file hive-default.xml.template is not used by Hive at all (as of Hive 0.9.0)  the canonical list of configuration options is only managed in the HiveConf java class. The template file has the formatting needed for hive-site.xml, so you can paste configuration variables from the template file into hive-site.xml and then change their values to the desired configuration.In Hive releases 0.9.0 through 0.13.1, the template file does not necessarily contain all configuration options found in HiveConf.java and some of its values and descriptions might be out of date or out of sync with the actual values and descriptions. However, as of Hive 0.14.0 the template file is generated directly from HiveConf.java and therefore it is a reliable source for configuration variables and their defaults.The administrative configuration variables are listed below. User variables are listed in Hive Configuration Properties. As of Hive 0.14.0 you can display information about a configuration variable with the SHOW CONF command. hive jdbc  sbt mvn(jdbc) Class.forName DriverManager. mysql  our password does not satisfy the current policy requirements 1234Here is what I do to remove the validate password plugin:1. Login to the mysql server as root mysql -h localhost -u root -p2. Run the following sql command: uninstall plugin validate_password; hive hive hive 2  beeline&gt; !connect jdbc:hive2//localhost:10000scan complete in 0ms18/11/05 15:06:11 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: minlog-1.2.jar (No such file or directory)18/11/05 15:06:11 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: objenesis-1.2.jar (No such file or directory)18/11/05 15:06:11 [main]: ERROR beeline.ClassNameCompleter: Fail to parse the class name from the Jar file due to the exception:java.io.FileNotFoundException: reflectasm-1.07-shaded.jar (No such file or directory)scan complete in 670msNo known driver to handle jdbc:hive2//localhost:10000  jdbchive2beeline&gt; !connect jdbc:hive2://h2slave1:10000 jdbc   1Required field &apos;client_protocol&apos; is unset  123456789//jdbc lib  server clientserverserver1.2.2libraryDependencies += &quot;org.apache.hive&quot; % &quot;hive-jdbc&quot; % &quot;2.1.1&quot;-&gt;libraryDependencies += &quot;org.apache.hive&quot; % &quot;hive-jdbc&quot; % &quot;1.2.2&quot; hive sql hdfs hive service hwi   -&gt; war -&gt; jar  json  json  hive Hive table creation with a default value hive  12345describe extended tablename-- desc formatted tablename-- show create table tablename; --table ? alter table *** location  ]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database,hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Amazon Redshift Learning]]></title>
    <url>%2F2018%2F12%2F04%2FAmazon%20Redshift%20Learning%2F</url>
    <content type="text"><![CDATA[overview redshift   ,Redshift  :  Amazon Redshift is based on PostgreSQL 8.0.2. postgresql vs mysqlsql]]></content>
      <categories>
        <category> - database</category>
      </categories>
      <tags>
        <tag>Redshift</tag>
        <tag>Amazon</tag>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bandwagon shadowsocks ]]></title>
    <url>%2F2018%2F12%2F04%2FBandwagon-shadowsocks-%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[socks5 bandwagon vps VPS(): BWH1ZBPVK6.00% bandwagonvps,(alipay),SPECIAL 10G KVM PROMO V3 - LOS ANGELES - CN2 shadowsocks server bashvps,shadowsocks docpythonpipvpsosgoogle() shadowsocks serverblog 1234567891011vim /etc/shadowsocks/config.json&#123; &quot;server&quot;:&quot;::&quot;, &quot;server_port&quot;:8388, // &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;mypassword&quot;,//shadowsocksclient &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, &quot;fast_open&quot;: false&#125;  1ssserver -c /etc/shadowsocks/config.json nohup &amp;  shadowsocks clientshadowsocks  conf: 123456789&#123; &quot;server&quot;:&quot;112.11.11.11&quot;, //vps ip &quot;server_port&quot;:8032, // shadowsocks port &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:34196,//  &quot;password&quot;:&quot;0juAtjMZVQ&quot;, //shadowsocks password &quot;timeout&quot;:600, &quot;method&quot;:&quot;aes-256-cfb&quot; //method &#125; 1nohup /usr/bin/python /usr/bin/sslocal -c /etc/shadowsocks.json &amp; More info: Deployment]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>Bandwagon</tag>
        <tag>shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIY Cluster]]></title>
    <url>%2F2018%2F12%2F04%2FDIY-cluster%2F</url>
    <content type="text"><![CDATA[ akka clusterdispatcher, descactor akka cluster   ,,,, (),(node)(member), vsMartin Fowler 1.  2.  , , ,    ()  akka gossip,,,, CAPC A p CAP Akka Cluster Akka Cluster  Akka Remoting, Akka Remoting alphaakka-samples-remote-scala  look up  ssh devopsansibleansibleaws]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Future&Promise]]></title>
    <url>%2F2018%2F12%2F04%2FFuture%26promise%2F</url>
    <content type="text"><![CDATA[FuturePromisescala FutureActor  What is Future scala.concurrent  Future[T]  T    future  Future   future  Future  Promise API   Promise ]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>Future</tag>
        <tag>Promise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[H2]]></title>
    <url>%2F2018%2F12%2F04%2FH2%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[H2 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859object H2Database extends LogSupport &#123; private var server: Server = null; private var webServer: Server = null; def start(): Unit = &#123; try &#123; Server.shutdownTcpServer(s&quot;tcp://$&#123;ProConfig.h2DatabaseHost&#125;:$&#123;ProConfig.h2DatabaseTcpPort&#125;&quot;, &quot;&quot;, true, true) &#125; catch &#123; case e: Throwable =&gt; &#125; try &#123; server = Server.createTcpServer(&quot;-tcp&quot;, &quot;-tcpAllowOthers&quot;, &quot;-tcpPort&quot;, ProConfig.h2DatabaseTcpPort.toString).start() log.info(&quot;server status: &quot; + server.getStatus) &#125; catch &#123; case e: Throwable =&gt; log.error(e.getMessage, e) &#125; &#125; def startWebConsole(openBrowser: Boolean = false): Unit = &#123; try &#123; if (webServer == null) &#123; webServer = Server.createWebServer(&quot;-web&quot;, &quot;-webAllowOthers&quot;, &quot;-webPort&quot;, ProConfig.h2DatabaseWebPort.toString).start() &#125; log.info(&quot;webServerStatus&quot; + webServer.getStatus) if (openBrowser) Server.openBrowser(s&quot;http://$&#123;ProConfig.h2DatabaseHost&#125;:$&#123;ProConfig.h2DatabaseWebPort&#125;&quot;) &#125; catch &#123; case e: Throwable =&gt; log.error(e.getMessage, e) &#125; &#125; def stop(): Unit = &#123; def shutdownServer(server: Server): Unit = &#123; try &#123; server match &#123; case s if s != null =&gt; s.stop() case _ =&gt; log.info(&quot;H2 server did not start&quot;) &#125; &#125; catch &#123; case e: Throwable =&gt; log.error(e.getMessage, e) &#125; &#125; shutdownServer(server) shutdownServer(webServer) try&#123; Server.shutdownTcpServer(s&quot;tcp://$&#123;ProConfig.h2DatabaseHost&#125;:$&#123;ProConfig.h2DatabaseTcpPort&#125;&quot;,&quot;&quot;,true,true) log.debug(&quot;serverStatus: &quot;+server.getStatus) &#125;catch &#123; case e:Throwable =&gt; log.error(e.getMessage,e) &#125; &#125;&#125;object H2Test extends App &#123; H2Database.start() Thread.sleep(10000) H2Database.stop()&#125; JDBCH2JDBC  CSVWRITE:  CSV 1CALL CSVWRITE(&apos;/home/xxhbak/FEATURECONF.csv&apos;, &apos;SELECT * FROM FEATURECONF&apos;); CSVREAD:CSVCSV CSVREAD: 1SELECT * FROM CSVREAD(&apos;test.csv&apos;); Another option is to use INSERT INTO  SELECT. 123CREATE TABLE TEST AS SELECT * FROM CSVREAD(&apos;test.csv&apos;);CREATE TABLE TEST(ID INT PRIMARY KEY, NAME VARCHAR(255)) AS SELECT * FROM CSVREAD(&apos;test.csv&apos;); Please note for performance reason, CSVREAD should not be used inside a join. Instead, import the data first (possibly into a temporary table), create the required indexes if necessary, and then query this table.   CALL CSVREAD(/home/xd/us.csv, SELECT * FROM us );insert into us (SELECT * FROM CSVREAD(/home/xd/USERCASERESULT.csv,NULL,UTF-8,|));]]></content>
      <categories>
        <category> - database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>H2</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop  3th Edition Part 1]]></title>
    <url>%2F2018%2F12%2F04%2FHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97V3-p1%2F</url>
    <content type="text"><![CDATA[     (RAID,HDFS) MapReduce     BB RDBMShadoophadooprdbms RDBMSmapreduce RDBMS Table 1-1. RDBMS compared to MapReduce item Traditional RDBMS MapReduce Data size Gigabytes Petabytes Access() Interactive and batch Batch Updates Read and write many times Write once, read many times Structure Static schema Dynamic schema Integrity() High Low Scaling Nonlinear Linear RDBMS:hadoop:  mapreduce   CPU Hadoop o b hadoop echosystem common  avro  maoreduce  HDFS  hive sqlhdfs hbase  zookeeper  sqoop:rdbms &lt;=&gt; hdfs oozie  hadoop  Table 1-2. Features supported by Hadoop release series Feature 1.x 0.22 2.x Secure authentication Yes No Yes Old configuration names Yes Deprecated Deprecated New configuration names No Yes Yes Old MapReduce API Yes Yes Yes New MapReduce API Yes (with somemissing libraries) Yes Yes MapReduce 1 runtime (Classic) Yes Yes No MapReduce 2 runtime (YARN) No No Yes HDFS federation No No Yes HDFS high-availability No No Yes  2.1     2.2 Unix 12345678910#!/usr/bin/env bashfor year in all/*doecho -ne `basename $year .gz`&quot;\t&quot;gunzip -c $year | \awk &apos;&#123; temp = substr($0, 88, 5) + 0;q = substr($0, 93, 1);if (temp !=9999 &amp;&amp; q ~ /[01459]/ &amp;&amp; temp &gt; max) max = temp &#125;END &#123; print max &#125;&apos;done awk  2.3 hadoopmap: 1234567891011121314151617181920212223242526272829import java.io.IOException;importimportimportimportorg.apache.hadoop.io.IntWritable;org.apache.hadoop.io.LongWritable;org.apache.hadoop.io.Text;org.apache.hadoop.mapreduce.Mapper;public class MaxTemperatureMapperextends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123;private static final int MISSING = 9999;@Overridepublic void map(LongWritable key, Text value, Context context)throws IOException, InterruptedException &#123;&#125;&#125;String line = value.toString();String year = line.substring(15, 19);int airTemperature;if (line.charAt(87) == &apos;+&apos;) &#123; // parseInt doesn&apos;t like leading plus signsairTemperature = Integer.parseInt(line.substring(88, 92));&#125; else &#123;airTemperature = Integer.parseInt(line.substring(87, 92));&#125;String quality = line.substring(92, 93);if (airTemperature != MISSING &amp;&amp; quality.matches(&quot;[01459]&quot;)) &#123;context.write(new Text(year), new IntWritable(airTemperature));&#125; reduce: 1234567891011121314151617import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MaxTemperatureReducerextends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;@Overridepublic void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context)throws IOException, InterruptedException &#123;&#125;&#125;int maxValue = Integer.MIN_VALUE;for (IntWritable value : values) &#123;maxValue = Math.max(maxValue, value.get());&#125;context.write(key, new IntWritable(maxValue)); main: 123456789101112131415161718192021222324import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MaxTemperature &#123;public static void main(String[] args) throws Exception &#123;if (args.length != 2) &#123;System.err.println(&quot;Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;&quot;);System.exit(-1);&#125;Job job = new Job();job.setJarByClass(MaxTemperature.class);job.setJobName(&quot;Max temperature&quot;);FileInputFormat.addInputPath(job, new Path(args[0]));FileOutputFormat.setOutputPath(job, new Path(args[1]));job.setMapperClass(MaxTemperatureMapper.class);job.setReducerClass(MaxTemperatureReducer.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(IntWritable.class);System.exit(job.waitForCompletion(true) ? 0 : 1);&#125;&#125; 2.4 2.5 Hadoop Streaming]]></content>
      <categories>
        <category> - hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop  3th Edition Part 2]]></title>
    <url>%2F2018%2F12%2F04%2FHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97V3-p2%2F</url>
    <content type="text"><![CDATA[ Hadoop  3.1 HDFS,the design of hdfs HDFS        3.2 HDFS Concepts blocks HDFS hdfs  fsck 1hadoop fsck / -files -blocks namenode &amp; datanode namenode,datanode namenode   clientnamenodedatanode datanode namenode namenode  namenode HDFSnamenode HDFSHA hadoop fs -help hadoophadoop hdfsjavaorg.apache.hadoop.fs.FileSystem  javaFileSystem]]></content>
      <categories>
        <category> - hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase]]></title>
    <url>%2F2018%2F12%2F04%2FHbase%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[HBase    30 500vshbase:*      Version   HDFS       HbaseMap Table RowHBaserowrow keyrow keyrow key ColumnHBase Column Family() row key Column Qualifiercontentcontent:htmlcontent:pdf CellCell TimestampRegionServerCell getstart -&gt; conf/hbase-site.xml, 1234&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;  -&gt; conf/hbase-env.shzk 1export HBASE_MANAGES_ZK=false ]]></content>
      <categories>
        <category> - database</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[https Certificate]]></title>
    <url>%2F2018%2F12%2F04%2FHttps%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[aliyunhttps Certificateakka-httpaws ec2 certificatealiyun 1-rw-rw-r-- 1 xxh xxh 4083 12 6 17:22 214043592950638.zip unzip .key.pem 12-rw-rw-r-- 1 xxh xxh 1675 6 20 17:36 214043592950638.key-rw-rw-r-- 1 xxh xxh 3892 6 20 17:36 214043592950638.pem .pfx12openssl pkcs12 -export -out 214043592950638.pfx -inkey214043592950638.key -in 214043592950638.pem password.pfx-rw-rw-r-- 1 xxh xxh 4437 12 7 09:28 214043592950638.pfx pfxsbtapppfxresources webserver 12345678910111213141516171819202122232425262728293031323334353637383940object WebServer extends LogSupport&#123; implicit val system = ActorSystem() implicit val mat = ActorMaterializer() implicit val dispatcher = system.dispatcher def main(args: Array[String]): Unit = &#123; // Manual HTTPS configuration val password: Array[Char] = &quot;mima&quot;.toCharArray val ks: KeyStore = KeyStore.getInstance(&quot;PKCS12&quot;) val keystore: InputStream = getClass.getClassLoader.getResourceAsStream(&quot;214043592950638.pfx&quot;) require(keystore != null, &quot;Keystore required!&quot;) ks.load(keystore, password) val keyManagerFactory: KeyManagerFactory = KeyManagerFactory.getInstance(&quot;SunX509&quot;) keyManagerFactory.init(ks, password) val tmf: TrustManagerFactory = TrustManagerFactory.getInstance(&quot;SunX509&quot;) tmf.init(ks) val sslContext: SSLContext = SSLContext.getInstance(&quot;TLS&quot;) sslContext.init(keyManagerFactory.getKeyManagers, tmf.getTrustManagers, new SecureRandom) val https: HttpsConnectionContext = ConnectionContext.https(sslContext) val routes: Route = path(&quot;test&quot;)&#123; get &#123; logger.info(&quot;response hello&quot;) complete(&quot;Hello world!&quot;) &#125; &#125; // sets default context to HTTPS  all Http() bound servers for this ActorSystem will use HTTPS from now on Http().setDefaultServerHttpContext(https) Http().bindAndHandle(routes, &quot;0.0.0.0&quot;, 443, connectionContext = https) logger.info(&quot;WebServer start&quot;) &#125;&#125; testec2443 ps:routes Http()routesok rel  https]]></content>
      <categories>
        <category> - web</category>
      </categories>
      <tags>
        <tag>https</tag>
        <tag>Certificate</tag>
        <tag>webapp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http]]></title>
    <url>%2F2018%2F12%2F04%2FHttp%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[HttpPostman Postman PostmanJSON Postman JSONJSON:JavaScript Object NotationJSON is a syntax for storing and exchanging dataJSON is text,written with JavaScript object notationJSONw3school Http learningHttpHyperText Transfer P rotocol  GETPOSTGET  GET  GET  GET  GET  GET  GET POST POST POST  POST  POST  POST HTTP GET  POSTPOST RESTfulRESTful REST: resource representation state transfer]]></content>
      <categories>
        <category> - web</category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA ]]></title>
    <url>%2F2018%2F12%2F04%2FIntellij%20IDEA%20%E5%BF%AB%E6%8D%B7%E9%94%AE%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[ Ctrl+Shift + Enter   Ctrl+E Ctrl+Shift+E Shift+Click Ctrl+[ OR ] Ctrl+F12 Ctrl+F7 F3  Ctrl+N Ctrl+Shift+N Alt+Q Ctrl+P Ctrl+Shift+Insert Alt+Insert/Getter/Setter Ctrl+Alt+Vnew String();  Ctrl+Alt+Ttry/catch Ctrl+Enter Ctrl+Alt+L Ctrl+Alt+I JSP  Ctrl+Alt+O Ctrl+R Ctrl+F Ctrl+Shift+Space Ctrl+ Ctrl+Shift+Alt+N Alt+Shift+C Alt+Shift+Up/Down/ Shift+F6 -  Ctrl+X Ctrl+D Ctrl+/Ctrl+Shift+////**/ Ctrl+Jserr Ctrl+Alt+J Ctrl+H Ctrl+Q Alt+F1 Alt+1 Ctrl+Alt+left/right Alt+left/right Alt+Up/Down Ctrl+Shift+Up/Down/ F2  Shift+F2 Tab Tab Ctrl+Shift+F7 Esc  Alt+F3 Ctrl+Up/Down Ctrl+B/Ctrl+Click Ctrl+Alt+B Ctrl+Shift+Backspace Ctrl+O Ctrl+Alt+Space Ctrl+Alt+Up/Down Ctrl+Shift+J Alt+F8 Ctrl+Shift+V Ctrl+Alt+Shift+V Shift+Esc F12 Shift+F1 Java  Ctrl+W Ctrl+Shift+W Alt+F7 Ctrl+I Ctrl+Shift+U Ctrl+Y Shift+Enter psvm/soutmain/System.out.println(); Ctrl+J Ctrl+Shift+F Ctrl+F/Shift+F3/F3 Ctrl+Shift+S Ctrl+U Ctrl+Alt+S Alt+Shift+Inert/ Ctrl+Alt+Shift+S/ Ctrl+G Alt+Home Ctrl+Enter Ctrl+Backspace Ctrl+&quot;+/-&quot; Ctrl+Shift+&quot;+/-&quot; Ctrl+F2 Alt+Shift+F9 Debug Alt+Shift+F10 Run Ctrl+Shift+F9 Ctrl+Shift+F10 Ctrl+Shift+F8 F8 F7 Shift+F7 Shift+F8 Alt+Shift+F8 Alt+Shift+F7 Alt+F9 Ctrl+Alt+F9 F9 Alt+F10 Ctrl+F8 Ctrl+F9 Alt+1 Alt+2 Alt+6TODO Alt+7 Ctrl+Shift+C Ctrl+Alt+Shift+C Ctrl+Alt+Y Ctrl+~ Shift+F12 Ctrl+Shift+F12/ Ctrl+F4 Ctrl+Shift+F4 Ctrl+Tab Ctrl+Shift+Tab Ctrl+Alt+Shift+T Shift+F6 F6 F5 Alt+Delete Ctrl+Alt+N Ctrl+F Ctrl+R F3 Shift+F3 Ctrl+Shift+F Ctrl+Shift+R Ctrl+Shift+S Ctrl+Shift+M Alt+F7 Ctrl+Alt+F7 Ctrl+F7 Ctrl+Shift+F7VCS Alt+~VCS  Ctrl+K Ctrl+T Ctrl+Alt+Shift+D]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>IDEA</tag>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8]]></title>
    <url>%2F2018%2F12%2F04%2FJava8%2F</url>
    <content type="text"></content>
      <categories>
        <category> - java</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript]]></title>
    <url>%2F2018%2F12%2F04%2FJavaScript%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[cookie W3C http://www.cnblogs.com/Darren_code/archive/2011/11/24/Cookie.html  1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// delete cookie function delete_cookie( name, path, domain ) &#123; if( get_cookie( name ) ) &#123; document.cookie = name + &quot;=&quot; + ((path) ? &quot;;path=&quot;+path:&quot;&quot;)+ ((domain)?&quot;;domain=&quot;+domain:&quot;&quot;) + &quot;;expires=Thu, 01 Jan 1970 00:00:01 GMT&quot;; &#125;&#125;//ORfunction delete_cookie( name ) &#123; document.cookie = name + &apos;=; expires=Thu, 01 Jan 1970 00:00:01 GMT;&apos;;&#125;//Anotherfunction createCookie(name,value,days) &#123; if (days) &#123; var date = new Date(); date.setTime(date.getTime()+(days*24*60*60*1000)); var expires = &quot;; expires=&quot;+date.toGMTString(); &#125; else var expires = &quot;&quot;; document.cookie = name+&quot;=&quot;+value+expires+&quot;; path=/&quot;;&#125;function readCookie(name) &#123; var nameEQ = name + &quot;=&quot;; var ca = document.cookie.split(&apos;;&apos;); for(var i=0;i &lt; ca.length;i++) &#123; var c = ca[i]; while (c.charAt(0)==&apos; &apos;) c = c.substring(1,c.length); if (c.indexOf(nameEQ) == 0) return c.substring(nameEQ.length,c.length); &#125; return null;&#125;function eraseCookie(name) &#123; createCookie(name,&quot;&quot;,-1);&#125;I use this// what I have usedfunction removeItem(sKey, sPath, sDomain) &#123; document.cookie = encodeURIComponent(sKey) + &quot;=; expires=Thu, 01 Jan 1970 00:00:00 GMT&quot; + (sDomain ? &quot;; domain=&quot; + sDomain : &quot;&quot;) + (sPath ? &quot;; path=&quot; + sPath : &quot;&quot;);&#125;removeItem(&quot;cookieName&quot;); HTMLDOMw3c: HTML DOM HTML  HTML DOM Node Tree HTML DOM  HTML  JS Window (BOM)  JavaScript  (BOM) Browser Object Model  JavaScript  BOM  window JSscala def afunc(a = 1,b=2){} js function simue(a=1,b=2){}  js arguments,  1234function simue()&#123;var a = arguments[0] ? arguments[0]:1;var b = arguments[1] ? arguments[1] : 2;&#125; JS AjaxUpload.js123456789101112131415161718192021function uploadFile() &#123; new AjaxUpload($(&quot;#importFile&quot;), &#123; action: url, type: &quot;POST&quot;, data: &#123;&quot;userId&quot;: userId&#125;, autoSubmit: true, responseType: &quot;json&quot;, name: &apos;file&apos;, onSubmit: function (file, ext) &#123; if (!(ext &amp;&amp; /^(rar|zip|pdf|pdfx|txt|csv|xls|xlsx|doc|docx|RAR|ZIP|PDF|PDFX|TXT|CSV|XLS|XLSX|DOC|DOCX)$/.test(ext))) &#123; pNotifyAutoCloseCenter(&quot;fail&quot;, &quot;&quot;, &quot;error&quot;); return false; &#125; console.log(&quot;onsubmit&quot;); &#125;, onComplete: function (file, response) &#123; pNotifyAutoCloseCenter(&quot;info&quot;, &quot;&quot; + response.status + &quot;&quot;, &quot;info&quot;); console.log(&quot;upload file complete, savePath: &quot; + response.savePath); &#125; &#125;);&#125;   WebUploader (produced by baidu) http://fex.baidu.com/webuploader/getting-started.html]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag></tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java]]></title>
    <url>%2F2018%2F12%2F04%2FJava%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[Java.util.Timer Javajava.util.Timerjava.util.TimerTaskrun()Timerpublic void schedule(TimerTask task, long delay, long period) 800 12345678910111213141516171819202122232425262728293031package scheduler;import java.util.Timer;import java.util.TimerTask;/** * Created by xhh on 2017/2/4. */public class TimerTest extends TimerTask &#123; private String jobName = &quot;&quot;; public TimerTest(String jobName) &#123; super(); this.jobName = jobName; &#125; @Override public void run() &#123; System.out.println(&quot;executing: &quot; + jobName); &#125; public static void main(String[] args)&#123; Timer timer = new Timer(); long delay1 = 1 * 1000; long period1 = 1000; timer.schedule(new TimerTest(&quot;job1&quot;),delay1,period1); long delay2 = 2 * 1000; long period2 = 2000; timer.schedule(new TimerTest(&quot;job2&quot;),delay2,period2); &#125;&#125; Timer Quartz OpenSymphony Quartz Quartz  Java  Quartz 1 2]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java]]></title>
    <url>%2F2018%2F12%2F04%2FJava%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[java       Printer Spooler 123456789101112131415161718192021222324252627282930public class SingletonTest &#123; public static void main(String[] args)&#123; System.out.println(&quot;Singleton1.1:&quot;+Singleton1.getInstance()); System.out.println(&quot;Singleton1.2:&quot;+Singleton1.getInstance()); System.out.println(&quot;Singleton2.1:&quot;+Singleton2.getInstance()); System.out.println(&quot;Singleton2.2:&quot;+Singleton2.getInstance()); System.out.println(&quot;ton1:&quot;+new ton()); System.out.println(&quot;ton2:&quot;+new ton()); &#125;&#125;//class Singleton1&#123; private static Singleton1 singleton1 = null; public static Singleton1 getInstance()&#123; if (singleton1 == null)&#123; singleton1 = new Singleton1(); &#125; return singleton1; &#125;&#125;//class Singleton2&#123; private static final Singleton2 singleton2 = new Singleton2(); public static Singleton2 getInstance()&#123; return singleton2; &#125;&#125; Java service domain dao  service domain dao  daodomain domaindaodomaindao service domaindomainservice objectservice domain DAO  DAO    IUserService UserService IUserDomain UserDomain IUserDao UserSqlDao UserHbaseDao  , Q:I need to learn the difference between the type of methods (in term of business logic) that should be inside the Domain, DAO and Service layers objects. For example, if I am building a small web application to create, edit and delete customers data, as far as I understand inside Domain layer object I should add methods that Get/Set Customers object properties, for example (getName, getDOB, setAddress, setPhone...etc). Now what I am trying to learn is what methods shall I put in DAO and Service layers objects. Thanks in advance for your time and efforts. A:Speaking generally (not Hibernate or Spring specific): The DAO layer contains queries and updates to save your domain layer into your datastore (usually a relational DB but doesn&apos;t have to be). Use interfaces to abstract your DAO away from the actual datastore. It doesn&apos;t happen often, but sometimes you want to change datastores (or use mocks to test your logic), and interfaces make that easier. This would have methods like &quot;save&quot;, &quot;getById&quot;, etc. The Service layer typically contains your business logic and orchestrates the interaction between the domain layer and the DAOs. It would have whatever methods make sense for your particular domain, like &quot;verifyBalance&quot;, or &quot;calculateTotalMileage&quot;.StringStringcompareTos1.compareTo(s2),,, JavaclasspathpathJAVA_HOME:JavaclasspathpathJAVA_HOME]]></content>
      <categories>
        <category> - java</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Master Json]]></title>
    <url>%2F2018%2F12%2F04%2FMasterJson%2F</url>
    <content type="text"><![CDATA[Json XML JSON(JavaScript Object Notation)  jsonjavascript js{} {keyvalue,keyvalue,}keyvalue .key   js[] [java,javascript,vb,]  Json scala spray Json 12&quot;com.typesafe.akka&quot; %% &quot;akka-http-spray-json&quot; % &quot;10.0.10&quot;, &quot;io.spray&quot; % &quot;spray-json_2.11&quot; % &quot;1.3.4&quot; Json Array 12345import spray.json._ //parseJsonval kkkk = &quot;&quot;&quot; |[&#123;&quot;a&quot;: &quot;1&quot;&#125;, &#123;&quot;b&quot;: &quot;2&quot;&#125;] &quot;&quot;&quot;.stripMargin.parseJson.asInstanceOf[JsArray] 22+play-json-extensions 22+ field case class formatter and more for play-json case class ,play-json spray-json 1234567891011121314151617181920package data.creationimport ai.x.play.json.Jsonximport common.HttpClientHelperimport data.beans.LastInfoimport play.api.libs.json.Json/** * Created by xxh on 18-7-30. */object LastInfoHelper &#123; implicit lazy val lastInfoFormat = Jsonx.formatCaseClass[LastInfo] def getLastInfo(url:String,email:String,shoeLastBaseNo:String,basicsize:Int): LastInfo =&#123; val lastArray = new HttpClientHelper().post(url = url, postJsonObj = s&quot;&quot;&quot;&#123;&quot;email&quot;:&quot;$email&quot;,&quot;shoeLastBaseNo&quot;:&quot;$shoeLastBaseNo&quot;,&quot;basicsize&quot;:$basicsize&#125;&quot;&quot;&quot;) val lastJsonArray = Json.parse(lastArray) val lastInfo = lastJsonArray(0).as[LastInfo] lastInfo &#125;&#125;]]></content>
      <categories>
        <category> - Json</category>
      </categories>
      <tags>
        <tag>Json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis Learning]]></title>
    <url>%2F2018%2F12%2F04%2FRedis%20Learning%2F</url>
    <content type="text"><![CDATA[redis  Redis RedisRedisredis postredshift githubscalaredis clientscala scala  Redis is an open source(BSD Licensed),in-memory data structure store,used as a database,cache,and message broker.It supports data structures such as strings,hashes,lists,sets,sorted sets with range queries,bitmaps,hyperloglogs and geospatial indexes with radius queries.build-in: replication Lua scripting LRU eviction transaction different levels of on-disk persistence provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster. You can run atomic operations on these types, like appending to a string; incrementing the value in a hash; pushing an element to a list; computing set intersection, union and difference; or getting the member with highest ranking in a sorted set. in-memory dataset: In order to achieve its outstanding performance, Redis works with an in-memory dataset. Depending on your use case, you can persist it either by dumping the dataset to disk every once in a while, or by appending each command to a log. Persistence can be optionally disabled, if you just need a feature-rich, networked, in-memory cache. no-blocking replication: Redis also supports trivial-to-setup master-slave asynchronous replication, with very fast non-blocking first synchronization, auto-reconnection with partial resynchronization on net split. transcation 6379-Redisredis clientredis redis-cliredisredisredis-cliclient JDBCredissdkredis redis RDBAOFAOF listLPUSH mylist 1// BRPOP mylist 0 key,keyBRPOP BPOPLPUSH (Hash Algorithm),[]  Hash() Hash]]></content>
      <categories>
        <category> - database</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SBT]]></title>
    <url>%2F2018%2F12%2F04%2FSBT%E5%A2%9E%E5%8A%A0%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E5%B9%B6%E7%94%9F%E6%95%88%2F</url>
    <content type="text"><![CDATA[ SBT sbt update   project/plugins.sbt resolved xx failed, nideasbtplugins.sbtSBT sudo apt-get remove sbt ,sbt, 1234`echo &quot;deb https://dl.bintray.com/sbt/debian /&quot; | sudo tee -a /etc/apt/sources.list.d/sbt.listsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823sudo apt-get updatesudo apt-get install sbt` ubuntu16]]></content>
      <categories>
        <category> - sbt</category>
      </categories>
      <tags>
        <tag>SBT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell]]></title>
    <url>%2F2018%2F12%2F04%2FShell%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[awkputhdfs,hiveNULL copyidea,223(226)awkNF 226 &gt; awk  $17  navicatN* notepad++ps: hive sqoop awk , 123awk -F &apos;,&apos; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0 &quot;,:&quot; OFS&#125;&apos; xxx.csvawk -F &apos;,&apos; &apos;&#123;if(NF&lt;226)print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0&#125;&apos; ./xxx.csv awk -F &apos;,&apos; -v OFS=&quot;+++&quot; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;,linecontent:&quot;$0&#125;&apos; ./xxx.csv awk grepsedawkawklinux awkawkawk {pattern + action} {filenames} chkconfig chkconfig chkconfig12345[root@fa ~]# chkconfig --list NetworkManagerNetworkManager 0:off 1:off 2:on 3:on 4:on 5:on 6:off[root@fa ~]# chkconfig NetworkManager off[root@fa ~]# chkconfig --list NetworkManagerNetworkManager 0:off 1:off 2:off 3:off 4:off 5:off 6:off  curl get url  http://mywebsite.com/index.PHP?a=1&amp;b=2&amp;c=3 weburl$_GET Linux curl http://mywebsite.com/index.php?a=1&amp;b=2&amp;c=3 $_GETa url&amp;linux&amp;  &amp;$_GET curl http://mywebsite.com/index.php?a=1\&amp;b=2\&amp;c=3shell  date 12[root48@vmaxverserver Qimingxing4AutoTestCoverageRate]$ date +%F-%T2016-10-20-10:20:41  cron  /// -eq // -ne // -gt // -lt // ge // le // hostname:/etc/hosts  /etc/sysconfig/network  12345678910111213#!/bin/shscriptPath=$1sh /home/upgrade/shutdown.shcd $scriptPathfor i in *updating.shdohostName=echo $i|cut -d _ -f 1runAt=echo $i|cut -d _ -f 2|cut -d . -f 1sh /home/upgrade/upgrade.sh $hostName $runAt $scriptPath/$idonesh /home/upgrade/startup.sh linux grep//todo shell for in &amp; scp log1234echo &quot;script -a \$basedir/conf/scp_drs_ftpini.log -q -c \&quot;ssh \$scpuser@\$sybaseip mkdir -p \$scppath\&quot;&quot; &gt;&gt; $outputecho &quot;script -a \$basedir/conf/scp_drs_ftpini.log -q -c \&quot;scp \$configFile \$scpuser@\$sybaseip:\$scppath\&quot;&quot; &gt;&gt; $outputecho &quot;script -a \$basedir/conf/scp_drs_ftpini.log -q -c \&quot;ssh \$scpuser@\$sybaseip chmod 777 -R \$scppath\&quot;&quot; &gt;&gt; $outputecho &quot;exit&quot; &gt;&gt; $output  -_2015/8/31_ 123456789101112basedir=$(pwd) #  basedirbakpath=/home/bak_netnumenif [ &quot;$bakpath&quot; ]; then #[]   rm -rf $bakpathfids=$(ls $basedir | grep -E &apos;vmax-app|vmax-data&apos;) #ls &quot;grep -E&quot;  &quot;vamx-app&quot;&quot;vmax-data&quot;for s in $&#123;ds[@]&#125; #for do ... do mkdir -p $bakpath/$s cp -r $s/database $bakpath/$s done shell dirname basename123456789101112131415161718[root@hadoopname ~]# clear[root@hadoopname ~]# lltotal 136-rw-------. 1 root root 2696 Jun 24 04:41 anaconda-ks.cfgdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Desktopdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Documentsdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Downloads-rwxr-xr-x. 1 root root 32 Dec 6 13:20 first-rw-r--r--. 1 root root 39935 Dec 5 17:38 i.log-rw-r--r--. 1 root root 39935 Jun 24 04:40 install.log-rw-r--r--. 1 root root 10175 Jun 24 04:38 install.log.syslogdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Musicdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Picturesdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Publicdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Templatesdrwxr-xr-x. 2 root root 4096 Jun 24 06:13 Videos[root@hadoopname ~]# pwd/root dirname:   12[root@hadoopname ~]# dirname first.  123[root@hadoopname ~]# dirname /root/first/root[root@hadoopname ~]# basename:    12345[root@hadoopname ~]# basename i.logi.log[root@hadoopname ~]# basename i.log logi.[root@hadoopname ~]#]]></content>
      <categories>
        <category> - shell</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2FSimpleLinearRegression%2F</url>
    <content type="text"><![CDATA[WHAT y=b0+b1x y b0y b1 b0b1b1 Simple linear regression(wiki) HOW pythonsk-learnscalabreezesk-learn  123456789101112131415161718192021222324252627282930313233343536import pandas as pdfrom sklearn import linear_modelimport matplotlib.pyplot as pltdf = pd.read_csv(&apos;C:/Users/lenovo/Desktop/3VD_1.csv&apos;)&quot;&quot;&quot;# &quot;&quot;&quot;regr = linear_model.LinearRegression()&quot;&quot;&quot;# &quot;&quot;&quot;regr.fit(df[&apos;basicsize&apos;].values.reshape(-1,1), df[&apos;footaround&apos;])a, b = regr.coef_, regr.intercept_print(a[0])feats =[&apos;footaround&apos;, &apos;footbackarcdistance54mm&apos;, &apos;footbackarcdistancemouth&apos;, &apos;footbackbodylength&apos;, &apos;footbackfacedistance&apos;, &apos;footbackgirth&apos;, &apos;footbasicwidth&apos;, &apos;footbitfingeroutsidewidth&apos;, &apos;footboatcurveheight&apos;, &apos;footbottomheartconcavity&apos;, &apos;footdegreerejection&apos;, &apos;footfifthtoeoutsidewidth&apos;, &apos;footfirstplantarjointheight&apos;, &apos;footfirsttoeinsidewidth&apos;, &apos;footforepalmconvexity&apos;, &apos;footfrontcross&apos;, &apos;footheadthickness&apos;, &apos;footheelbulgeheight&apos;, &apos;footheelheartconvexity&apos;, &apos;footheelheartwidth&apos;, &apos;footheelheight&apos;, &apos;footlandspot&apos;, &apos;footmetatarsalgirth&apos;, &apos;footmouthbackheight&apos;, &apos;footmouthlength&apos;, &apos;footmouthwidth&apos;, &apos;footpalmwidth&apos;, &apos;footpocketheelgirth&apos;, &apos;footshoelastcalculatelength&apos;, &apos;foottarsalboneheight&apos;, &apos;foottarsalgirth&apos;, &apos;footthumbinsidewidth&apos;, &apos;footwaistgirth&apos;, &apos;footwaistwidth&apos;, &apos;footwidth&apos;]para_lin = []for feat in feats: regr.fit(df[&apos;basicsize&apos;].values.reshape(-1,1), df[feat]) a, b = regr.coef_, regr.intercept_ para_lin.append((feat,a[0],b))   OLS)  y=w0+w1x  w0  w1 wikipedia scala 12345678910111213141516def simple_linear_regression(x: Vector[Int], y: Vector[Double]) = &#123; //initial sums val n = x.length.toFloat val sum_x = x.sum val sum_y = y.sum val sum_xy = (x, y).zipped.map(_ * _).sum val sum_xx = x.map(n =&gt; n * n).sum //formula for w0 val slope = (sum_xy - (sum_x * sum_y) / n) / (sum_xx - (sum_x * sum_x) / n) //formula for w1 val intercept = sum_y / n - slope * (sum_x / n) (slope,intercept) &#125; PS: nMap, 123456789101112131415val is = getClass.getClassLoader.getResourceAsStream(&quot;origin-fat-last-data&quot;) val source = scala.io.Source.fromInputStream(is).getLines().map(_.split(&quot;\t&quot;).toList).toList val title = source.head val data = source.tail.filter(_.last == &quot;normal&quot;).toVector val mapData = data.map(_.zip(title).map(field=&gt; Map(field._2 -&gt; field._1)).reduce(_ ++ _)) val basicSizeVector = mapData.map(_.get(&quot;basicsize&quot;).get.toInt) val yMapDatas = mapData.map(_.filterNot(m =&gt; (&quot;shoes_size_specification&quot;,&quot;brand&quot;,&quot;shoelastbaseno&quot;,&quot;gender&quot;,&quot;headform&quot;,&quot;shoepadthicknessforefoot&quot;,&quot;shoepadthicknessheel&quot;,&quot;shoetype&quot;,&quot;footmouthgirth&quot;,&quot;footstruct&quot;).productIterator.contains(m._1))) .map(_.map(m =&gt; Map(m._1 -&gt; m._2.toDouble)).reduce(_ ++ _)) val yMapDataKeys =yMapDatas.head.map(_._1) val keyValues = yMapDataKeys.map(key =&gt; Map(key -&gt; yMapDatas.map(d=&gt; d.getOrElse(key,0.00d)))) def res = keyValues.map(kv =&gt;&#123; Map(kv.head._1 -&gt; simple_linear_regression(basicSizeVector,kv.head._2)) &#125;).reduce(_ ++ _)]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark-API-RDD]]></title>
    <url>%2F2018%2F12%2F04%2FSpark-RDD%2F</url>
    <content type="text"><![CDATA[spark version: Spark2.3.0 Spark Core: Combine SQL, streaming, and complex analytics. Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application. spark-core: RDDsprak-sql: DataFrame,DataSetspark-streaming:spark-MLib:spark-GraphX: SparkContextSpark App spark cluster SparkContext? sparkSession VS sparkContext JVMsparkcontext 1val sparkContext = new SparkContext(&quot;spark://localhost:7077&quot;,&quot;idea&quot;) SparkConfSparkContext SparkConfRDDRDDspark SparkRDD() RDD hadoop file system,hadoop-supported file system Driver scala Array RDDRDDRDD  broadcast variables accumulators]]></content>
      <categories>
        <category> - spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>RDD</tag>
        <tag>spark-API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow Model java ]]></title>
    <url>%2F2018%2F12%2F04%2FTensorflow4--javadeploy%2F</url>
    <content type="text"><![CDATA[Tensorflow Model java tensorflowapipython python pythonjavamnist Python //todo:  README.md Java //todo:  README.md]]></content>
      <categories>
        <category> - tensorflow</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow Model java ]]></title>
    <url>%2F2018%2F12%2F04%2FTensorflow5--seq2seq%2F</url>
    <content type="text"><![CDATA[IntroductionSeq2Seq (NMT)  decoder / attention wrapper APITensorFlow 1.2 data iterator   NMT  tips  tricksGoogle NMT Basic | Background on Neural Machine Translation  phrase-based  NMT  NMTencoder vector decoder  vector 1 encoder-decoder  multi-layer RNN  LSTM  RNN  Traning - How to build our first NMT system -  NMT  Hands-on - Lets train an NMT model12345678910111213python -m nmt.nmt \ --src=vi --tgt=en \ --vocab_prefix=/home/xxh/data/nmt_data_2/vocab \ --train_prefix=/home/xxh/data/nmt_data_2/train \ --dev_prefix=/home/xxh/data/nmt_data_2/tst2012 \ --test_prefix=/home/xxh/data/nmt_data_2/tst2013 \ --out_dir=/home/xxh/data/nmt_model \ --num_train_steps=12000 \ --steps_per_stats=100 \ --num_layers=2 \ --num_units=128 \ --dropout=0.2 \ --metrics=bleu python -m nmt.nmt src=vi tgt=en vocab_prefix=/home/xxh/data/my_nmt_data/train train_prefix=/home/xxh/data/my_nmt_data/train dev_prefix=/home/xxh/data/my_nmt_data/tst2012 test_prefix=/home/xxh/data/my_nmt_data/tst2012 out_dir=/home/xxh/data/my_nmt_model num_train_steps=12000 steps_per_stats=100 num_layers=2 num_units=128 dropout=0.2 metrics=bleu 12 python -m nmt.nmt out_dir=/home/xxh/data/nmt_model inference_input_file=/home/xxh/data/nmt_data_2/my.vi inference_output_file=/home/xxh/data/nmt_attention_model/output_infer]]></content>
      <categories>
        <category> - tensorflow</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat]]></title>
    <url>%2F2018%2F12%2F04%2FTomcat%2F</url>
    <content type="text"><![CDATA[TOMCATTomcat WebServlt; |bin Tomcattomcat |conf Tomcatserver.xmlweb.xml |docTomcat |lib/japser/commonTomcatJARS |logsTomcatLOG |srcTomcat |webappsTomcatWeb |workjspclass web.xml12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_4.dtd&quot;&gt;&lt;web-app&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;listener&gt; &lt;listener-class&gt;spray.servlet.Initializer&lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt; &lt;servlet-name&gt;SprayConnectorServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;spray.servlet.Servlet30ConnectorServlet&lt;/servlet-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;SprayConnectorServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/autotest/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; servlet servlet  javax.servlet.Servlet Servlet  API servlet Servlet  Web Servlet  JSP Servlet  Web  webAppWeb (HTMLCSS) servlet jsp   web.xml)  WebContext path/HelloServlet/HelloServletindex.html/HelloServlet/HelloServlet/index.html Web/WEB-INFBrowser/WEB-INF(/WEB-INF)404 Not Found/WEB-INF 1/WEB-INF/web.xml  2/WEB-INF/classes (.class)(package) 3/WEB-INF/lib JAR WebJARServletJSPJAR JAR/META-INF/resourcesJSP/META-INFindex.htmlURL/HelloServlet/index.html,/HelloServletindex.htmlJAR/META-INF/resources/index.html Web/WEB-INF/classes/WEB-INF/libJAR(JARTomcatTomcatlib) /WEB-INF/WEB-INFServletContextgetResource()getResourceAsStream()RequestDispatcher Web app URLlocalhost:8080/FirstServlet/ &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; JAR/META-INF/resources WebWARFirstServlet.warWeb]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04/18LS]]></title>
    <url>%2F2018%2F12%2F04%2FUbuntu16.04LS%E4%BD%BF%E7%94%A8%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ubuntu  softwareapt-get//dpkg --listii xserver-common 2:1.18.3-1ub all common files used by various X se ii xserver-xorg 1:7.7+13ubun amd64 X.Org X server . . . ii xz-utils 5.1.1alpha+2 amd64 XZ-format compression utilities ii yelp 3.18.1-1ubun amd64 Help browser for GNOME ii yelp-xsl 3.18.1-1 all XSL stylesheets for the yelp help ii youdao-dict 1.1.0-0~ubun amd64 Youdao Dict for Linux ii zenity 3.18.1.1-1ub amd64 Display graphical dialog boxes fr//sudo apt-get --purge remove youdao-dictpurge  sudo dpkg -i xxxsudo dpkg -i youdao-dict.deb ubuntuwin10,ubuntu,win10ubuntuwindows N win ubuntu  /home/xxh/package/r8168-8.044.021sh autorun.sh  UbuntuUbuntuPC  UBUNTU16ray, JAVA - java. EC2 - terminalec2(sudo ssh -i ~/aws/xuxianhong.pem ec2-user@ec2-52-197-178-250.ap-northeast-1.compute.amazonaws.com) ec2 descriptionec2-user. ubuntu18 - Ubuntu16.04 ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible]]></title>
    <url>%2F2018%2F12%2F04%2Fansible%2F</url>
    <content type="text"><![CDATA[what Working in IT, youre likely doing the same tasks over and over. What if you could solve problems once and then automate your solutions going forward? Ansible is here to help. puppetsaltstackchef installapt-getyum,github 1git clone git://github.com/ansible/ansible.git --recursive ansible freshman get startinventory InventorypatternsAd-Hoc Commandsplaybooksansible playbooksplaybooks playbooks ansible  ps: ,playbooks .,. playbook PlaybooksYAML playbook  plays . plays .  play ,. ansible ,play , tasks,., ansible ,. plays ,playbook  plays , playbook,, webservers ,  database server , webservers ,,. 123456789101112131415161718---- hosts: webservers vars: http_port: 80 max_clients: 200 remote_user: root tasks: - name: ensure apache is at the latest version yum: pkg=httpd state=latest - name: write the apache config file template: src=/srv/httpd.j2 dest=/etc/httpd.conf notify: - restart apache - name: ensure apache is running service: name=httpd state=started handlers: - name: restart apache service: name=httpd state=restarted play  Playbook(Roles) Includevariables ansible awsansible tower -&gt;  -&gt;  -&gt;  error ubuntu18.04error 266701ERROR! Unexpected Exception, this is probably a bug: &apos;module&apos; object has no attribute &apos;SSL_ST_INIT&apos; resolved: 1sudo pip2 install -U pyOpenSSL ec2b ansible-galaxy role sudo  -&gt; sudo-user remote-user,+ become ansible,bootstrap.ssh  akka remote]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[breeze]]></title>
    <url>%2F2018%2F12%2F04%2Fbreeze%2F</url>
    <content type="text"><![CDATA[whatBreeze is a library for numerical processing, machine learning, and natural language processing. Its primary focus is on being generic, clean, and powerful without sacrificing (much) efficiency. Breeze is the merger of the ScalaNLP and Scalala projects, because one of the original maintainers is unable to continue development. howinstallsbt sbt]]></content>
      <categories>
        <category> - scala</category>
      </categories>
      <tags>
        <tag>breeze</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[build-tool]]></title>
    <url>%2F2018%2F12%2F04%2Fbuild-tool%2F</url>
    <content type="text"><![CDATA[ JavaAntMaven AntMakeJUnitAntMakeAntMavenIDE MavenANT AntMaven JavaGradleBuildr MavenXML BuildrApacheRubyBuildrJava GradleJavaGroovyGroovyJVMJVMJVMJavaJava GradleGradle1.0GradleWare Gradle 1.02010Spring2011JAXJavaSpringSpringGradleGradle  GradleJavaMaven src/main/javasrc/main/resourcessrc/test/javaGradlebuild.gradle apply plugin: java repositories { mavenCentral()} dependencies { compile( com.google.guava:guava:13.0.1, joda-time:joda-time:2.1 ) testCompile( junit:junit:4.10, org.mockito:mockito-all:1.9.0 )} build.gradle  gradle build build/libJARGradle ID:s8:https://www.applysquare.com/topic-cn/SlxE26k3q/ gradlegradle installubuntu16,manually kafka 1gradle build  debug: 12gradle build --debug  gradle~/.gradle/gradle.properties;shadowsockshttps sbtmaven]]></content>
      <categories>
        <category>build tool</category>
      </categories>
      <tags>
        <tag>build tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2Fdatawarehouse%2F</url>
    <content type="text"><![CDATA[ OLTP OLAP                        ; ; .     OLAP OLAP 8 (1NF\2NF\3NF) 1NF 2NF 1NF, OrderDetailOrderIDProductIDUnitPriceDiscountQuantityProductName OrderID OrderIDProductID DiscountQuantityOderIDProductID UnitPriceProductName  ProductID OrderDetail  2NF 2NF OrderDetailOrderDetailOrderIDProductIDDiscountQuantityProductProductIDUnitPriceProductNameUnitPriceProductName 2NF 3NF 1NF[2NF ] 3NF2NF3NF2NF  2NF A  B B  OrderOrderIDOrderDateCustomerIDCustomerNameCustomerAddrCustomerCityOrderID  OrderDateCustomerIDCustomerNameCustomerAddrCustomerCity OrderID 2NF CustomerNameCustomerAddrCustomerCity  CustomerID 3NFOrderOrderOrderIDOrderDateCustomerIDCustomerCustomerIDCustomerNameCustomerAddrCustomerCity 3NF 2NF3NF2NF3NF .,,,,,,,,\,, 123create view &lt;&gt; [(&lt;&gt;[,&lt;&gt;]...)]as &lt;&gt;[with check option] ]]></content>
      <categories>
        <category>datawarehouse</category>
      </categories>
      <tags>
        <tag>datawarehouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ffmpeg]]></title>
    <url>%2F2018%2F12%2F04%2Fffmpeg%2F</url>
    <content type="text"><![CDATA[ ubuntu16:Linuxffmpeg ubuntu18: 1./ffmpeg -threads 2 -y -r 1 -framerate 30 -i ~/data/fmpgtest/p/i%d.png -vf scale=853:480 ~/data/fmpgtest/output1.mp4]]></content>
      <categories>
        <category> - ffmpeg</category>
      </categories>
      <tags>
        <tag>ffmpeg</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github]]></title>
    <url>%2F2018%2F12%2F04%2Fgithub%E5%8D%A1%E6%85%A2%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[github,dns \2017/12/04 bloghosts F12githubiphosts dns 2017/12/05  google-chromefirefox]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop ]]></title>
    <url>%2F2018%2F12%2F04%2Fhadoop%2F</url>
    <content type="text"><![CDATA[Map reduce job getting stuck at map 0% reduce 0%yarnactive node0 cpu mapMap reduce job getting stuck at map 0% reduce 0% ? mapreduce job  Hadoop 2.6 MapReducelog]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce,hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive  ]]></title>
    <url>%2F2018%2F12%2F04%2Fhive%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[hive  mrsql hive  hive hive lib jarhive bin hive conf  hive clihivevar/hiveconf/system/env .hivec    string  timestamp  binary   double float int tinyint smallint bigint   cast(s as int)  struct map array    (schema on write)hive(schema on read) hive  hiveql ]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>database,hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka]]></title>
    <url>%2F2018%2F12%2F04%2Fkafka%2F</url>
    <content type="text"><![CDATA[introduction publist and subscribe     Streaming data  Streaming react Kafka is generally used for two broad classes of applications: Building real-time streaming data pipelines that reliably get data between systems or applications Building real-time streaming applications that transform or react to the streams of data First a few concepts: Kafka is run as a cluster on one or more servers that can span multiple datacenters. The Kafka cluster stores streams of records in categories called topics. Each record consists of a key, a value, and a timestamp. API producer api cunsumer api streams api connector api kafka git clone gradle build  debug: 12gradle build --debug  gradle~/.gradle/gradle.properties;shadowsockshttps BUILG FAILD scala   1./gradlew -PscalaVersion=2.11.11 clean core:test build failed  1Could not find org.jacoco.agent.jar Apache Kafka Getting started with contributing to Apache Kafka (Part 1): Build and run Kafka from source code 123&gt; cd kafka&gt; gradle&gt; ./gradlew jar importa kafka project run zookeeper server.propertieszk run kafka cp conf/log4j.properties  core/src/main/resources/log4j.properties Run -&gt; Kafka   /  kafka.logs.dir=~/data/kafka-source-logs  log4j.rootLogger=TRACE, stdout  server.propertiessocket  socketsocket4BDS UNIXIP InternetSocketSocket220 110  sendsocketread    producer API  kafka monitor  systems performance 1234567891011121314151617181920212223# kafka start bash#!/bin/bashkafka_home=/home/xxh/wkspc/kafka/kafka_2.11-2.0.0kafka_start_sh=$kafka_home/bin/kafka-server-start.shkafkaconf0=$kafka_home/config/server.propertieskafkaconf1=$kafka_home/config/server-1.propertieskafkaconf2=$kafka_home/config/server-2.propertieskafkalog0=$kafka_home/logs/nohup.logkafkalog1=$kafka_home/logs/nohup_1.logkafkalog2=$kafka_home/logs/nohup_2.lognohup $kafka_start_sh $kafkaconf0 &gt; $kafkalog0 2&gt;&amp;1 &amp;nohup $kafka_start_sh $kafkaconf1 &gt; $kafkalog1 2&gt;&amp;1 &amp;nohup $kafka_start_sh $kafkaconf2 &gt; $kafkalog2 2&gt;&amp;1 &amp;jps# close use ./bin/kafka-server-stop.sh kafka  java8 java  Pulsar kafka Stream]]></content>
      <categories>
        <category>kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux]]></title>
    <url>%2F2018%2F12%2F04%2Flinux%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[ubuntu git ssh wirshark 123sudo add-apt-repository ppa:wireshark-dev/stablesudo apt-get updatesudo apt-get install wireshark wireshark-gnome shadowsocksenvhttp_proxy wtfkshadowsocks1080 -&gt; 34177 ok,windows  AWSEC2 ssh aws ec2ec2ssh  ec2sshd 1/etc/ssh/sshd_config =&gt; 12ServerAliveInterval 60ServerAliveCountMax 10  linuxjava8java7,java8 java8 jdk1.7 rpm -qa | grep java 1234[ec2-user@ip-10-0-1-89 ~]$ rpm -qa |grep javajava-1.7.0-openjdk-1.7.0.151-2.6.11.0.74.amzn1.x86_64tzdata-java-2017b-1.69.amzn1.noarchjavapackages-tools-0.9.1-1.5.amzn1.noarch remove jdk1.7 yum -y remove java-1.7.0-openjdk-1.7.0.151-2.6.11.0.74.amzn1.x86_64 check 12345[ec2-user@ip-10-0-1-89 ~]$ java -version-bash: java: command not found[ec2-user@ip-10-0-1-89 ~]$ rpm -qa |grep javatzdata-java-2017b-1.69.amzn1.noarchjavapackages-tools-0.9.1-1.5.amzn1.noarch res:CentOS OpenJdk install jdk8 java8 rpm  1.1 123456789101112131415161718192021222324 [ec2-user@ip-10-0-1-89 ~]$ java -version -bash: java: command not found [ec2-user@ip-10-0-1-89 ~]$ rpm -ivh .bash_history .bash_profile jdk-8u152-linux-x64.rpm .bash_logout .bashrc .ssh/ [ec2-user@ip-10-0-1-89 ~]$ rpm -ivh jdk-8u152-linux-x64.rpm error: can&apos;t create transaction lock on /var/lib/rpm/.rpm.lock (Permission denied) [ec2-user@ip-10-0-1-89 ~]$ sudo rpm -ivh jdk-8u152-linux-x64.rpm Preparing... ################################# [100%] Updating / installing... 1:jdk1.8-2000:1.8.0_152-fcs ################################# [100%] Unpacking JAR files...tools.jar...plugin.jar...javaws.jar...deploy.jar...rt.jar...jsse.jar...charsets.jar...localedata.jar... [ec2-user@ip-10-0-1-89 ~]$ java -version java version &quot;1.8.0_152&quot; Java(TM) SE Runtime Environment (build 1.8.0_152-b16) Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode) 2.2 12sudo apt-get install oracle-java8-installer[](https://www.cnblogs.com/a2211009/p/4265225.html)]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[log4j]]></title>
    <url>%2F2018%2F12%2F04%2Flog4j%E8%B8%A2%E5%B9%B3%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[** log4j:WARN No appenders could be found for logger (scala.runtime.Nothing$). ** log4j  stackoverflow Just to get you going you have two simple approaches you can take. First one is to just add this line to your main method:BasicConfigurator.configure();Second approach is to add this standard log4j.properties (taken from the above mentioned guide) file to your classpath: Set root logger level to DEBUG and its only appender to A1.log4j.rootLogger=DEBUG, A1 A1 is set to be a ConsoleAppender.log4j.appender.A1=org.apache.log4j.ConsoleAppender A1 uses PatternLayout.log4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n The first one ]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag>log4j</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scikit-learn]]></title>
    <url>%2F2018%2F12%2F04%2Fml-sklearn%2F</url>
    <content type="text"><![CDATA[Scikit-learn anaconda3scikit-learn Scikit-learn ]]></content>
      <categories>
        <category> - scikit-learn</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
        <tag>scikit-learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy]]></title>
    <url>%2F2018%2F12%2F04%2Fnumpy%2F</url>
    <content type="text"><![CDATA[ numpy, pandas Why numpy  pandas  C , pandas  numpy,  numpy   python  Numpy Anaconda Numpy]]></content>
      <categories>
        <category> - Numpy</category>
      </categories>
      <tags>
        <tag></tag>
        <tag></tag>
        <tag>ml</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2Fothers-blog%2F</url>
    <content type="text"><![CDATA[[https://home.cnblogs.com/u/sharpxiajun/feed/1.html [zhaorongcuncsdn:http://blog.csdn.net/zrc199021 [ scala http://hongjiang.info/ [ [stark_summer[ [cwiki [http://blog.csdn.net/honglei915/article/details/37697655 [(18) [ [ [ [UML [hadoop  [IDEA [concurrnetHashMap  [- [linux shell [JAVA [java [github]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas]]></title>
    <url>%2F2018%2F12%2F04%2Fpandas%2F</url>
    <content type="text"><![CDATA[### anaconda3scikit-learn Scikit-learn ]]></content>
      <categories>
        <category> - pandas</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python ]]></title>
    <url>%2F2018%2F12%2F04%2Fpython%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[python python python List 2 String How to convert list to string By using .join 12list1 = [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;]str1 = &apos;&apos;.join(list1) Or if the list is of integers, convert the elements before joining them. 12list1 = [1, 2, 3]str1 = &apos;&apos;.join(str(e) for e in list1) python 1234567file = &quot;/home/xxh/data/footlastlist&quot;+time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()) f= open(file,&quot;w+&quot;) try: f.write(&apos;&apos;.join(str(e) for e in footlastlist)) finally: f.close() fileopenclose(with) openw+file]]></content>
      <categories>
        <category> - python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sbt package VS sbt assembly VS sbt dist]]></title>
    <url>%2F2018%2F12%2F04%2FsbtpackageVSsbtassembly%2F</url>
    <content type="text"><![CDATA[sbt sbt package sbt assembly sbt dist 1. sbt packagesbt Doc[]Running package  src/main/resources  src/main/scala  src/main/java  class  jar package Creates a jar file containing the files in src/main/resources and the classes compiled from src/main/scala and src/main/java.  1sbt package target// 2. sbt assemblyhttps://github.com/sbt/sbt-assembly Deploy fat JARs. Restart processes. sbt-assembly is a sbt plugin originally ported from codahales assembly-sbt, which Im guessing was inspired by Mavens assembly plugin. The goal is simple: Create a fat JAR of your project with all of its dependencies.  sbt package,sbt assemblyproject githubREADME.md project/plugin.sbt 1addSbtPlugin(&quot;com.eed3si9n&quot; % &quot;sbt-assembly&quot; % &quot;0.14.6&quot;) build.sbt 1234//java jar main()mainClass in assembly := Some(&quot;AwsSupportS3&quot;)//jarassemblyJarName in assembly := s&quot;my-api-hdfs1.1.0607.jar  1sbt assembly target/***/ 3. sbt distsbt  githubhttps://github.com/sbt/sbt-native-packager sbthttps://www.scala-sbt.org/sbt-native-packager/  Code once, deploy anywhere  mark project/plugin.sbt 1addSbtPlugin(&quot;com.typesafe.sbt&quot; % &quot;sbt-native-packager&quot; % &quot;1.1.5&quot;) build.sbt 1234567891011121314151617181920enablePlugins(JavaServerAppPackaging)//main class mainClass in Compile := Some(&quot;***.aip.ml.platform.rest.WebServer&quot;)//projectsrc/main/resourcesmappings in Universal ++= &#123; // optional example illustrating how to copy additional directory directory(&quot;scripts&quot;) ++ // copy configuration files to config directory contentOf(&quot;src/main/resources&quot;).toMap.mapValues(&quot;config/&quot; + _)&#125;// add &apos;config&apos; directory first in the classpath of the start script,// an alternative is to set the config file locations via CLI parameters// when starting the applicationscriptClasspath := Seq(&quot;../config/&quot;) ++ scriptClasspath.valuelicenses := Seq((&quot;CC0&quot;, url(&quot;http://creativecommons.org/publicdomain/zero/1.0&quot;)))  1sbt dist  1sbt clean update project]]></content>
      <categories>
        <category> - sbt</category>
      </categories>
      <tags>
        <tag>sbt</tag>
        <tag>package</tag>
        <tag>assembly</tag>
        <tag>dist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala Try]]></title>
    <url>%2F2018%2F12%2F04%2FscalaTry%2F</url>
    <content type="text"><![CDATA[Scala Try:Try 123456789101112131415161718192021222324252627package tryAndException/** * Created by xxh on 18-7-17. */object MasterTry &#123; case class Customer(age:Int) class Cigarettes case class UnderAgeException(message:String) extends Exception(message) def buyCigarettes(customer: Customer):Cigarettes = &#123; if (customer.age &lt;16) throw UnderAgeException(s&quot;Customer must be older than 16 but was $&#123;customer.age&#125;&quot;) else new Cigarettes &#125; val youngCustomer = Customer(15) try &#123; buyCigarettes(youngCustomer) &quot;Yo, here are your cancer sticks! Happy smokin&apos;!&quot; &#125; catch &#123; case UnderAgeException(msg) =&gt; msg &#125;&#125;   actor     Scala scala  Try Try Option[A]  Try[A]   A  Throwable   Try  Success[A]: ThrowableFalure[A]: 12345import scala.util.Tryimport java.net.URLdef parseURL(url: String): Try[URL] = Try(new URL(url))val url = parseURL(Console.readLine(&quot;URL: &quot;)) getOrElse new URL(&quot;http://duckduckgo.com&quot;)  isSuccess  Try  get   getOrElse  Try  gitbook]]></content>
      <categories>
        <category> - s cala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala implict]]></title>
    <url>%2F2018%2F12%2F04%2Fscala%20implicit%2F</url>
    <content type="text"><![CDATA[why  Scala howscala implicit X .method X  method) X  YY method  X  Y Y  method implicit  X,Y,XY 12345678910scala&gt; implicit def double2Int(d:Double) = d.toIntwarning: there were 1 feature warning(s); re-run with -feature for detailsdouble2Int: (d: Double)Intscala&gt; implicit def double2Int(d:Double):Int = d.toIntwarning: there were 1 feature warning(s); re-run with -feature for detailsdouble2Int: (d: Double)Intscala&gt; val i:Int = 3.5i: Int = 3 implicit implicitimplicit   :implict     ,   ]]></content>
      <categories>
        <category> - scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scalaXML]]></title>
    <url>%2F2018%2F12%2F04%2Fscalaxml%2F</url>
    <content type="text"><![CDATA[scala xml12345678910111213141516171819202122232425262728293031323334353637val loadXML = xml.XML.loadFile(AutomatedTestingWebConfig.distributeInfoXML)//loadFileXMLscala.xml.Elemval version = (loadXML \\ &quot;autotest&quot; \ &quot;@version&quot;).text//@val hostSeqNode = (loadXML \ &quot;hosts&quot; \ &quot;host&quot;)//scala.xml.NodeSeqdef getAttrValue(node: Node)(attrName: String): String = &#123; node.attributes.get(attrName) match &#123; case Some(x) =&gt; x.text case _ =&gt; &quot;&quot; &#125; &#125; val hosts = &#123; (loadXML \ &quot;hosts&quot; \ &quot;host&quot;).map(hostNode =&gt; &#123; def getHostAttrValue(attrName: String) = getAttrValue(hostNode)(attrName) Host(getHostAttrValue(&quot;name&quot;), getHostAttrValue(&quot;desc&quot;), getHostAttrValue(&quot;config&quot;)) &#125;).toList &#125; val sqlScript = &#123; (loadXML \ &quot;script&quot; \ &quot;sql&quot;).map(sqlNode =&gt; &#123; def getSqlScriptAttrValue(attrName: String) = getAttrValue(sqlNode)(attrName) val lines = sqlNode.nonEmptyChildren.filterNot(_.text.trim == &quot;&quot;).map(_.text).mkString(&quot;;&quot;) val database = getSqlScriptAttrValue(&quot;database&quot;) SqlScript(database, lines) &#125;).toList &#125;case class AutoTest(version: String, hosts: List[Host], sqlScript: List[SqlScript], shellScript: List[ShellScript])case class Host(name: String, desc: String, config: String)case class SqlScript(database: String, line: String)case class ShellScript(host: String, line: List[String])  SCALA XML pattern attrbute guy Because Scala doesnt support XML patterns with attributes. scala  XPath]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>XML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala ]]></title>
    <url>%2F2018%2F12%2F04%2Fscala%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[TomCatTomcat JAVA_HOME; tomcat netstat -an  server.xml; startup.bat  scala List forall: xs forall pxspptrue;  exists: xs exists pxs ptrue scala List TO String  mkString()  12345678910object TestGroupDao extends App&#123; new GroupDao().updateGroup(GroupInfo(3456,&quot;centrum&quot;))&#125;object MainTestGroupDao&#123; def main (args: Array[String]) &#123; val groupDao = new GroupDao println(&quot;groupDao: &quot; + groupDao) &#125;&#125; case class  =&gt; List RE123456789101112131415161718192021productIterator.toListscala&gt; case class aaa(i:Int,j:Int)defined class aaascala&gt; val q = aaa(1,2)q: aaa = aaa(1,2)scala&gt; q.toList&lt;console&gt;:11: error: value toList is not a member of aaa q.toList ^scala&gt; q.toStringres1: String = aaa(1,2)scala&gt; q.productIterator.toListres2: List[Any] = List(1, 2)scala&gt; q.productIterator.toList.mkString(&quot;&apos;&quot;)res3: String = 1&apos;2 case class   Map RE123456789101112131415def updateUser(userInfo: UserInfo): Boolean = &#123; //val userInfoFieldName = userInfo.getClass.getDeclaredFields.map(_.getName) val userInfoFieldValueMap = (Map[String, Any]() /: userInfo.getClass.getDeclaredFields) &#123; (a, f) =&gt; f.setAccessible(true) a + (f.getName -&gt; f.get(userInfo)) &#125; val updateUserSqlset = if(userInfo.password == &quot;&quot;) userInfoFieldValueMap.keys.map(a =&gt; &#123;if (a != &quot;password&quot;) a + &quot;=&apos;&quot; + userInfoFieldValueMap(a)&#125;).mkString(&quot;&apos;,&quot;) else userInfoFieldValueMap.keys.map(a =&gt; a + &quot;=&apos;&quot; + userInfoFieldValueMap(a)).mkString(&quot;&apos;,&quot;) val updateUserSqlWhere = &quot;userId = &apos;&quot; + userInfoFieldValueMap(&quot;userId&quot;) + &quot;&apos;&quot; val updateUserSql = s&quot;&quot;&quot; |update userinfo set $updateUserSqlset where $updateUserSqlWhere &quot;&quot;&quot;.stripMargin Sql(updateUserSql).execute &#125; List    1. List(...)  1234567scala&gt; val fruit = List(&quot;Apple&quot;,&quot;Orange&quot;,&quot;pears&quot;)fruit: List[String] = List(Apple, Orange, pears)scala&gt; val List(a,b,c) = fruita: String = Appleb: String = Orangec: String = pears 2. ::   Nil  123456789101112scala&gt; val fruit = List(&quot;Apple&quot;,&quot;Orange&quot;,&quot;pears&quot;)fruit: List[String] = List(Apple, Orange, pears)scala&gt; val List(a,b,c) = fruita: String = Appleb: String = Orangec: String = pearsscala&gt; val a:: b :: rest = fruita: String = Appleb: String = Orangerest: List[String] = List(pears) 1.  1234567def isort(xs: List[Int]): List[Int] = if(xs.isEmpty) Nil else insert(xs.head,isort(xs.tail))def insert(x: Int, xs: List[Int]): List[Int] = if(xs.isEmpty || x &lt;= xs.head) x :: xs else xs.head :: insert(x,xs.tail) 2. //  123456789def isort(xs: List[Int]) = xs match &#123; case List() =&gt; List() case x :: xs1 =&gt; insert(x,isort(xs1))&#125;def insert(x: Int, xs: List[Int]) = xs match &#123; case List() =&gt; List(x) case y :: ys =&gt; if(x &lt;= y) x :: xs else y :: insert(x,ys)&#125;http://hongjiang.info/scala/Scala //todoscala sort 123xmlFileList.map((xml.XML.loadFile(_))).map(parseXML(_)).filter((_.version.compareTo(currentVersion) &gt; 0)).map(a =&gt; (a.version, a)).&lt;strong&gt;sortWith((a,b) =&gt; (a._1).compareTo(b._1) &lt; 0)&lt;/strong&gt;.toMap  scala //todo scala ListgetFootInfoByUUID(l: _*) getFootInfoByUUID()l _*  scala scala sbt read file from resources1234val is = getClass.getClassLoader.getResourceAsStream(&quot;remote_app.conf&quot;)val source = scala.io.Source.fromInputStream(is).getLines().map(_.split(&quot;\t&quot;).toList).toList//.foreach(as =&gt; println(as.length))[How to get load file under resources folder in scala sbt](https://stackoverflow.com/questions/40724082/how-to-get-load-file-under-resources-folder-in-scala-sbt) scala  1libraryDependencies += &quot;org.scala-lang&quot; % &quot;scala-reflect&quot; % &quot;2.11.11&quot; scala Jsonplay-json-extensions +22 field case class formatter and more for play-json case class ,play-json spray-json 1234567891011121314151617181920package data.creationimport ai.x.play.json.Jsonximport common.HttpClientHelperimport data.beans.LastInfoimport play.api.libs.json.Json/** * Created by xxh on 18-7-30. */object LastInfoHelper &#123; implicit lazy val lastInfoFormat = Jsonx.formatCaseClass[LastInfo] def getLastInfo(url:String,email:String,shoeLastBaseNo:String,basicsize:Int): LastInfo =&#123; val lastArray = new HttpClientHelper().post(url = url, postJsonObj = s&quot;&quot;&quot;&#123;&quot;email&quot;:&quot;$email&quot;,&quot;shoeLastBaseNo&quot;:&quot;$shoeLastBaseNo&quot;,&quot;basicsize&quot;:$basicsize&#125;&quot;&quot;&quot;) val lastJsonArray = Json.parse(lastArray) val lastInfo = lastJsonArray(0).as[LastInfo] lastInfo &#125;&#125; case object vs case class vs object vs classcase class \ case object pattern match  toString  equal object  new traitnew extendstrait 123456789101112131415161718192021trait MyTrait&#123;val a = 1val b = 2&#125;//----------------val anonClassMixingInTrait = new Mytrait&#123; def aFunctionInMyClass = &quot;111&quot;&#125;//is the equivalent of:class MyClass extends MyTrait&#123; def aFunctionInMyClass = &quot;111&quot;&#125;val anonClassMixingInTrait = new MyClass   12345678910111213141516val t = new MyTrait &#123; val t1 = ... //some expression val t2 = ... //some expression&#125;is the same asval t = new AnyRef with MyTrait &#123; val t1 = ... //some expression val t2 = ... //some expression&#125;is the same asval t = new Object with MyTrait &#123; val t1 = ... //some expression val t2 = ... //some expression&#125; typescala (dollar)$ 12scala&gt; val a = s&quot;$$a&quot;a: String = $a]]></content>
      <categories>
        <category> - scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell]]></title>
    <url>%2F2018%2F12%2F04%2Fshell%E8%84%9A%E6%9C%AC%E8%BF%81%E7%A7%BBgitlab%2F</url>
    <content type="text"><![CDATA[gitlab A repo gitlab B coder,gitlab  repo 12345678910111213141516171819202122232425262728293031323334#!/bin/bash# baseDir=$(pwd)# fatherDir=$(dirname $baseDir)#echo $fatherDir#projList=$(ls $baseDir)# .. [Linux Shell ](https://blog.csdn.net/DLUTBruceZhang/article/details/9244897)projList=$(ls -l |grep &quot;^d&quot; |awk &apos;&#123;print $9&#125;&apos;)for p in $&#123;projList[@]&#125; do if [ $p != &quot;/home/xxh/data/mycloud&quot; ] #,if  then #echo &quot;wocao&quot; newP=$fatherDir/gitlabbakmirror/$p #repo dir mkdir -p $newP echo $newP git clone git@aigit.epoque.cn:xianhong.xu/$p.git $newP cd $newP # gitlabrepo [repogitlab](https://blog.csdn.net/lets_do/article/details/78110913) # [private token ](https://www.safaribooksonline.com/library/view/gitlab-cookbook/9781783986842/ch06s05.html) info=&quot;name=$p&amp;path=$p&amp;wiki_enabled=no&amp;public_jobs=false&amp;public=false&amp;default_branch=master&amp;private_token=WwspxLGvbxAAY_5HQBH9&quot; # ip curl -d $info &quot;http://172.17.60.240:8020/api/v3/projects&quot; git remote set-url origin git@172.17.60.240:xuxianhong/$p.git #remote url -&gt; gitlab git add . #  git commit -m &apos;migrate&apos; git push origin master cd $baseDir else echo $p&quot;,no process&quot; fi done ]]></content>
      <categories>
        <category> - shell</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark]]></title>
    <url>%2F2018%2F12%2F04%2Fspark%E5%A1%AB%E5%9D%91%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[spark streamingERROR DFSClient: Failed to close inode xxxxtopic checkpoint spark sparkext331998()32000    /home/mr/spark/conf/spark-defaults.conf 123spark.worker.cleanup.enabled=true,spark.worker.cleanup.appDataTtl = 259200(3*24*3600), #worker App sparksparkSQL]]></content>
      <categories>
        <category> - spark</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark3API:RDDs/DataFrames/DataSets]]></title>
    <url>%2F2018%2F12%2F04%2Fspark%E7%9A%84%E4%B8%89%E4%B8%AAAPI%2F</url>
    <content type="text"><![CDATA[Apache SparkAPIRDDDataFrameDataset]]></content>
      <categories>
        <category> - spark</category>
      </categories>
      <tags>
        <tag>RDD</tag>
        <tag>Spark</tag>
        <tag>DataFrame</tag>
        <tag>DataSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql]]></title>
    <url>%2F2018%2F12%2F04%2Fsql%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[sqlSQLSQLSQL WHERESELECT ONWHERESELECT DescartesA=a,bB=0,1,2(a,0),(a,1),(a,2),(b,0),(b,1), (b,2) ABAB WHERESELECT ONONWHERE ON WHEREONSELECTON WHEREON ONWHERE     sqlsql sqlFROMSELECT   SQL server2000  2005: FROM : fromVT1; ON: VT1ONVT2; OUTER(JOIN) OUTER JOINCROSS JOIN (INNER JOIN),preserved table VT2,VT3.FROM13 WHEREVT3WHEREtrueVT4. GROUP BYGROUP BYVT4VT5. CUBE|ROLLUP(Suppergroups)VT5,VT6. HAVINGVT6HAVINGtrueVT7. SELECTSELECTVT8. DISTINCTVT8VT9. ORDER BYVT9ORDER BY VC10). TOPVC10VT11, sqlsqlsql sql selectfromwhere from   userid join where userid   sql sql      sql   +  QA Varchar:Varchar 12345678910create table xxh.sample( prjnum int not null, prjname varchar(200), emynum int not null, emyname varchar(200), salcategory char(1), salpackage int, primary key(prjnum) ); ibm sqlsql union: sqlgroup by &amp; having\GOGOGOselect * from sysobjects where id=aselect getdate()select * from sysobjects where id=agoselect getdate()goselect getdate()\ mysql-Every derived table must have its own alias\ _mysql_exceptions.OperationalError: (1248, &apos;Every derived table must have its own alias&apos;)\ select select count(*) from (select * from A where uuid=&apos;123&apos;) ; // error\ select count(*) from (select * from A where uuid=&apos;123&apos;) as a ; // okCAST CAST CAST()AS  12//CAST (expression AS data_type)cast(minute(clttime)/5 AS int) mysqlmysql45]]></content>
      <categories>
        <category> - database</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop]]></title>
    <url>%2F2018%2F12%2F04%2Fsqoop%2F</url>
    <content type="text"><![CDATA[installsqoop install &amp;&amp; test 1.4.71.99.7 sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz  template 123456789101112131415161718# Set Hadoop-specific environment variables here.#Set path to where bin/hadoop is available#export HADOOP_COMMON_HOME=export HADOOP_COMMON_HOME=/home/xxh/wkspc/hadoop/hadoop-2.7.7#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/home/xxh/wkspc/hadoop/hadoop-2.7.7#set the path to where bin/hbase is availableexport HBASE_HOME=/home/xxh/wkspc/hbase/hbase-2.1.0#Set the path to where bin/hive is availableexport HIVE_HOME=/home/xxh/wkspc/hive/apache-hive-1.2.2-bin#Set the path for where zookeper config dir isexport ZOOCFGDIR=/home/xxh/wkspc/zookeeper/zookeeper0/conf  1234567891011121314151617181920212223242526sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root --password admin123sqoop import \--connect jdbc:mysql://localhost:3306/mysql \--username root \--password admin123 \--table user \--target-dir /sqoop/data_1 \--delete-target-dir \--num-mappers 1 \--fields-terminated-by &quot;\t&quot;# mysql on other computersqoop import \--connect jdbc:mysql://172.17.60.59:3306/mysql \--username root \--password Admin@321 \--table user \--target-dir /sqoop/data_user_59 \--delete-target-dir \--num-mappers 1 \--fields-terminated-by &quot;\t&quot; Sqoop mrHDFS Sqoopjdbc sqljava  sqoop mr,yarn   123# 8088  app id,yarn application -kill application_1540448968752_0003 sqoop json 12345678910111213141516171819Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/json/JSONObject at org.apache.sqoop.util.SqoopJsonUtil.getJsonStringforMap(SqoopJsonUtil.java:43) at org.apache.sqoop.SqoopOptions.writeProperties(SqoopOptions.java:785) at org.apache.sqoop.metastore.hsqldb.HsqldbJobStorage.createInternal(HsqldbJobStorage.java:399) at org.apache.sqoop.metastore.hsqldb.HsqldbJobStorage.create(HsqldbJobStorage.java:379) at org.apache.sqoop.tool.JobTool.createJob(JobTool.java:181) at org.apache.sqoop.tool.JobTool.run(JobTool.java:294) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252)Caused by: java.lang.ClassNotFoundException: org.json.JSONObject at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 12 more solve: 1234567By spending some more time reading blogs:I found solution as below:downloaded java-json.jar file from location http://www.java2s.com/Code/Jar/j/Downloadjavajsonjar.htmstored this jar file at location /usr/lib/sqoop/lib/java-json.jar Sqoop import exception java.lang.NoClassDefFoundError: org/json/JSONObject name node is in safe mode solve: 1hdfs dfsadmin -safemode leave name node is in safe mode sqoop  ACCEPED [warn] error*,nodeyarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage -&gt; 98.58088 UI Active nodes1  123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#yarn-site.xml&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/yarnlocaldir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/yarnlogdir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage&lt;/name&gt; &lt;value&gt;98.5&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;# hdfs-site.xmldatanodedfs.datanode.data.dir /tmpT&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/namenode-namespace&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/xxh/data/hadoop/datanodespace&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; MapReduce jobs get stuck in Accepted state redshift   hive  123456789101112./bin/sqoop job --delete customer_order./bin/sqoop job --create customer_order \-- import \--connect jdbc:redshift://north-1.redshift.amazonaws.com.cn:59/prod \--driver com.amazon.redshift.jdbc42.Driver \--username boot_last \--table XXX.XXXX \--target-dir /hive/warehouse/XXXX/XXXX \--fields-terminated-by &apos;,&apos; \--lines-terminated-by &apos;\n&apos; \--hive-drop-import-delims \-m 1 map -m 1? hash,hashsqoophash sqoop  table column redshift jdbc Driver  mavenjar,sqoop homelib ? sqoop hive ]]></content>
      <categories>
        <category>sqoop</category>
      </categories>
      <tags>
        <tag>sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH]]></title>
    <url>%2F2018%2F12%2F04%2Fssh%E6%B7%B1%E5%85%A5%2F</url>
    <content type="text"><![CDATA[SSH(Secure Shell)Secure ShellSSH SSHSSH[2]SSHSSHSSH SSHTelnetshellTelnetBerkeley rloginrshrexec[4]SSH SSH[2] -    SSHX11SFTPSCP Unix /home  ~/.ssh/authorized_keys [9]SSHssh-issh-keygenSSHSSHSSH aws ec2ec2-iec2/.ssh/authorized_keys ec2/.ssh/authorized_keys]]></content>
      <categories>
        <category> - shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow2--]]></title>
    <url>%2F2018%2F12%2F04%2Ftensorflow2--%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Tensorflow Tf (graph) (session)(context) tensor (Variable) feedfetcharbitrary operation TensorFlow , .  op (operation ).  op  0  Tensor, ,  0  Tensor.  Tensor . , ,  [batch, height, width, channels].  TensorFlow . ,   .   op  CPU  GPU   ,  op . ,  tensor .  Python ,  tensor  numpy ndarray ;  C  C++ ,  tensor  tensorflow::Tensor . TensorFlow . , op  . ,  op. , ,  op. TensorFlow  C, C++, Python . , TensorFlow  Python , ,  C  C++ .  (session libraries) . ,  op (source op).  op ,   (Constant).  op  op . Python , op  op ,  op . TensorFlow Python  (default graph), op .  .  Graph   .  ]]></content>
      <categories>
        <category> - tensorflow</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow3--mnist]]></title>
    <url>%2F2018%2F12%2F04%2Ftensorflow3--mnist%2F</url>
    <content type="text"><![CDATA[mnistMnisttfhello world mnist_softmax.pysoft_max Softmaxlogistic \textstyle y SoftmaxMNIST10Softmax/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# Copyright 2015 The TensorFlow Authors. All Rights Reserved.## Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ==============================================================================&quot;&quot;&quot;A very simple MNIST classifier.See extensive documentation athttps://www.tensorflow.org/get_started/mnist/beginners&quot;&quot;&quot;#PythonPython__future__from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionimport argparseimport sys#tfinput_datafrom tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tf#FLAGS = Nonedef main(_): # Import data mnist = input_data.read_data_sets(FLAGS.data_dir) # Create the model #  &quot;&quot;&quot; xyTensorFlow x2shape[None, 784]784MNISTNonebatchxy_210one-hot,MNIST placeholdershapeTensorFlow WbTensorFlow TensorFlowVariable &quot;&quot;&quot; x = tf.placeholder(tf.float32, [None, 784]) W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) #xWbsoftmax y = tf.matmul(x, W) + b # Define loss and optimizer # y_ = tf.placeholder(tf.float32, [None]) y_ = tf.placeholder(tf.int64, [None]) # The raw formulation of cross-entropy, # # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)), # reduction_indices=[1])) # # can be numerically unstable. # # So here we use tf.losses.sparse_softmax_cross_entropy on the raw # outputs of &apos;y&apos;, and then average across the batch. # cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y) #TensorFlowTensorFlowTensorFlow 0.5. train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) #session sess = tf.InteractiveSession() # tf.global_variables_initializer().run() # Train # 1000100train_step for _ in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) # Test trained model # tf.argmax tensor 0,11tf.argmax(y,1)x  tf.argmax(y_,1)  tf.equal () [True, False, True, True][1,0,1,1]0.75 correct_prediction = tf.equal(tf.argmax(y, 1), y_) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print(sess.run( accuracy, feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels &#125;)) # SAVE THE MODEL # model builder = tf.saved_model.builder.SavedModelBuilder(&quot;/tmp/mnist/model/1620&quot;) builder.add_meta_graph_and_variables( sess, [tf.saved_model.tag_constants.SERVING] ) builder.save()if __name__ == &apos;__main__&apos;: parser = argparse.ArgumentParser() parser.add_argument( &apos;--data_dir&apos;, type=str, default=&apos;/tmp/tensorflow/mnist/input_data&apos;, help=&apos;Directory for storing input data&apos;) FLAGS, unparsed = parser.parse_known_args() tf.app.run(main=main, argv=[sys.argv[0]] + unparsed) //Todo]]></content>
      <categories>
        <category> - tensorflow</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jep]]></title>
    <url>%2F2018%2F12%2F04%2Fusingjep%2F</url>
    <content type="text"><![CDATA[jep whatJep: (Java Embeded Python) Jep takes a different route and embeds CPython in Java using JNI. Long story short, if you need to include CPython modules (such as numpy) Jep is the way to go. cPython jep whypython python,lightgbm,tensorflowjavascala  python python restjava Jython,JyNI cpython jep  https://github.com/ninia/jep 2017  jep wiki pip  jepjava Jep()jep1234567891011121314151617181920Exception in thread &quot;main&quot; java.lang.UnsatisfiedLinkError: no jep in java.library.path at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867) at java.lang.Runtime.loadLibrary0(Runtime.java:870) at java.lang.System.loadLibrary(System.java:1122) at jep.MainInterpreter.initialize(MainInterpreter.java:121) at jep.MainInterpreter.getMainInterpreter(MainInterpreter.java:97) at jep.Jep.&lt;init&gt;(Jep.java:232) at jep.Jep.&lt;init&gt;(Jep.java:228) at jep.Jep.&lt;init&gt;(Jep.java:140) at JepTest.JepDemo$.delayedEndpoint$JepTest$JepDemo$1(JepDemo.scala:27) at JepTest.JepDemo$delayedInit$body.apply(JepDemo.scala:9) at scala.Function0$class.apply$mcV$sp(Function0.scala:34) at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) at scala.App$class.main(App.scala:76) at JepTest.JepDemo$.main(JepDemo.scala:9) at JepTest.JepDemo.main(JepDemo.scala) S.O.:  LD_LIBRARY_PATH  1-Djava.library.path=/usr/local/lib/python2.7/dist-packages/jep/ github jepissueBut setup.py cmdclass={ &#39;setup_java&#39;: setup_java, &#39;build_java&#39;: build_java, &#39;javadoc&#39;: javadoc, &#39;build_jar&#39;: build_jar, &#39;build&#39;: jep_build, &#39;build_ext&#39; : build_ext, &#39;build_scripts&#39;: build_scripts, &#39;install_lib&#39;: jep_install, &#39;clean&#39;: really_clean, &#39;test&#39;: test, },  python3 setup.py install_lib   numpy include found at /home/xxh/anaconda3/lib/python3.6/site-packages/numpy/core/include running install_lib running build_py running build_ext ln -sf jep.cpython-36m-x86_64-linux-gnu.so /home/xxh/anaconda3/lib/python3.6/site-packages/jep/libjep.so libjep.soidearun-Edit ConfigurationsVM 1-Djava.library.path=/home/xxh/anaconda3/lib/python3.6/site-packages/jep/ () 1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/xxh/anaconda3/lib/python3.6/site-packages/jep/ scalapythonpython]]></content>
      <categories>
        <category> - scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>python</tag>
        <tag>jep</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker]]></title>
    <url>%2F2018%2F12%2F04%2F%E4%BA%86%E8%A7%A3Docker%2F</url>
    <content type="text"><![CDATA[Docker What is Docker?    docker ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[--note]]></title>
    <url>%2F2018%2F12%2F04%2F%E4%BA%BA%E4%BA%BA%E9%83%BD%E6%98%AF%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86-note%2F</url>
    <content type="text"><![CDATA[     ,,,,, ,,,  :VS. ,,,,,,,,,,,,,,,,,,,, VS - VS ,,,,,,,, ,VS :VS. ,,, ,,,/,,, Demo,;,,,, ,, ,,,,, ,31,1,,?, ,,,,?,?,?,?! ,,,,,, ,,,;,, ,,,,,,,,,,,,, -13]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2018%2F12%2F04%2F%E4%BD%BF%E7%94%A8git%2F</url>
    <content type="text"><![CDATA[git   project a 12git checkout -b &apos;newbranch&apos;git push origin newbranch project b 12345678910111213141516git init//copy .gitignoregit add .git commit -m &apos;b&apos;git remote add origin git@***.gitgit pull origin sparks3git statusgit add .git commit -m &apos;sfs&apos;git push origin master]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%85%B3%E4%BA%8E%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[Java mockTDDTest-Driven Development  bug  bug 10   unit testing       TDDdaoH2BaseH2Testtest-h2-applicationContext.xmldaoservicemockito@RunWith(MockitoJUnitRunner.class)spring beanredisdiamondmqdao https://www.jianshu.com/p/a8e17afd8c90 ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%88%A9%E7%94%A8python%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[2 python           bit.ly1.usa.govmovie3 IPythonpython cmd Tab     ?  ??  %run  %paste   4 NumPynumpypython Numpyndarray:ndarrayshapedtype()]]></content>
      <categories>
        <category> - python</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>python</tag>
        <tag></tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparkS3]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%9C%A8spark%E4%B8%AD%E4%BD%BF%E7%94%A8S3%2F</url>
    <content type="text"><![CDATA[aws S3 awshadoopemrS3 aws build.sbt hadoop-awshadoop 1234567891011121314151617181920212223242526272829303132333435name := &quot;my-api-hdfs&quot;version := &quot;1.1.provided&quot;scalaVersion := &quot;2.11.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.core&quot; % &quot;jackson-core&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.core&quot; % &quot;jackson-databind&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.core&quot; % &quot;jackson-annotations&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.module&quot; %% &quot;jackson-module-scala&quot; % &quot;2.7.8&quot;dependencyOverrides += &quot;com.fasterxml.jackson.module&quot; % &quot;jackson-module-paranamer&quot; % &quot;2.7.8&quot;libraryDependencies += &quot;org.apache.hadoop&quot; % &quot;hadoop-client&quot; % &quot;3.1.0&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-awslibraryDependencies += &quot;org.apache.hadoop&quot; % &quot;hadoop-aws&quot; % &quot;3.1.0&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3libraryDependencies += &quot;com.amazonaws&quot; % &quot;aws-java-sdk-s3&quot; % &quot;1.11.338&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-corelibraryDependencies += &quot;com.amazonaws&quot; % &quot;aws-java-sdk-core&quot; % &quot;1.11.338&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-kmslibraryDependencies += &quot;com.amazonaws&quot; % &quot;aws-java-sdk-kms&quot; % &quot;1.11.338&quot; % &quot;provided&quot;// https://mvnrepository.com/artifact/org.apache.spark/spark-corelibraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.3.0&quot; % &quot;provided&quot;libraryDependencies += &quot;cn.**.aip&quot; %% &quot;cn-***-aip-common-databases&quot; % &quot;1.0.13&quot;mainClass in assembly := Some(&quot;AwsSupportS3&quot;)assemblyJarName in assembly := s&quot;my-api-hdfs1.1.0607.jar&quot; s3 scala 12345678910111213141516object SparkS3 &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(s&quot;sparkS3&quot;).setMaster(&quot;yarn&quot;) val spark = new SparkContext(conf) spark.hadoopConfiguration.set(&quot;fs.s3n.impl&quot;, &quot;com.amazon.ws.emr.hadoop.fs.EmrFileSystem&quot;) spark.hadoopConfiguration.set(&quot;fs.s3n.endpoint&quot;, &quot;s3.cn-north-1.amazonaws.com.cn&quot;) spark.hadoopConfiguration.set(&quot;fs.s3n.awsAccessKeyId&quot;, &quot;&quot;) spark.hadoopConfiguration.set(&quot;fs.s3n.secret.key&quot;, &quot;HM5&quot;) // val testweet = spark.textFile(&quot;s3n://-test-/testweet.json&quot;) // testweet.saveAsTextFile(s&quot;s3n://xuxianhong-test-data/data/$&#123;DateUtil.currentDate&#125;&quot;) spark.stop() &#125;&#125; java(aws) 123456789101112131415161718192021222324252627282930public class AwsSupportS3 &#123; public static void main(String args[])&#123; new AwsSupportS3().awsCodeS3(); &#125; public void awsCodeS3()&#123; AmazonS3 s3client = new AmazonS3Client(new DefaultAWSCredentialsProviderChain()); //Map&lt;String, EC2MetadataUtils.IAMSecurityCredential&gt; credentialMap = EC2MetadataUtils.getIAMSecurityCredentials(); Region region = Region.getRegion(Regions.CN_NORTH_1); s3client.setRegion(region); SparkConf sparkConf = new SparkConf().setAppName(&quot;test app&quot;).setMaster(&quot;yarn&quot;); JavaSparkContext jsc = new JavaSparkContext(sparkConf); jsc.hadoopConfiguration().set(&quot;fs.s3n.impl&quot;,&quot;com.amazon.ws.emr.hadoop.fs.EmrFileSystem&quot;); jsc.hadoopConfiguration().set(&quot;fs.s3n.endpoint&quot;, &quot;s3.cn-north-1.amazonaws.com.cn&quot;); jsc.hadoopConfiguration().set(&quot;fs.s3n.awsAccessKeyId&quot;, &quot;&quot;); jsc.hadoopConfiguration().set(&quot;fs.s3n.awsSecretAccessKey&quot;, &quot;//&quot;); JavaRDD&lt;String&gt; resource = jsc.textFile(&quot;s3n://--data//ham.txt&quot;); resource.cache(); String date = DateUtil.currentDate(); resource.saveAsTextFile(&quot;s3n://--data/data/&quot;+date); List&lt;String&gt; list = resource.collect(); System.out.println(&quot;---------------------&quot;+resource.count()); //System.out.println(list.toString()); jsc.close(); &#125;&#125; ]]></content>
      <categories>
        <category> - spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>S3</tag>
        <tag>aws</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%BA%8F%E5%88%97%E5%8C%96%E4%B8%8E%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[ rest+jsonjsonjson, scalajson play-json-extensions +22 field case class formatter and more for play-json case class ,play-json spray-json 1234567891011121314151617181920package data.creationimport ai.x.play.json.Jsonximport common.HttpClientHelperimport data.beans.LastInfoimport play.api.libs.json.Json/** * Created by xxh on 18-7-30. */object LastInfoHelper &#123; implicit lazy val lastInfoFormat = Jsonx.formatCaseClass[LastInfo] def getLastInfo(url:String,email:String,shoeLastBaseNo:String,basicsize:Int): LastInfo =&#123; val lastArray = new HttpClientHelper().post(url = url, postJsonObj = s&quot;&quot;&quot;&#123;&quot;email&quot;:&quot;$email&quot;,&quot;shoeLastBaseNo&quot;:&quot;$shoeLastBaseNo&quot;,&quot;basicsize&quot;:$basicsize&#125;&quot;&quot;&quot;) val lastJsonArray = Json.parse(lastArray) val lastInfo = lastJsonArray(0).as[LastInfo] lastInfo &#125;&#125;]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[N()  MonolithMicroservice monolithmicroservice  :   ,  n devops API gateway :lagom]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HttpResponse]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%8E%8C%E6%8F%A1HttpResponse%2F</url>
    <content type="text"><![CDATA[HTTPHTTP http ASCII //ToDo http  : //[]  HTTP-Version Status-Code Reason-Phrase CRLF HTTP-VersionHTTPStatus-CodeReason-Phrase 1xx2xx3xx4xx5xx 200 OK400 Bad Request401 UnauthorizedWWW-Authenticate403 Forbidden404 Not FoundURL500 Internal Server Error503 Server UnavailableHTTP/1.1 200 OKCRLF]]></content>
      <categories>
        <category> - web</category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>HttpResponse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%94%92%E6%8A%80%E8%83%BD%2F</url>
    <content type="text"><![CDATA[  ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[XPScrumLean Software DevelopmentDSDMFeature Driver DevelopmentCrystal Clear   -   -      Story          story  1  41.1  41.2  41.3  42  43  74  95        ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[  =  +  .1   ,   T(n)  T(n) .1  ####  O(1)O(logn)O(n)O(n^2)O(2^r) #####  Last-in-first-outLIFOPushPop  ADT java.util.Stack push()pop()peek() top()getSize() empty() isEmpty() pop() peek() ExceptionStackEmpty  JVM]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%97%A5%E5%BF%97%E6%89%93%E5%8D%B0%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[ v2.0(ID) ]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag>log</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01%2F</url>
    <content type="text"><![CDATA[  rel   , , . , , , . , , , . , , . , , . , , , , . , .  , , , . , , . , , , , , . , . optimization()optimization: Newtons method() Least Squares method( ) Gradient Descent()  cost = (predicted - real)^2 = (Wx-y)^2 (W-0)^2 W ##### and  ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[python bs4 request urllib2  Python  csv]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>Http</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux shell]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%94%A8shell%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[NLinux shell  ftp get  ftp get  11234567891011121314151617181920212223242526272829303132#!/bin/bash#dirname    pwdbasedir=$(cd $(dirname $0); pwd)cd $basedirconfigFile=$basedir/ftphost.confhomedir=$(awk -F &apos;=&apos; &apos;/^homedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)ftphomedir=$(awk -F &apos;=&apos; &apos;/^ftphomedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)grep -i -E -w &quot;ftp|sftp&quot; $configFile | awk -v hdir=$homedir -v fdir=$ftphomedir -F &quot;,&quot; &#123; protocol=$1 ip=$2 port=$3 user=$4 password=$5 ftp -n&lt;&lt;FTPDopen $ipuser $user $passwordbinarypassivelcd $hdirpromptif [[ ! -d $fdir ]];then mkdir $fdirficd $fdirput $2closequitFTPD 1234567891011121314151617181920212223242526272829303132333435363738394041424344#! /bin/bash#dirname    pwdbasedir=$(cd $(dirname $0); pwd)echo $basedircd $basedirconfigFile=$basedir/ftphost.confftpscriptfile=$basedir/ftpscript.shhomedir=$(awk -F &apos;=&apos; &apos;/^homedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)ftphomedir=$(awk -F &apos;=&apos; &apos;/^ftphomedir/ &#123;print $2&#125;&apos; $configFile |sed &apos;s/ //g&apos;)echo &quot;homedir: &quot;$homedirecho &quot;ftphomedir&quot; $ftphomedirecho &quot;#ftp&quot; &gt;&gt;$ftpscriptfilegrep -i -E -w &quot;ftp|sftp&quot; $configFile | awk -v hdir=$homedir -v fdir=$ftphomedir -F &quot;,&quot; &apos;&#123; # protocol=$1 # ip=$2 # port=$3 # user=$4 # password=$5 print &quot;ftp -n&lt;&lt;FTPD&quot; print &quot;open &quot;ip print &quot;user &quot; user &quot; &quot; password print &quot;binary&quot; print &quot;passive&quot; print &quot;cd &quot; fdir &#125;&apos; &gt;&gt;$ftpscriptfile echo &quot;prompt&quot; &gt;&gt;$ftpscriptfile echo &quot;if [[ ! -d \$3 ]];then&quot; &gt;&gt;$ftpscriptfile echo &quot; mkdir \$3&quot; &gt;&gt;$ftpscriptfile echo &quot;fi &quot; &gt;&gt;$ftpscriptfile echo &quot;cd \$3&quot; &gt;&gt;$ftpscriptfile echo &quot;if [[ ! -d \$4 ]];then&quot; &gt;&gt;$ftpscriptfile echo &quot; mkdir \$4&quot; &gt;&gt;$ftpscriptfile echo &quot;fi &quot; &gt;&gt;$ftpscriptfile echo &quot;cd \$4&quot; &gt;&gt;$ftpscriptfile echo &quot;lcd \$1&quot; &gt;&gt;$ftpscriptfile echo &quot;put \$2&quot; &gt;&gt;$ftpscriptfile echo &quot;close&quot; &gt;&gt;$ftpscriptfile echo &quot;quit&quot; &gt;&gt;$ftpscriptfile echo &quot;FTPD&quot; &gt;&gt;$ftpscriptfile 12345678910111213141516171819202122232425262728293031 1 filterAndGet() 2 &#123; 3 srcfile=$1 4 key=$2 5 value=$3 6 tgtfile=$4 7 shift 4 8 if [[ $# -gt 0 ]];then 9 ids=$*10 else11 ids=012 fi1314 awk -F &quot;,&quot; &apos;BEGIN &#123;15 split(&apos;&quot;\&quot;$ids&quot;\&quot;&apos;, indexs,&quot; &quot;)16 &#125;&#123;17 if($&apos;$key&apos;==&quot;&apos;$value&apos;&quot;) &#123;18 for (i=1;i&lt;=length(indexs);i++)&#123;19 if(indexs[i]==9)&#123;20 system(&quot;\\echo -n $(\\date -d \&quot;2000-01-01 +&quot; $indexs[i] &quot; seconds\&quot; +\&quot;%Y-%m-%d %H:%M:%S\&quot;)&quot;)21 printf(&quot;.%03d,&quot;,$28)22 &#125;23 else24 printf(&quot;%s,&quot;,$indexs[i])25 &#125;26 printf(&quot;\n&quot;);27 &#125;28 &#125;&apos; $srcfile &gt;&gt; $tgtfile29 &#125;filterAndGet $1 39 0 $2/a_$&#123;day&#125;.csv 9 14 15 52 2 3 5 10 11 53 54 55 56 57 58 59 88 16 6 7 8 21 35 34 23 86 17 19 20 2IP3]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag>shell</tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A7%8B%E7%BB%88%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%2F</url>
    <content type="text"><![CDATA[ cpuIO CPUCPU CPUCPU CPU i7cpu            Mutex CPU  actorakka]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[,, ####   Java : ; ; , java ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%84%9A%E5%9E%8B%E6%95%B0%E6%8D%AE%E3%80%81%E6%A5%A6%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E6%96%B9%E6%A1%88%E7%A8%BF%2F</url>
    <content type="text"><![CDATA[  ##   ##     ####                - Unix  0,1,2  null]]></content>
      <categories>
        <category>datawarehouse</category>
      </categories>
      <tags>
        <tag>datawarehouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%A1%8C%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[   atomatomic operation MQMQMQ stream (munge/munging/wrangling)]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%B7%B3%E6%9D%BF%E6%9C%BA%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[2000PS:  Jsch  123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147import com.jcraft.jsch.JSch;import com.jcraft.jsch.Session;import com.jcraft.jsch.UIKeyboardInteractive;import com.jcraft.jsch.UserInfo;import javax.swing.*;import java.awt.*;import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;public class PortForwardingL4Aip &#123; public static void main(String[] arg)&#123; int lport; String rhost; int rport; try&#123; JSch jsch=new JSch(); // Session session=jsch.getSession(&quot;&quot;, &quot;ip&quot;, ssh); session.setPassword(&quot;&quot;); // lport = 9527; //host rhost = &quot;rhost&quot;; // rport = 61539; // username and password will be given via UserInfo interface. //MyUserInfoUserInfotrace // session.setPassword(&quot;&quot;); UserInfo //UserInfo ui=new MyUserInfo(); //session.setUserInfo(ui); // session.setConfig(&quot;StrictHostKeyChecking&quot;, &quot;no&quot;); session.connect(); int assinged_port=session.setPortForwardingL(lport, rhost, rport); System.out.println(&quot;localhost:&quot;+assinged_port+&quot; -&gt; &quot;+rhost+&quot;:&quot;+rport); //  Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;); Connection conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:9527/sync?user=xu_foot_last&quot;); Statement statement = conn.createStatement(); ResultSet resultSet = statement.executeQuery(&quot;show databases&quot;); resultSet.first(); System.out.print(resultSet.getString(&quot;Database&quot;)); &#125; catch(Exception e)&#123; System.out.println(e); &#125; &#125; public static class MyUserInfo implements UserInfo, UIKeyboardInteractive&#123; public String getPassword()&#123; return passwd; &#125; public boolean promptYesNo(String str)&#123; Object[] options=&#123; &quot;yes&quot;, &quot;no&quot; &#125;; int foo=JOptionPane.showOptionDialog(null, str, &quot;Warning&quot;, JOptionPane.DEFAULT_OPTION, JOptionPane.WARNING_MESSAGE, null, options, options[0]); return foo==0; &#125; String passwd; JTextField passwordField=(JTextField)new JPasswordField(20); public String getPassphrase()&#123; return null; &#125; public boolean promptPassphrase(String message)&#123; return true; &#125; public boolean promptPassword(String message)&#123; Object[] ob=&#123;passwordField&#125;; int result= JOptionPane.showConfirmDialog(null, ob, message, JOptionPane.OK_CANCEL_OPTION); if(result==JOptionPane.OK_OPTION)&#123; passwd=passwordField.getText(); return true; &#125; else&#123; return false; &#125; &#125; public void showMessage(String message)&#123; JOptionPane.showMessageDialog(null, message); &#125; final GridBagConstraints gbc = new GridBagConstraints(0,0,1,1,1,1, GridBagConstraints.NORTHWEST, GridBagConstraints.NONE, new Insets(0,0,0,0),0,0); private Container panel; public String[] promptKeyboardInteractive(String destination, String name, String instruction, String[] prompt, boolean[] echo)&#123; panel = new JPanel(); panel.setLayout(new GridBagLayout()); gbc.weightx = 1.0; gbc.gridwidth = GridBagConstraints.REMAINDER; gbc.gridx = 0; panel.add(new JLabel(instruction), gbc); gbc.gridy++; gbc.gridwidth = GridBagConstraints.RELATIVE; JTextField[] texts=new JTextField[prompt.length]; for(int i=0; i&lt;prompt.length; i++)&#123; gbc.fill = GridBagConstraints.NONE; gbc.gridx = 0; gbc.weightx = 1; panel.add(new JLabel(prompt[i]),gbc); gbc.gridx = 1; gbc.fill = GridBagConstraints.HORIZONTAL; gbc.weighty = 1; if(echo[i])&#123; texts[i]=new JTextField(20); &#125; else&#123; texts[i]=new JPasswordField(20); &#125; panel.add(texts[i], gbc); gbc.gridy++; &#125; if(JOptionPane.showConfirmDialog(null, panel, destination+&quot;: &quot;+name, JOptionPane.OK_CANCEL_OPTION, JOptionPane.QUESTION_MESSAGE) ==JOptionPane.OK_OPTION)&#123; String[] response=new String[prompt.length]; for(int i=0; i&lt;prompt.length; i++)&#123; response[i]=texts[i].getText(); &#125; return response; &#125; else&#123; return null; // cancel &#125; &#125; &#125;&#125; Scala database]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E9%B8%A1%E6%B1%A4%E6%AF%8F%E5%A4%A9%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[                             [](https://www. zhihu. com/question/21515151/answer/41371992) ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FTP]]></title>
    <url>%2F2018%2F12%2F04%2FFTP%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[FTPTCP FTPSFTP FTP TCP21    FTP FTPTCP   java: 1,21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package ftp;import org.apache.commons.net.ftp.FTPClient;import org.apache.commons.net.ftp.FTPReply;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;/** * Created by on 2016/9/22. */public class FtpUtil &#123; public static boolean uploadFile(String url, int port, String userName, String password, String path, String fileName, InputStream input) &#123; boolean success = false; FTPClient ftpClient = new FTPClient(); int reply; try &#123; ftpClient.connect(url, port); ftpClient.login(userName, password); reply = ftpClient.getReplyCode(); if (!FTPReply.isPositiveCompletion(reply)) &#123; ftpClient.disconnect(); return success; &#125; ftpClient.changeWorkingDirectory(path); ftpClient.setBufferSize(1024); ftpClient.setControlEncoding(&quot;utf-8&quot;); ftpClient.setFileType(FTPClient.BINARY_FILE_TYPE); ftpClient.storeFile(fileName, input); input.close(); ftpClient.logout(); success = true; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (ftpClient.isConnected())&#123; try &#123; ftpClient.disconnect(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return success; &#125; public static void main(String args[])&#123; try &#123; FileInputStream in = new FileInputStream(&quot;D:/FtpUtil.java&quot;); boolean flag = uploadFile(&quot;127.0.01&quot;,21,&quot;root&quot;,&quot;root123&quot;,&quot;D:/&quot;,&quot;FileUtil.java.java&quot;,in); System.out.println(&quot;flag: &quot; + flag); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; scala:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294package webapp.utils.ftpimport org.apache.commons.net.ftp.&#123;FTP, FTPReply, FTPClient&#125;import org.apache.commons.net.PrintCommandListenerimport java.io._import scala.util.&#123;Failure, Success, Try&#125;import scala.Someclass FtpFileSystemAccessor(server: String, port: String, user: String, password: String) extends FtpAccessor &#123; def using(f: AnyRef =&gt; Unit): Unit = &#123; getConnection match &#123; case Some(ftp: FTPClient) =&gt; try &#123; f(ftp) &#125; catch &#123; case e: IOException =&gt; log.error(s&quot;Could not connect to server due to $&#123;e.getMessage&#125;&quot;, e) case e: Throwable =&gt; log.error(e.getMessage, e) &#125; finally &#123; close(ftp) &#125; case _ =&gt; throw new Exception(&quot;Ftp type error&quot;) &#125; &#125; protected def getConnection: Some[AnyRef] = &#123; val ftp = new FTPClient() if (port == &quot;&quot;) &#123; ftp.connect(server) &#125; else &#123; ftp.connect(server, port.toInt) &#125; val reply = ftp.getReplyCode ftp.addProtocolCommandListener(new PrintCommandListener(new PrintWriter(System.out))) // if (!FTPReply.isPositiveCompletion(reply)) &#123; throw new Exception(&quot;Ftp server refused connection&quot;) &#125; if (!ftp.login(user, password)) &#123; ftp.logout() throw new Exception(&quot;FTP server refused login.&quot;) &#125; Some(ftp) &#125; protected def close(ftp: AnyRef): Unit = &#123; ftp match &#123; case ftp: FTPClient =&gt; if (ftp.isConnected) &#123; Try(ftp.noop()) Try(ftp.logout()) Try(ftp.disconnect()) &#125; case _ =&gt; throw new Exception(&quot;Ftp type error&quot;) &#125; &#125; def makeDir(remote: String): String = &#123; var checkedRemotePath: String = null using &#123; case ftp: FTPClient =&gt; checkedRemotePath = makeDir(ftp, remote) case _ =&gt; &#125; checkedRemotePath &#125; private def makeDir(ftp: FTPClient, remote: String): String = &#123; def remotePathVerified(path: String): String = path.take(1) match &#123; case &quot;.&quot; =&gt; remotePathVerified(path.drop(1)) case &quot;/&quot; =&gt; path.drop(1) case _ =&gt; path &#125; val checkedRemotePath = remotePathVerified(remote) checkedRemotePath.split(&apos;/&apos;).init.foldLeft(&quot;.&quot;)((dir, a) =&gt; &#123; ftp.makeDirectory(dir + &quot;/&quot; + a) dir + &quot;/&quot; + a &#125;) checkedRemotePath &#125; def makeDirectoryRecursive(pathname: String): Boolean = &#123; def directoryHelperFTP(ftpClient: FTPClient, dir: File, res: List[String] = Nil): List[String] = &#123; val path = dir.getPath.replace(&quot;\\&quot;, &quot;/&quot;) Try(ftpClient.getStatus(path)) match &#123; case Success(_) =&gt; res case Failure(_) =&gt; directoryHelperFTP(ftpClient, dir.getParentFile, path :: res) &#125; &#125; var isSuccess:Boolean = false using &#123; case ftp: FTPClient =&gt; isSuccess = directoryHelperFTP(ftp, new File(pathname)).forall(dir =&gt; ftp.makeDirectory(pathname)) case _ =&gt; &#125; isSuccess &#125; override def deleteDirectory(remote: String): Int = &#123; var isDeleteDirectorySuccess: Int = 0 using &#123; case ftp: FTPClient =&gt; ftp.setFileType(FTP.BINARY_FILE_TYPE) val remoteWithoutPoint: String = remote.take(1) match &#123; case &quot;.&quot; =&gt; remote.drop(1) case _ =&gt; remote &#125; isDeleteDirectorySuccess = deleteDirectory(ftp, remoteWithoutPoint) case _ =&gt; &#125; isDeleteDirectorySuccess &#125; private def deleteDirectory(ftp: FTPClient, remoteWithoutPoint: String): Int = &#123; ftp.listFiles(s&quot;./$remoteWithoutPoint&quot;).foreach(f =&gt; &#123; if (f.isDirectory) &#123; val filePath = s&quot;$remoteWithoutPoint/$&#123;f.getName&#125;&quot; deleteDirectory(ftp, filePath) ftp.rmd(s&quot;./$filePath&quot;) ftp.changeWorkingDirectory(s&quot;./$filePath/$&#123;f.getName&#125;&quot;) &#125; else if (f.isFile) &#123; ftp.deleteFile(s&quot;./$remoteWithoutPoint/$&#123;f.getName&#125;&quot;) &#125; &#125;) ftp.rmd(s&quot;./$remoteWithoutPoint&quot;) &#125; override def delete(pathname: String): Unit = &#123; using &#123; case ftp: FTPClient =&gt; ftp.deleteFile(pathname) case _ =&gt; &#125; &#125; def isExists(file: String, filter: String =&gt; Boolean = f =&gt; true): Boolean = &#123; var rtnValue: Boolean = false using &#123; case ftp: FTPClient =&gt; var index = file.lastIndexOf(&apos;/&apos;) if (index &lt; 0) index = file.lastIndexOf(&apos;\\&apos;) val parentPath = file.substring(0, index) val fileName = file.substring(index + 1, file.length) val result = ftp.listNames(parentPath) val files = result.filter(filter) println(result.mkString(&quot;,&quot;)) val fileNames = files.toList.map(x =&gt; x.split(&quot; &quot;).last) rtnValue = fileNames.contains(fileName) case _ =&gt; &#125; rtnValue &#125; override def download(src: String, dst: String, timeout: Int): Unit = &#123; using &#123; case ftp: FTPClient =&gt; ftp.setFileType(FTP.BINARY_FILE_TYPE) ftp.enterLocalPassiveMode() ftp.setBufferSize(1024 * 1024) ftp.setDataTimeout(timeout) //,, download(ftp, src, dst) case _ =&gt; &#125; &#125; private def download(ftp: FTPClient, src: String, dst: String): Boolean = &#123; ftp.setFileType(FTP.BINARY_FILE_TYPE) var index = src.lastIndexOf(&apos;/&apos;) if (index == -1) index = src.lastIndexOf(&apos;\\&apos;) val fileName = src.substring(index + 1, src.length) val localFile = new File(dst + &quot;/&quot; + fileName) val localStream = new FileOutputStream(localFile) val isDownloadSuccessful = ftp.retrieveFile(src, localStream) localStream.close() ftp.noop() // check that control connection is working OK ftp.logout() isDownloadSuccessful &#125; /** *  * relativePath ftp?&quot;? * srcDir  * baseDstDir  * ext  */ override def downloadByExt(srcDir: String, baseDstDir: String, ext: String): Unit = &#123; using &#123; case ftp: FTPClient =&gt; downloadByExt(ftp, &quot;&quot;, srcDir, baseDstDir, ext) case _ =&gt; &#125; &#125; private def downloadByExt(ftp: FTPClient, relativePath: String, srcDir: String, baseDstDir: String, ext: String): Unit = &#123; val array = srcDir.split(&quot;/&quot;).toList array.foreach(x =&gt; if (x != &quot;&quot;) ftp.changeWorkingDirectory(x)) ftp.listFiles.foreach(f =&gt; &#123; if (f.isFile) &#123; if (f.getName.endsWith(ext)) &#123; val dst = baseDstDir + &quot;/&quot; + relativePath val src = f.getName val file = new File(dst + &quot;/&quot; + f.getName) if (!file.exists) &#123; if (!file.getParentFile.exists) file.getParentFile.mkdirs download(ftp, src, dst) &#125; &#125; &#125; else if (f.isDirectory) &#123; val relativeDir = if (relativePath == &quot;&quot;) f.getName else relativePath + &quot;/&quot; + f.getName downloadByExt(ftp, relativeDir, f.getName, baseDstDir, ext) &#125; &#125;) ftp.changeToParentDirectory() &#125; override def upload(localStream: InputStream, remote: String): Boolean = &#123; var result = false using &#123; case ftp: FTPClient =&gt; ftp.setFileType(FTP.BINARY_FILE_TYPE) val remotePath = makeDir(remote) result = ftp.storeFile(s&quot;./$remotePath&quot;, localStream) localStream.close() case _ =&gt; &#125; result &#125; override def upload(local: String, remote: String): Boolean = &#123; val input = new FileInputStream(local) upload(input, remote) &#125; def list(parentPath: String, filter: String =&gt; Boolean = f =&gt; true): Array[String] = &#123; var result: Array[String] = Array.empty using &#123; case ftp: FTPClient =&gt; result = ftp.listFiles(parentPath).map(_.getName).filter(filter) case _ =&gt; &#125; result &#125; def listFiles(parentPath: String): Array[String] = &#123; var result: Array[String] = Array.empty using &#123; case ftp: FTPClient =&gt; val files = ftp.listFiles(parentPath).filter(_.isFile) result = files.map(_.getName) case _ =&gt; &#125; result &#125; def listDirectories(parentPath: String): Array[String] = &#123; var result: Array[String] = Array.empty using &#123; case ftp: FTPClient =&gt; val files = ftp.listDirectories(parentPath) result = files.map(_.getName) case _ =&gt; &#125; result &#125; def rename(src:String, dst:String):Boolean = &#123; var result: Boolean = true using &#123; case ftp: FTPClient =&gt; ftp.rename(src,dst) result = true case _ =&gt; &#125; result &#125;&#125;object FtpFileSystemAccessor &#123; def apply(ftpInfo: FtpServerConfiguration): FtpFileSystemAccessor = new FtpFileSystemAccessor(ftpInfo.ip, ftpInfo.port, ftpInfo.user, ftpInfo.password)&#125;]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow1--morvan]]></title>
    <url>%2F2018%2F12%2F04%2FTensorflow1--morvan%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[tensorflow TensorFlowGooglePython, .TensorFlow , , Python  C++, . TensorFlow  . .TensorFlow, . TensorFlow , , . , . tensorflow 123sudo apt-get install python-pip python-devpip install tensorflow pippip y=ax+b y = a*x + b xinputs, youtputs, ab. ,ab,  a=0.5, b=2,x=3,  0.5*3 + 2 . , , . tensorflow tensorflow :123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3# ,  tf.Variable  y .#  y_data = x_data*0.1 + 0.3#  y=Weights * x + biases,#  Weights  0.1, biases  0.3.,#  tf.Variable  y .#  y_data = x_data*0.1 + 0.3  y=Weights * x + biases,#  Weights  0.1, biases  0.3.#---create model---Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biases#------#---calculate loss---loss = tf.reduce_mean(tf.square(y-y_data))#------#--- ---optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)#------#---train---init = tf.global_variables_initializer() # sess = tf.Session()sess.run(init) # Very importantfor step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases))#------#---save model---saver = tf.train.Saver()save_path = saver.save(sess, &quot;my_net/save_net.ckpt&quot;)print(&quot;Save to path: &quot;, save_path)print(&quot;-------------------------------------------------------&quot;)#------#---restore and train---saver.restore(sess, &quot;my_net/save_net.ckpt&quot;)for step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases)) tensorflow training tensorflow(data flow graphs) ((tensor))(Nodes)(edges)tensorflowtensorflow Tensor() (scalar)1 (vector),[1,2,3] (matrix),[[1,2,3],[3,4,5],[4,5,6]]  SessionSession  Tensorflow ,.  session.run() , . ps:sparksessionscrunacton 123456789101112131415161718192021import tensorflow as tf# create two matrixesmatrix1 = tf.constant([[3,3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1,matrix2)# method 1sess = tf.Session()result = sess.run(product)print(result)sess.close()# [[12]]# method 2with tf.Session() as sess: result2 = sess.run(product) print(result2)# [[12]] Variable  Tensorflow  Python   state = tf.Variable()  init:  init = tf.initialize_all_variables() placeholder Tensorflow  placeholder , placeholder  Tensorflow . Tensorflow data,  tf.placeholder(),  sess.run(**, feed_dict={input: *}). 1234567891011import tensorflow as tf# Tensorflow  placeholder  type  float32 input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)# mul = multiply input1input2  outputouput = tf.multiply(input1, input2)#,  sess.run() , feed_dict=&#123;&#125;  input. placeholder  feed_dict=&#123;&#125; with tf.Session() as sess: print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;))  Tensorflow   activation function tensorflow  +matplot 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&quot;&quot;&quot;Please note, this code is only for python 3+. If you are using python 2+, please modify the code accordingly.&quot;&quot;&quot;from __future__ import print_functionimport tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltdef add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# Make up some real datax_data = np.linspace(-1,1,300)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape)y_data = np.square(x_data) - 0.5 + noise# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1])# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediciton and real dataloss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split(&apos;.&apos;)[1]) &lt; 12 and int((tf.__version__).split(&apos;.&apos;)[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)# plot the real datafig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data, y_data)plt.ion()plt.show()for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to visualize the result and improvement try: ax.lines.remove(lines[0]) except Exception: pass prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;) # plot the prediction lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw=5)plt.pause(0.1)]]></content>
      <categories>
        <category> - tensorflow</category>
      </categories>
      <tags>
        <tag></tag>
        <tag>ml</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[UsingZookeeper]]></title>
    <url>%2F2018%2F12%2F04%2FUsingZookeeper%2F</url>
    <content type="text"><![CDATA[welcomeApache ZooKeeper is an effort to develop and maintain an open-source server which enables highly reliable distributed coordination.(zookeeper ) ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services.All of these kinds of services are used in some form or another by distributed applications.()Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable.()Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage.Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed. source code readingidea zookeeper ant  123git clone git@github.com:apache/zookeeper.gitcd zookeeperant eclipse  idea  zookeeper project file -&gt; new -&gt; project from existing source -&gt;  -&gt; eclipse -&gt; ok   projectzookeeper-notes,zkServer.sh :Hadoop  zookeeper API: create group , join group, delete group  zookeeper  zookeeper (zookeeper ) data model() summary znode: dataACL(Access Control List)  path znode, Ephemeral Znodes znodeephemeralpersistentznodeznode znodesessionephemeralznodepersistentznode Ephemeral znodeEphemeral znodesessionACL Ephemeral znodeEphemeral znodeZnode znodeZooKeeperznodeznode znodeznode/a/b-ZooKeeper/a/b-3znode /a/b-znodeZooKeeper/a/b-5znodeZooKeeper Java APIcreate()znodeA Lock Serviceznodeshare lock watches znodeZooKeeperznodeexistsznodeznode znodeexistsfalseznodeznode znodeznode znodeznodeexistsA Configuration Serviceoperations() base: create delete exits getACL setACL getChildren getData setData sync(Synchronizes a clients view of a znode with ZooKeeper) deletesetDataznodeversion number muti update Implementation()Consistency()Sessions() READEDIT zookeeperzookeeperzookeeper zkwatcher  masterworkerhdfsnamenode znodestringvalue,znodepathkey client,client class ActiveKeyValueStore: 1234567891011121314151617181920212223242526272829303132package usingzookeeper.configurationservice;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs;import org.apache.zookeeper.data.Stat;import usingzookeeper.ConnectionWatcher;import java.nio.charset.Charset;/** * Created by xxh on 17-12-13. */public class ActiveKeyValueStore extends ConnectionWatcher &#123; private static final Charset CHARSET = Charset.forName(&quot;UTF-8&quot;); public void write(String path,String value) throws InterruptedException,KeeperException&#123; Stat stat = zk.exists(path,false); if (stat == null)&#123; zk.create(path,value.getBytes(CHARSET), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125;else &#123; zk.setData(path,value.getBytes(CHARSET),-1); &#125; &#125; public String read(String path, Watcher watcher) throws KeeperException, InterruptedException &#123; byte[] data = zk.getData(path,watcher,null); return new String(data,CHARSET); &#125;&#125; class ConfigUpdater 123456789101112131415161718192021222324252627282930313233343536package usingzookeeper.configurationservice;import org.apache.zookeeper.KeeperException;import java.io.IOException;import java.util.Random;import java.util.concurrent.TimeUnit;/** * Created by xxh on 17-12-13. */public class ConfigUpdater &#123; public static final String PATH = &quot;/config&quot;; private ActiveKeyValueStore store; private Random random = new Random(); public ConfigUpdater(String hosts) throws IOException, InterruptedException &#123; store = new ActiveKeyValueStore(); store.connect(hosts); &#125; public void run() throws KeeperException, InterruptedException &#123; while (true)&#123; String value = random.nextInt(100) + &quot;&quot;; store.write(PATH,value); System.out.printf(&quot;Set %s to %s\n&quot;,PATH,value); TimeUnit.SECONDS.sleep(random.nextInt(10)); &#125; &#125; public static void main(String[] args) throws Exception&#123; ConfigUpdater configUpdater = new ConfigUpdater(&quot;localhost&quot;); configUpdater.run(); &#125;&#125; class ConfigUpdater: 123456789101112131415161718192021222324252627282930313233343536package usingzookeeper.configurationservice;import org.apache.zookeeper.KeeperException;import java.io.IOException;import java.util.Random;import java.util.concurrent.TimeUnit;/** * Created by xxh on 17-12-13. */public class ConfigUpdater &#123; public static final String PATH = &quot;/config&quot;; private ActiveKeyValueStore store; private Random random = new Random(); public ConfigUpdater(String hosts) throws IOException, InterruptedException &#123; store = new ActiveKeyValueStore(); store.connect(hosts); &#125; public void run() throws KeeperException, InterruptedException &#123; while (true)&#123; String value = random.nextInt(100) + &quot;&quot;; store.write(PATH,value); System.out.printf(&quot;Set %s to %s\n&quot;,PATH,value); TimeUnit.SECONDS.sleep(random.nextInt(10)); &#125; &#125; public static void main(String[] args) throws Exception&#123; ConfigUpdater configUpdater = new ConfigUpdater(&quot;localhost&quot;); configUpdater.run(); &#125;&#125; ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[()]]></title>
    <url>%2F2018%2F12%2F04%2F%E5%9B%A2%E9%98%9F%E5%8D%8F%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[   52  15   16 0 1   QQ   h5O2O3  N 1 2  3 4 5 PK         169-1214-17 28-9500 39 4google 5    ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[post]]></title>
    <url>%2F2018%2F12%2F04%2F%E6%8E%8C%E6%8F%A1post%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[restsprayGET REQUESTPOST REQUEST HTTP/1.1  HTTP  OPTIONSGETHEADPOSTPUTDELETETRACECONNECT  POST  POST HTTP  ASCII  TCP/IP  HTTP  123&lt;method&gt; &lt;request-URL&gt; &lt;version&gt;&lt;headers&gt;&lt;entity-body&gt;  POST entity-body HTTP  phppython  frameworkheaders Content-Type  POST  Content-Type POST  application/x-www-form-urlencoded multipart/form-data application/json text/xml spray  handle  request 1. application/x-www-form-urlencoded  POST   enctype  application/x-www-form-urlencoded  123POST http://www.example.com HTTP/1.1Content-Type: application/x-www-form-urlencoded;charset=utf-8title=test&amp;sub%5B%5D=1&amp;sub%5B%5D=2&amp;sub%5B%5D=3 Content-Type  application/x-www-form-urlencoded key1=val1&amp;key2=val2 key  val  URL  PHP $_POST[title]  title $_POST[sub]  sub  Ajax  JQuery  QWrap  AjaxContent-Type application/x-www-form-urlencoded;charset=utf-8 spray handle 12345678910111213141516171819202122232425path(&quot;uploadMeasurementItems&quot;) &#123; post &#123; detach() &#123; entity(as[FormData]) &#123; measurementItems =&gt; &#123; complete(s&quot;&quot;&quot;--token: $&#123;measurementItems.fields(0) _2&#125;&quot;&quot;&quot;) /*if (measurementItems.token == AuthorityConfig.authorityToken)&#123; new MeasurementItemsService(measurementItems).upload2Redshift match &#123; case true =&gt; complete(200,&quot;ok,upload measurementItems success&quot;) case false =&gt; complete(500,&quot;inner server error&quot;) &#125; //Todo: dataredshift //complete(&quot;measurementItems type: &quot; + measurementItems.flag + &quot;measurementItems data: &quot; + measurementItems.measurementItem) &#125; else &#123; complete(401,&quot;token error,access denied&quot;) &#125;*/ &#125; &#125; &#125; &#125; &#125;  FormData class 1234567891011121314151617181920package spray.httpimport spray.http.HttpHeaders._sealed trait HttpForm &#123; type FieldType def fields: Seq[FieldType]&#125;/** * Model for `application/x-www-form-urlencoded` form data. */case class FormData(fields: Seq[(String, String)]) extends HttpForm &#123; // TODO: better FormData(query: Uri.Query)? type FieldType = (String, String)&#125;object FormData &#123; val Empty = FormData(Seq.empty) def apply(fields: Map[String, String]): FormData = this(fields.toSeq)&#125;  case class FormData Model for application/x-www-form-urlencoded form data entity-body  key-value  fields Seq(String,String),field 2. multipart/form-datamultipart/form-datahandle case class 123456789101112131415161718/** * Model for `multipart/form-data` content as defined in RFC 2388. * All parts must contain a Content-Disposition header with a type form-data * and a name parameter that is unique */case class MultipartFormData(fields: Seq[BodyPart]) extends HttpForm &#123; // TODO: `fields: BodyPart*` is probably better type FieldType = BodyPart def get(partName: String): Option[BodyPart] = fields.find(_.name.exists(_ == partName))&#125;object MultipartFormData &#123; val Empty = MultipartFormData(Seq.empty) def apply(fields: Map[String, BodyPart]): MultipartFormData = this&#123; fields.map &#123; case (key, value)  value.copy(headers = `Content-Disposition`(&quot;form-data&quot;, Map(&quot;name&quot; -&gt; key)) +: value.headers) &#125;(collection.breakOut) &#125;&#125; entity-body fields: Seq[BodyPart] get   POST    enctype  application/x-www-form-urlencoded enctype  text/plain Web  WebApp Ajax  3. application/json application/json  Content-Type  JSON  JSON  IE  JSON.stringify JSON  JSON JSON  JSON  JSON  val x-www-form-urlencoded  AJAXjson  RESTful  Chrome FirebugFiddler JSON  12345678910111213141516171819202122232425path(&quot;uploadMeasurementItems&quot;/&quot;another&quot;) &#123; post &#123; detach() &#123; entity(as[MeasurementItems]) &#123; measurementItems =&gt; &#123; //complete(s&quot;&quot;&quot;--token: $&#123;measurementItems.fields(0)&#125;&quot;&quot;&quot;) if (measurementItems.token == AuthorityConfig.authorityToken)&#123; new MeasurementItemsService(measurementItems).upload2Redshift match &#123; case true =&gt; complete(200,&quot;ok,upload measurementItems success&quot;) case false =&gt; complete(500,&quot;inner server error&quot;) &#125; //Todo: dataredshift //complete(&quot;measurementItems type: &quot; + measurementItems.flag + &quot;measurementItems data: &quot; + measurementItems.measurementItem) &#125; else &#123; complete(401,&quot;token error,access denied&quot;) &#125; &#125; &#125; &#125; &#125; &#125;  case class  unmashaller 1234567object MeasurementItemsJsonImplicits extends DefaultJsonProtocol &#123; implicit val CustomerInfoJson = jsonFormat9(CustomerInfo) implicit val MeasurementItemDetailJson = jsonFormat2(MeasurementItemDetail) implicit val MeasurementItemInfoJson = jsonFormat2(MeasurementItemInfo) implicit val AnalysisReportJson = jsonFormat6(AnalysisReport) implicit val MeasurementItemsJson = jsonFormat9(MeasurementItems)&#125; .text/xml  POST ]]></content>
      <categories>
        <category> - web</category>
      </categories>
      <tags>
        <tag>webapp</tag>
        <tag>HttpPost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E8%AF%B4%E7%9A%84%E5%A5%BD%2F</url>
    <content type="text"><![CDATA[https://www.zhihu.com/question/33578621/answer/451931102                 Tag  Bug            Highxiaonaigou,xxxx,j1,jl,llst.                UrlGetPost   33  3333333333    TDDIDE     Demo      TPS  70%                    ]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[]]></title>
    <url>%2F2018%2F12%2F04%2F%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[  VMarevmareubuntu windowscnblogs centosiso,ubuntucentosminimal matrixclone,java,clone hostname,/etc/hosts     777700 zookeeperZookeeper  Zookeeper  Zookeeper Zookeeper  zookeeper dataDirmyidid1 cp confzoo_sample.cfgzoo.cfg,zookeeper1234567891011121314151617181920212223242526272829303132333435 # The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/root/data/zookeeper# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1#logDirdataLogDir=/root/logs/zookeeperserver.1=0.0.0.0:2888:3888server.2=ake:2888:3888server.3=angela:2888:3888server.4=houyi:2888:3888server.5=houzi:2888:3888 dataDirzookeeperserver.id=ip/hostname:leader:leaderleader zookeeperip/hostname0.0.0.0,leaderrefstackoverflow zookeeperzkServer.sh 123456 /root/zookeeper/zookeeper-3.4.10/bin/zkServer.sh startvim /root/zookeeper/zookeeper-3.4.10/conf/zoo.cfg#/root/zookeeper/zookeeper-3.4.10/bin/zkServer.sh stop/root/zookeeper/zookeeper-3.4.10/bin/zkServer.sh status zookeeper.out status 123 ZooKeeper JMX enabled by defaultUsing config: /root/zookeeper/zookeeper-3.4.10/bin/../conf/zoo.cfgMode: follower ref: IBM CentOS 7 zookeeper ZooKeeper KAFKA kafka zookeeper.connect hostname 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142# Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the &quot;License&quot;); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# see kafka.server.KafkaConfig for additional details and defaults############################# Server Basics ############################## The id of the broker. This must be set to a unique integer for each broker.broker.id=3# Switch to enable topic deletion or not, default value is false#delete.topic.enable=true############################# Socket Server Settings ############################## The address the socket server listens on. It will get the value returned from# java.net.InetAddress.getCanonicalHostName() if not configured.# FORMAT:# listeners = listener_name://host_name:port# EXAMPLE:# listeners = PLAINTEXT://your.host.name:9092listeners=PLAINTEXT://houyi:9092hostname=houyiport=9092# Hostname and port the broker will advertise to producers and consumers. If not set,# it uses the value for &quot;listeners&quot; if configured. Otherwise, it will use the value# returned from java.net.InetAddress.getCanonicalHostName().#advertised.listeners=PLAINTEXT://your.host.name:9092# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL# The number of threads that the server uses for receiving requests from the network and sending responses to the networknum.network.threads=3# The number of threads that the server uses for processing requests, which may include disk I/Onum.io.threads=8# The send buffer (SO_SNDBUF) used by the socket serversocket.send.buffer.bytes=102400# The receive buffer (SO_RCVBUF) used by the socket serversocket.receive.buffer.bytes=102400# The maximum size of a request that the socket server will accept (protection against OOM)socket.request.max.bytes=104857600############################# Log Basics ############################## A comma seperated list of directories under which to store log fileslog.dirs=/tmp/kafka-logs# The default number of log partitions per topic. More partitions allow greater# parallelism for consumption, but this will also result in more files across# the brokers.num.partitions=1# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.# This value is recommended to be increased for installations with data dirs located in RAID array.num.recovery.threads.per.data.dir=1############################# Internal Topic Settings ############################## The replication factor for the group metadata internal topics &quot;__consumer_offsets&quot; and &quot;__transaction_state&quot;# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1############################# Log Flush Policy ############################## Messages are immediately written to the filesystem but by default we only fsync() to sync# the OS cache lazily. The following configurations control the flush of data to disk.# There are a few important trade-offs here:# 1. Durability: Unflushed data may be lost if you are not using replication.# 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.# 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks.# The settings below allow one to configure the flush policy to flush data after a period of time or# every N messages (or both). This can be done globally and overridden on a per-topic basis.# The number of messages to accept before forcing a flush of data to disk#log.flush.interval.messages=10000# The maximum amount of time a message can sit in a log before we force a flush#log.flush.interval.ms=1000############################# Log Retention Policy ############################## The following configurations control the disposal of log segments. The policy can# be set to delete segments after a period of time, or after a given size has accumulated.# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens# from the end of the log.# The minimum age of a log file to be eligible for deletion due to agelog.retention.hours=168# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining# segments don&apos;t drop below log.retention.bytes. Functions independently of log.retention.hours.#log.retention.bytes=1073741824# The maximum size of a log segment file. When this size is reached a new log segment will be created.log.segment.bytes=1073741824# The interval at which log segments are checked to see if they can be deleted according# to the retention policieslog.retention.check.interval.ms=300000############################# Zookeeper ############################## Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181# Timeout in ms for connecting to zookeeperzookeeper.connection.timeout.ms=6000############################# Group Coordinator Settings ############################## The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.# The default value for this is 3 seconds.# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.group.initial.rebalance.delay.ms=0 brokerid,brokerid kafka 12345bin/kafka-server-start.sh config/server.properties &amp;bin/kafka-topics.sh --create --zookeeper yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181 --replication-factor 3 --partitions 3 --topic topicTestbin/kafka-topics.sh --list --zookeeper yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181bin/kafka-console-producer.sh --broker-list yase:9092,ake:9092,angela:9092,houyi:9092,houzi:9092 --topic topicTestbin/kafka-console-consumer.sh --zookeeper yase:2181,ake:2181,angela:2181,houyi:2181,houzi:2181 --topic topicTest --from-beginning hadoop  binary2.7 slaves core-site.xml hdfs-site.xml  yasemasteryaseslavesslavedatanode core-site.xmlfile:// , 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/hdpspk/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://yase:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 12bin/hdfs namenode -formatsbin/start-all.sh ref:hadoop2.7 spark master ref:spark22 Amazon EC2,ssh : securecrtlinux ]]></content>
      <categories>
        <category> - </category>
      </categories>
      <tags>
        <tag></tag>
        <tag></tag>
        <tag></tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web Note]]></title>
    <url>%2F2018%2F12%2F04%2FWeb-Note%2F</url>
    <content type="text"><![CDATA[          BST  B-B+B* LSM  BitSet              Java    KMP                ACID   MVCC  Java  &amp;    &amp; CAS ABA  CopyOnWrite RingBuffer  &amp;   &amp;     CPU     Linux   23    MVC IOC AOP UML    &amp;  &amp;   APM  (CI/CD) Jenkins   Ansible puppet chef  TDD     A/B   KVM Xen OpenVZ  Docker  OpenStack DevOps   Web Server Nginx OpenResty Apache Httpd Tomcat   Jetty     Web Memcached Redis   Tair    RabbitMQ RocketMQ ActiveMQ Kafka Redis  ZeroMQ    RPC Dubbo Thrift gRPC  Sharding Jdbc    API    OSI  TCP/IP HTTP HTTP2.0 HTTPS  Epoll Java NIO kqueue   Zero-copy () Hessian Protobuf    MySQL  InnoDB   ,   (AHI) explain NoSQL MongoDB Hbase   Lucene Elasticsearch Solr sphinx    CDN      Storm Flink Kafka Stream  Hadoop HDFS MapReduce Yarn Spark  web  XSS CSRF SQL  Hash Dos    DDoS               RBAC OAuth2.0 2FA (SSO)    Log4jLog4j2 Logback ORM  Web  Spring      &amp;                CAP  BASE    PAXOS Zab Raft Gossip     Leader  TCC(Try/Confirm/Cancel)   ID  ID Hash  &amp;  DDD(Domain-driven Design - ) (CQRS)  Actor   Reactor RxJava Vert.x DODAF2.0 Serverless Service Mesh      Review RUP  SCRUM  XP  FMEA     253                             APP      VPS Toc generated by simple-php-github-toc   javaqueue ConcurrentLinkedQueue()CAScompareAndSwapObject ArrayBlockingQueue()LinkedBlockingQueueDelayQueuePriorityBlockingQueue ReentrantLock  LinkedListConcurrentLinkedQueueLinkedBlockingQueue  Java Set  JavaList  Java map  - API  javaStack Java Stack  java stack Stack        1 - :  2-3 BSTBinary Search Treeordered binary tree,sorted binary tree :      :   B-B+B*MySQLB+ B-B+B* B-B+B* B+  B- LSM  LSMLog-Structured Merge-Trees B+ ()HbaseLevelDBTairLong DBnessDB  LSM LSM LSM VS B+ B+ key LSM Nflush LSMLog-Structured Merge Tree LSMHBaseMySQL Bloom filter compact  Hbase flushB+HDFSupdateHbaseflushmerge updateflush BitSet Java Bitset Java BitSet      JavaSelectionSort   2   O(n)       () () TODO  ()        [0,1)n      (java)   O(logN) java- while + Java  Arrays.sortCollections.sort Collections.sort Arrays.sort() 2   emailurl     Bitmap(Bloom Filter) Redis  Redis  Bitmap  URL(BloomFilter) Java BitSet   hash KMP KMPKnuth-Morris-PrattKMP KMP  BFSDFS        -      P(B|A)=P(A|B)P(B)/P(A) 1 2   TOP 10   KruskalPrim  Dijkstra Java  Java  JAVA  40Java  Java  ACID  ACID   UPDATERCSQL ServerOracle  Mysql InnoDB   4  MySQLInnoDB    SELECT  FOR UPDATE  MySQLInnoDB  MVCC mysqlinnodbMVCC innodb  MVCC  Repeatable-Read  MVCC  MYSQL MVCC   MVCC    Java Java  synchronizedReentrantLock ReadWriteLock JavaAQS Java Semaphore   acquire release javaMutex vs Semaphore  Mutex Semaphore  &amp;    ReentrantLock  synchronized ReentrantLock  + MySQL&amp; +  select  for update (share ) Mysqlselect.. for update mysqlinnodb  Mysql  &amp; CAS CAS MySQL ABA CASAAMysql  Java CAS ABA Java  ABA AtomicStampedReference  AtomicStampedReference CopyOnWriteCopyOnWriteCopyOnWrite JAVA(Copy-On-Write)Map   -JavaCopy-On-Write RingBuffer RingBuffer  &amp;      Javasynchronized  java.util.concurrent.locks.ReentrantLock ReenTrantLocksynchronized synchronized  ReenTrantLock   synchronized  &amp; ReentrantLock ReadWriteLock SemaphoreCountDownLatch ReadWriteLock  ReadWriteLock    Java JConsole  java jstack    CPU CPU L1  32kL2  256kL3 12M200 CPU CPU 1CPU JavaCPU TODO    python-yieldactor  . IOIOCPUIO. Linux Linux    , , ,, ,  /, 23  23  JDK   java.util.Arrays#asList() JDBC  Map.putAllList.addAllSet.addAll  java.util.Collections#checkedList|Map|Set|SortedSet|SortedMap  valueOf(int)  java.lang.reflect.Proxy :  java.util.Calendar#getInstance() (Builder)java.lang.StringBuilder#append()  *  java.lang.Object#toString()java.lang.Class#newInstance() java.lang.Object#clone()  java.lang.Runtime#getRuntime()   javax.servlet.Filter#doFilter() java.lang.Runnable java.text.Formatjava.text.Normalizer  java.util.Iterator java.lang.reflect.Method#invoke()  java.util.Collections#emptyList()  java.util.EventListener  java.util.Collections#sort() Spring- Mybatis      TODO MVC MVC  (model)(view)(controller) IOC  IOC IOC  new  DIDependency Injection AOP AOP() Spring AOP Spring AOP Spring AOPJDKCGLIB Spring AOP  CGLIB  Spring AOP  AOP Spring AOP  JDK  AOP Spring AOP  CGLIB  AOP  UML UML      -      20  &amp;  &amp;   ()      ZabbixNagiosGangliaZenossOpen-falcon 360    topsartsarnload 20 Linux  JVMjpsjstackjmapjhatjstathprof APMAPM  Application Performance Management Dapper CNCF OpenTracing  Apache SkyWalking CAT CNCF jaeger Pinpoint Zipkin APM  GoogleDapper     APP App Annietalking data   (CI/CD)  8 Jenkins Jenkins   Ansible Ansible Ansible puppet puppet chef Chef  TDD   - TDD XPExtreme Programming.   JavaJUnit UT JUnit 4  TestNG  TestNG  JUnit       Apache ab   10//  tcpcopy nGrinder   618ForceBot    A/B   | AB  nginx IP  A/B   VPSOpenVZXenKVM KVM KVM KVM  Xen Xen OpenVZ Linux OpenVZ  Docker  docker  Docker  Docker  OpenStack OpenStack DevOps DevOps DevOps  Confluence- GitLab? Wiki Web ServerNginx Ngnix-Apache Nginx Apache  IO(Nginx)CPU(Apache)Nginxweb nginxApache nginx OpenResty   OpenResty  Lua Nginx Apache Httpd  Tomcat TOMCAT Tomcat Tomcat , 1 :  Tomcat JBoss vs. Tomcat: Choosing A Java Application Server Tomcat  Serverlet  JEE Srping Jboss JEE  Tomcat  NIOAPRAJPNginx+tomcatAJP tomcat httpajp AJPHTTP AJP 8009ServerApacheAJP() AJPHTTP Jetty Jetty  Tomcat  jettytomcat :JettyTomcat JettyTomcatJettyNIOI/OTomcatBIOI/OTomcat JettyServlet;Tomcat JEEServlet   FIFO LRULFU  HashMap EhCache    Guava Cache  Nginx Pagespeed    Cache-Control  H5  WebView  Web nuster - nuster cache varnish - varnish cache squid - squid cache Memcached Memcached  Memcached  slab memcachedSlab1MBSlab slabchunkChunkChunk 1.25IO Memcached Memcache memcache  add  set replace  keytruefalse memcached Redis Redis  redis  ziplist ziplist  skiplist()Level Redis RDBforkRDB  AOF  AOF  3CAS  Redis  redis Tair  TairRedis  Hash Hadoop ConfigserverDataServerConfigserver Configserver : MDBSession RdbRedisaofRestfull LDBLevelDBLDB rdbgoogleLSM(Log-Structured-Merge Tree)Hash Tair  -/ &amp; ActiveMQJMS RabbitMQ  Kafka  Push Pull KafkaRabbitMQRocketMQ    VS   RabbitMQ RabbitMQ  RabbitMQ RabbitMQ+Confirm RocketMQJavaKafka RocketMQ  RocketMQ  ActiveMQJavaJMSJava ActiveMQ KafkaIO  Kafka Kafka Redis list   blpop  RedisRedis ZeroMQ TODO  linuxcron Linux cron fork  + sleep  Quartz Quartz -  quartz  QuartzSchedulerThread while()triggerjob   opencronLTSXXL-JOBElastic-JobUncode-ScheduleAntares Quartz QuartzQuartzQuartz Elastic-Job-Lite  Elastic-Job-Cloud  RPC RPC - RPC Server: Client: Registry:  RPC dubbomotanrpcxgRPCthrift Dubbo  dubbo ** SPI ** TODO Thrift  Thrift RPC  gRPC  RPC Sharding Jdbc   ELKB ELK -  Apollo -  Spring Boot  Spring Cloud   zookeeper  Spring Cloud Config  servlet 3.0  servlet3.0  API  API API ZuulAPI Gateway Spring Cloud Gateway  HTTP APIKong OSI  OSITCP/IP TCP/IP  TCP/IP  TCP HTTP http() HTTP2.0 HTTP 2.0  HTTP2.0   HTTPS https    SSL-Https  webI/oweb I/OI/OI/OI/O()I/OI/OI/OI/O  Web Server Prefork()Worker()Event selectpollepoll selectpollepollI/O select 10242048 for x641001000poll selectpoll fdepollCPUselectpollfdepoll pollepoll selectpollepoll  selectpollepollepoll Java NIO NIO  IO  IO  IO  BIONIOAIO ReactorProactor Epoll epoll Java NIO Java NIO Java NIOSocket kqueue kqueue  TCP/IP  Netty Reactor  Netty  Reactor  Zero-copy  Netty ByteBuf (Zero Copy)  buffer ()Hessian HessianBinary-RPC; Protobuf ProtobufJavaGoolgeHessian .proto  Protocol Buffers * ;  protobuf  protostuff protostuff  .proto Java     2NF1NF 2NF MySQL MySQLInnoDB MySQLMyISAMInnoDB Innodb  myisaminnodb InnoDB MysqlInnoDB  MySQL36 MYSQL20+ SQL mysql   MYSQLlimit  ,  MySQL / MyISAMInnoDB MyISAM InnoDB    (AHI) InnoDB explain MySQL  Explain  NoSQLMongoDB MongoDB  Mongodb GridFSSchema-less ShardingNoSQL mongodbmongodbMongoDBMySQLIT Hbase  HBase  HBase HBase Hbase  HBase Rowkey rowkey     Lucene Lucene Elasticsearch Elasticsearch Elasticsearch Solr  Apache Solr elasticsearchsolr sphinx Sphinx   155      QPSQPSQPS CDN  CDN  CDN  Java  Java Storm  Storm Flink Flink Flink Kafka Stream Kafka Stream       Hadoop hadoop, Hadoop HDFS HadoopHDFS MapReduce Map/Reduce  map-reducejava Yarn Yarn Spark Spark():  web XSS xssCSRF CSRF SQL  SQL Hash Dos JAVA HASH DOS JsonObjet JsonJsonObject HashMaphashHashCPU DoS-Hash Hash Collision DoS    DVWA W3af OpenVAS      DDoS  DDoS DDoS  salt  *   TODO     LibJava   DES3DESBlowfishAES DES  56Blowfish 1448AES 128192256 DES 56 AES  AES    MD5  SHA-1   SHA-256  HashURL   RSADSAECDSA()  RSA  DSA RSARSA 256ECC3072RSA   Linux15Linux TODO TODO   RBAC  RBAC SpringShiro OAuth2.0 OAuth 2.0 OAuth2.0 2FA2FA - Two-factor authentication   + Key USB key 2FA(http://www.ruanyifeng.com/blog/2017/11/2fa-tutorial.html) (SSO)  CAS    Log4jLog4j2 log4j  log4j2  Log4j1,LogbackLog4j2 Log4J  Logback LogBack java ORM ORM  MyBatis mybatis SqlSessionSqlSession mappernamespaceSqlSession LRU  cacheEnabled  MyBatisGenerator TODO Web Spring Spring Spring  Spring Boot  Spring Boot Spring Cloud Spring Boot  Spring Cloud  Spring Cloud  Apache Commons  Google guava      + MySQL Proxy ID  +   &amp;      2.         ()()100%999.999%5  NginxF5 F5 /    QoS DNS  Nginx web LVS+Keepalived  4 HAProxy  HTTP Haproxy+Keepalived+MySQL  rabbitmq+haproxy+keepalived     Guava  RateLimiter  Nginx  limit_req    Hystrix  Bug () Hystrix Hystrix,   =  /   HystrixCommand   key  key             docker        1.vip2. flush ()3,  JVMjavaSystem.exitKill SIGTERM kill-9 Runtime.addShutdownHook  JavaJavaSrpingDubbo   Mysql MySQL Haproxy+MySQL(Slave)  DRBD+Heartbeat+Mysql DRDB  MySQL Cluster     sharding-jdbcTShardingAtlasMyCATVitess JoinID ;;   MySQL  5Oracle  MySql MySQL    Eureka   SpringCloud:Consul vs Zookeeper vs Etcd vs Eureka CAPConsulCAzookeepercpetcdcp euerkaap  Consul  Spring cloud  Zookeeper APIPinterestAirbnb watcherPUSH  4    injvm(jvm)innative(),  CAP  BASE  CAPBASE ()()() CAP() BASEBasically AvailableSoft stateEventually consistent BASE     Zookeeper zk Zookeeper  + Java  jedisLockredis  setnx(set if ont exists)falsetrue Memcached  Redis   memcached  addsetkeyfalse PAXOS Paxos Paxos&gt;Fast Paxos&gt;Zookeeper ZookeeperPaxos Zab ZabZookeeper  Raft Raft  LeaderFollowerCandidate  Gossip Gossip      MVCC()token   6  Leader  zookeeperleader TCC(Try/Confirm/Cancel)   BASE +()  -    HDFS FastDFS ID ID Id Twitter Snowflake 41+10IP+12() Flicker MySQLID + REPLACE INTO XXX:SELECT LAST_INSERT_ID(); UUID MongoDB  ObjectId TDDL SEQUENCE  sequence id id 1000~2000 sequence id id Hash   &amp; DDD(Domain-driven Design - ) DDD DDD (--)DDD DomainDomain Expert  DDD   Doamin Bounded Context Domain ModelDDD   /   Entity Value ObjectDate Domain Service () AggregateAggregate Root70%30%2~3 Factory RepositoryDB (DDD) CarEngineWheelTank 2VODTODOPO (CQRS)CQRS  Command Query Responsibility Seperation  ()CQRS CQ DDD CQRS  CQRS  CQRS  CQRS/EventSourcing CQRS  + 12306    Service OOService ServiceFacadeDAOOODODAOService ServiceDOOODODO  Actor TODO ReactorTODO RxJavaTODO Vert.xTODO DODAF2.0 DODAF2.0 DODAF2.0 Serverless Serverless Serverless  Serverless  Serverless  Serverless  Baas (Mobile) Backend as a Service  Faas Functions as a service Service Mesh Service Mesh  Service Mesh Service Mesh     12  Java  Review!  check list  Code Review  review  Code Review Code Review Checklist Java Code Review Checklist  gitlab  code review RUP RUP 4+1   SCRUMSCRUM -  3:Product Owner(PO) ;Scrum MasterSMScrum;Team  3Product Backlog TODOLIST;Sprint Backlog  TODO LIST  -Scrum 3scrum TODO XPXP - eXtreme Programming XP  4     5 5review reviewbug  PDCA PPLAN DDO CCHECK AACT  PDCA FMEATODO TODO TODO TODO 253253 3   25312532     NB 1. 2. 3.           2477     TODO  36kr Techweb TODO       - COOLSHELL- hellojava- Cms Blog DD--Spring Cloud  CSDN  51cto.com ITeye  Java   ChinaUnix  Linux      ITWeb ITPUB  +   IBM DeveloperWorks  LinkedKeeper  DZone Reddit  segmentfault +  stackoverflow   QUEST MOBILE  TalkingData  :   TesterHome : * [](http://www.yunweipai.com/) * [Abcdocker](https://www.abcdocker.com/) Java: ImportNew  Java  HowToDoInJava    FreeBuf    DockerInfo  Docker  Linux Linux     Spring Cloud - - 201870M InfoQ   Java   12               IT-   1   CTO           TODO  github Apache   W3Cschool Runoob.com HTML  CSSXMLJavaPythonPHP Love2.io  gitbook.cn  ApacheCN AI  Quick Code  gitbook.com  Cheatography Cheat Sheets  Tutorialspoint JavaPythonJSSQL    segmentfault    51CTO  QCon ArchSummit GITC :  APP    Boss   100Offer     Coding           (AWS)   VPS Linode]]></content>
      <categories>
        <category></category>
      </categories>
      <tags>
        <tag>note</tag>
      </tags>
  </entry>
</search>
